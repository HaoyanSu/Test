[
    {
        "id": "1001",
        "readme": "## 1. ComfyUI Workflow: AnimateDiff + ControlNet | Cartoon Style\n\nThis ComfyUI workflow adopts an methodology for video restyling, incorporating nodes such as AnimateDiff and ControlNet within the Stable Diffusion framework to augment the capabilities of video editing. **AnimateDiff** facilitates the conversion of text prompts into video content, extending beyond the conventional text-to-image models to produce dynamic videos. Conversely, **ControlNet** leverages reference images or videos to guide the motion of the generated content, ensuring that the output closely aligns with the reference in terms of movement. Integrating AnimateDiff's text-to-video generation with ControlNet's detailed movement control, this ComfyUI workflow provides a robust suite for generating high-quality, restyled video content.\n\n## 2. Overview of AnimateDiff\n\nPlease check out the details on [How to use AnimateDiff in ComfyUI](https://www.runcomfy.com/comfyui-workflows/transform-video-into-ceramic-art-style-using-animatediff-controlnet-within-comfyui#2-how-to-use-animatediff)\n\n## 3. Overview of ControlNet\n\nPlease check out the details on [How to use ControlNet in ComfyUI](https://www.runcomfy.com/comfyui-workflows/convert-video-to-japanese-anime-style-using-animatediff-controlnet-ipadapter-in-comfyui#3-how-to-use-controlnet)\n"
    },
    {
        "id": "1002",
        "readme": "## 1. ComfyUI Workflow: ControlNet Tile + 4x UltraSharp for Image Upscaling\n\nThis ComfyUI workflow offers an advanced approach to video enhancement, beginning with **AnimeDiff** for initial video generation. It incorporates the **ControlNet Tile Upscale** for detailed image resolution improvement, leveraging the ControlNet model to regenerate missing details while maintaining consistency with the input. This is particularly useful for segment-based upscaling. Then the process is complemented by the **4x UltraSharp Model Upscale**, known for its exceptional clarity and detail enhancement capabilities, ensuring high-resolution outcomes without detail loss. Also, **frame interpolation** at each upscaling phase further elevates the video's quality, showcasing ComfyUI's comprehensive solution for achieving superior video resolution.\n\n## 2. Overview of AnimateDiff\n\nPlease check out the details on this [Introduction to AnimateDiff](https://www.runcomfy.com/comfyui-workflows/comfyui-animatediff-controlnet-and-auto-mask-workflow-video2video#2-overview-of-animatediff)\n\n## 3. Overview of ControlNet Tile\n\n### 3.1. Introduction to ControlNet Tile\n\nThe ControlNet Tile for Upscale utilizes a ControlNet model to enhance image resolution by accurately regenerating missing or inconsistent details, thereby selectively ignoring discrepancies between the input prompt and local image features. This capability is particularly advantageous for image upscaling in segments or tiles, making it an ideal solution for generating high-quality, detailed images.\n\n## 4. Overview of 4x-Ultrasharp Upscaler\n\n### 4.1. Introduction to 4x-Ultrasharp Upscaler\n\nThe 4x-Ultrasharp Upscaler stands out for its ability to transform images into higher resolutions with remarkable clarity and detail, addressing the common challenge of detail loss in upscaling processes.\n"
    },
    {
        "id": "1003",
        "readme": "## 1. ComfyUI Workflow: AnimateDiff + IPAdapter | Image to Video\n\nThis ComfyUI workflow is designed for creating animations from reference images by using **AnimateDiff** and **IP-Adapter**. The AnimateDiff node integrates model and context options to adjust animation dynamics. Conversely, the IP-Adapter node facilitates the use of images as prompts in ways that can mimic the style, composition, or facial features of the reference image, significantly enhancing the customization and quality of generated animations or images.\n\n## 2. Overview of AnimateDiff\n\nPlease check out the details on [How to use AnimateDiff in ComfyUI](https://www.runcomfy.com/comfyui-workflows/transform-video-into-ceramic-art-style-using-animatediff-controlnet-within-comfyui#2-how-to-use-animatediff)\n\n## 3. Overview of IP-Adapter\n\n### 3.1. Introduction to IP-Adapter\n\nIP-Adapter stands for \"Image Prompt Adapter,\" a novel approach for enhancing text-to-image diffusion models with the capability to use image prompts in image generation tasks. IP-Adapter aims to address the shortfalls of text prompts which often require complex prompt engineering to generate desired images. The introduction of image prompts, alongside text, allows for a more intuitive and effective way to guide the image synthesis process.\n\n**Different Models of IP-Adapter**\n\nThe IP-Adapter suite includes a variety of models, each tailored for specific use cases and levels of image synthesis complexity. Here is an overview of the different models available:\n\n**3.1.1. v1.5 Models**\n\n- `ip-adapter_sd15`: The standard model for version 1.5, which utilizes the power of IP-Adapter for image-to-image conditioning and text prompt augmentation.\n- `ip-adapter_sd15_light`: A lighter version of the standard model, optimized for less resource-intensive applications while still leveraging IP-Adapter technology.\n- `ip-adapter-plus_sd15`: An enhanced model that produces images more closely aligned with the original reference, improving on the fine details.\n- `ip-adapter-plus-face_sd15`: Similar to IP-Adapter Plus, with a focus on more accurate facial feature replication in the generated images.\n- `ip-adapter-full-face_sd15`: A model that emphasizes full-face details, likely offering a \"face swap\" effect with high fidelity.\n- `ip-adapter_sd15_vit-G`: A variant of the standard model using the Vision Transformer (ViT) BigG image encoder for more detailed image feature extraction.\n\n**3.1.2. SDXL Models**\n\n- `ip-adapter_sdxl`: The base model for the SDXL, which is designed to handle larger and more complex image prompts.\n- `ip-adapter_sdxl_vit-h`: The SDXL model paired with the ViT H image encoder, balancing performance with computational efficiency.\n- `ip-adapter-plus_sdxl_vit-h`: An advanced version of the SDXL model with enhanced image prompt detail and quality.\n- `ip-adapter-plus-face_sdxl_vit-h`: An SDXL variant focused on face details, ideal for projects where facial accuracy is paramount.\n\n**3.1.3. FaceID Models**\n\n- `FaceID`: A model using InsightFace to extract Face ID embeddings, offering a unique approach to face-related image generation.\n- `FaceID Plus`: An improved version of the FaceID model, combining InsightFace for facial features and CLIP image encoding for global facial features.\n- `FaceID Plus v2`: An iteration on FaceID Plus with an improved model checkpoint and the ability to set a weight on the CLIP image embedding.\n- `FaceID Portrait`: A model similar to FaceID but designed to accept multiple images of cropped faces for more diverse face conditioning.\n\n**3.1.4. SDXL FaceID Models**\n\n- `FaceID SDXL`: The SDXL version of FaceID, maintaining the same InsightFace model as the v1.5 but scaled for SDXL applications.\n- `FaceID Plus v2 SDXL`: An SDXL adaptation of FaceID Plus v2 for high-definition image generation with enhanced fidelity.\n\n### 3.2. Key Features of IP-Adapter\n\n**3.2.1. Text and Image Prompt Integration**: The IP-Adapter's unique capability to use both text and image prompts enables multimodal image generation, providing a versatile and powerful tool for controlling diffusion model outputs.\n\n**3.2.2. Decoupled Cross-Attention Mechanism**: The IP-Adapter employs a decoupled cross-attention strategy that enhances the model's efficiency in processing diverse modalities by separating text and image features.\n\n**3.2.3. Lightweight Model**: Despite its comprehensive functionality, the IP-Adapter maintains a relatively low parameter count (22M), offering performance that rivals or exceeds that of fine-tuned image prompt models.\n\n**3.2.4. Compatibility and Generalization**: The IP-Adapter is designed for broad compatibility with existing controllable tools and can be applied to custom models derived from the same base model for enhanced generalization.\n\n**3.2.5. Structure Control**: IP-Adapter supports detailed structure control, enabling creators to guide the image generation process with greater precision.\n\n**3.2.6. Image-to-Image and Inpainting Capabilities**: With support for image-guided image-to-image translation and inpainting, the IP-Adapter broadens the scope of possible applications, enabling creative and practical uses in a variety of image synthesis tasks.\n\n**3.2.7. Customization with Different Encoders**: The IP-Adapter allows for the use of various encoders, such as OpenClip ViT H 14 and ViT BigG 14, to process reference images. This flexibility facilitates handling different image resolutions and complexities, making it a versatile tool for creators looking to tailor the image generation process to specific needs or desired outcomes.\n\nThe incorporation of IP-Adapter technology in image generation projects not only simplifies the creation of complex and detailed images but also significantly enhances the quality and fidelity of the generated images to the original prompts. By bridging the gap between text and image prompts, IP-Adapter provides a powerful, intuitive, and efficient approach to controlling the nuances of image synthesis, making it an indispensable tool in the arsenal of digital artists, designers, and creators working within the ComfyUI workflow or any other context that demands high-quality, customized image generation.\n"
    },
    {
        "id": "1011",
        "readme": "## 1. ComfyUI AnimateDiff and Batch Prompt Schedule Workflow\n\nThe ComfyUI workflow presents a method for creating animations with seamless scene transitions using Prompt Travel (Prompt Schedule). This technique enables you to specify different prompts at various stages, influencing style, background, and other animation aspects. By scheduling prompts at specific frames, you can effortlessly craft dynamic, visually appealing transitions and transformations within GIFs or videos.\n\n## 2. Overview of AnimateDiff\n\nPlease check out the details on this [page](https://www.runcomfy.com/comfyui-workflows/comfyui-animatediff-controlnet-and-auto-mask-workflow-video2video#2-overview-of-animatediff)\n\n## 3. Overview of Batch Prompt Travel / Batch Prompt Travel\n\n### 3.1. Introduction to Prompt Travel / Prompt Travel\n\nPrompt Travel, also known as Prompt Schedule, is used in creating animations with AnimateDiff. This method involves the use of \"Prompt Interpolation,\" where different prompts are specified at various points in the animation process to control specific details, such as style, background, clothing choices, among others, within the final GIF or video creation. Essentially, Prompt Travel allows for fine-tuning of the animation by scheduling different prompts at different times to automatically add transition effects between them, enhancing the visual aesthetics of the animations.\n\nUtilizing Prompt Travel requires you to simply format your instructions within the animation's prompt. This involves specifying frame numbers followed by the corresponding content or visual theme desired for those specific junctures. For instance, you can dictate that the animation transitions from \"content A\" at frame number 1 to \"content B\" by frame number 20. This functionality not only facilitates the creation of dynamic and complex animations but also ensures a smooth and natural transition between different frames. So using this ComfyUI workflow, you can make your animation evolve from one state to another, demonstrating transformations across specified frames.\n"
    },
    {
        "id": "1015",
        "readme": "## 1. ComfyUI Stable Video Diffusion (SVD) and FreeU Workflow\n\nThis ComfyUI workflow facilitates an optimized image-to-video conversion pipeline by leveraging Stable Video Diffusion (SVD) alongside FreeU for enhanced quality output. FreeU elevates diffusion model results without accruing additional overhead—there's no need for retraining, parameter augmentation, or increased memory or compute time. This integration ensures that you can achieve higher fidelity in video outputs.\n\n## 2. Overview of Stable Video Diffusion (SVD)\n\nPlease check out the details on [Introduction to SVD](https://www.runcomfy.com/comfyui-workflows/comfyui-stable-video-diffusion-svd-workflow-text2video#2-overview-of-stable-video-diffusion-svd)\n\n## 3. Overview of FreeU\n\nPlease check out the details on this [Introduction to FreeU](https://www.runcomfy.com/comfyui-workflows/comfyui-animatediff-controlnet-and-ipadapter-workflow-video2video#5-overview-of-freeu)\n"
    },
    {
        "id": "1017",
        "readme": "## 1. ComfyUI AnimateDiff, ControlNet, IP-Adapter and FreeU Workflow\n\nThe ComfyUI workflow implements a methodology for video restyling that integrates several components—AnimateDiff, ControlNet, IP-Adapter, and FreeU—to enhance video editing capabilities.\n\n**AnimateDiff**: This component employs temporal difference models to create smooth animations from static images over time. It operates by identifying the differences between consecutive frames and incrementally applying these variations to reduce abrupt changes, thus preserving the motion's coherence.\n\n**ControlNet**: ControlNet harnesses control signals, such as those derived from pose estimation tools like OpenPose, to guide the animation's movement and flow. These control signals are layered and processed by models akin to control nets, which in turn shape the final animated output.\n\n**IP-Adapter**: The IP-Adapter is designed to adapt input images so that they align more closely with the targeted output styles or features. It undertakes processes such as colorization and style transfer, altering image attributes unsupervisedly.\n\n**FreeU**: As a cost-effective enhancement tool, FreeU refines diffusion models by fine-tuning the existing U-Net architectures. This results in a substantial boost in the quality of image and video generation, requiring only minimal modifications.\n\nTogether, these components synergize within this ComfyUI workflow to transform inputs into stylized animations through a sophisticated, multi-stage diffusion process.\n\n## 2. Overview of AnimateDiff\n\nPlease check out the details on [How to use AnimateDiff in ComfyUI](https://www.runcomfy.com/comfyui-workflows/transform-video-into-ceramic-art-style-using-animatediff-controlnet-within-comfyui#2-how-to-use-animatediff)\n\n## 3. Overview of ControlNet\n\nPlease check out the details on [How to use ControlNet in ComfyUI](https://www.runcomfy.com/comfyui-workflows/convert-video-to-japanese-anime-style-using-animatediff-controlnet-ipadapter-in-comfyui#3-how-to-use-controlnet)\n\n## 4. Overview of IP-Adapter\n\nPlease check out the details in the [Introduction to IPAdapter](https://www.runcomfy.com/comfyui-workflows/comfyui-animatediff-and-ipadapter-workflow-stable-diffusion-animation#3-overview-of-ip-adapter)\n\n\n## 5. Overview of FreeU\n\n### 5.1. Introduction to FreeU\n\nFreeU is a cutting-edge enhancement for diffusion models that elevates sample quality without additional overhead. It works within the existing system, requiring no further training, no extra parameters, and maintaining current memory and processing time. FreeU utilizes the diffusion U-Net architecture's existing mechanisms to instantly improve generation quality.\n\nThe innovation of FreeU lies in its ability to harness the diffusion U-Net's architecture more effectively. It refines the balance between the U-Net's denoising backbone and its high-frequency feature-adding skip connections, optimizing the quality of generated images and videos without compromising semantic integrity.\n\nFreeU is designed for easy integration with popular diffusion models, requiring minimal adjustments and the tuning of just two scaling factors during inference to deliver marked improvements in output quality. This makes FreeU an attractive option for those seeking to enhance their generative workflows efficiently.\n\n### Parameters for Optimal FreeU Performance\n\nFeel free to adjust these parameters based on your models, image/video style, or tasks. The following parameters are for reference only.\n\n**SD1.4: （will be updated soon）**\n\n**b1**: 1.3, **b2**: 1.4, **s1**: 0.9, **s2**: 0.2\n\n**SD1.5: (will be updated soon）**\n\n**b1**: 1.5, **b2**: 1.6, **s1**: 0.9, **s2**: 0.2\n\n**SD2.1**\n\n**b1**: 1.4, **b2**: 1.6, **s1**: 0.9, **s2**: 0.2\n\n**SDXL**\n\n**b1**: 1.3, **b2**: 1.4, **s1**: 0.9, **s2**: 0.2\n\n**Range for More Parameters**\n\nWhen trying additional parameters, consider the following ranges:\n\n- **b1**: 1 ≤ b1 ≤ 1.2\n- **b2**: 1.2 ≤ b2 ≤ 1.6\n- **s1**: s1 ≤ 1\n- **s2**: s2 ≤ 1\n\nFor more information, check it on [Github](https://github.com/ChenyangSi/FreeU/blob/main/README.md)\n"
    },
    {
        "id": "1018",
        "readme": "## 1. ComfyUI AnimateDiff, QR Code Monster and Upscale Workflow | Visual Effects\n\nIn this workflow, we employ AnimateDiff and ControlNet, featuring QR Code Monster and Lineart, along with detailed prompt descriptions to enhance the original video with stunning visual effects. QR Code Monster introduces an innovative method of transforming any image into AI-generated art. It is not necessary to input black-and-white videos, although this can be a great way to achieve good results. It is important to adjust the Strength, Start_Percent, or End_Percent of QR Code Monster to control the effects it introduces. Additionally, making the prompt detailed can yield better results.\n\nThis workflow is inspired by [the_marconi](https://civitai.com/articles/3172/nature-powers-netflix-seasons-workflow-and-details)\n\n## 2. Overview of AnimateDiff\n\nPlease check out the details on this [Introduction to AnimateDiff](https://www.runcomfy.com/comfyui-workflows/comfyui-animatediff-controlnet-and-auto-mask-workflow-video2video#2-overview-of-animatediff)\n\n## 3. Overview of QR Code Monster\n\n### 3.1. Introduction to QR Code Monster\n\nQR Code Monster revolutionizes the integration of QR codes into images. This model not only enables the creation of creative and scannable QR codes but also emphasizes a balance between aesthetics and functionality. By allowing QR codes to blend seamlessly into images with a gray background for enhanced integration, this model advances the capabilities of digital art. It's important to note that not all generated codes may be readable. Still, by experimenting with various parameters and prompts, users can achieve optimal results.\n\nFurthermore, QR Code Monster introduces an innovative method of transforming any image into AI-generated art. This is achieved by creatively integrating symbols or logos into images and leveraging multiple checkpoints within AI models to produce visually captivating images.\n\n### 3.2. Mastering Balance and Precision in QR Code Monster Art\n\nCreating visually appealing yet functional QR codes within artwork requires a nuanced balance between visibility and scannability. Below is a guide for artists and creators on how to navigate these settings to produce QR codes that are both striking and scannable.\n\n**3.2.1. Control Weight (Strength)**: This setting influences the final appearance of the QR code within the artwork. Adjusting the control weight helps manage the QR code's prominence in the image. A higher control weight increases visibility and ease of scanning but may detract from the artistic elements. In contrast, a lower control weight allows the QR code to blend more subtly into the artwork, enhancing aesthetics but possibly compromising scannability.\n\n**3.2.2. Ending Control Step (Start_Percent)**: This parameter affects the image generation's final stages. Reducing the Ending Control Step can make the QR code less obtrusive, preserving the artistic integrity of the piece. Conversely, increasing it can make the QR code more visible, which is crucial for its scanning functionality.\n\n**3.2.3. Starting Control Step (End_Percent)**: Adjusting the Starting Control Step may be necessary if the QR code is too conspicuous in the initial preview. Incrementally increasing this value can diminish its prominence, ensuring a more integrated appearance. However, it's crucial to avoid setting it too high, as this could compromise the scannability of the code.\n\n## 4. Overview of Upscale\n\nPlease check out the details on [Introduction to UltraSharp Upscale](https://www.runcomfy.com/comfyui-workflows/comfyui-hi-res-fix-upscaling-workflow-controlnet-tile-4x-ultrasharp#4-overview-of-4x-ultrasharp-upscaler)\n"
    },
    {
        "id": "1020",
        "readme": "## 1. ComfyUI Stable Video Diffusion (SVD) Workflow\n\nThe ComfyUI workflow seamlessly integrates text-to-image (Stable Diffusion) and image-to-video (Stable Video Diffusion) technologies for efficient text-to-video conversion. This workflow allows you to generate videos directly from text descriptions, starting with a base image that evolves into a dynamic video sequence. This workflow facilitates the realization of text-to-video animations or videos.\n\n## 2. Overview of Stable Video Diffusion (SVD)\n\n### 2.1. Introduction to Stable Video Diffusion (SVD)\n\nStable Video Diffusion (SVD) is a state-of-the-art technology developed to convert static images into dynamic video content. Leveraging the foundational Stable Diffusion image model, SVD introduces motion to still images, facilitating the creation of brief video clips. This advancement in latent diffusion models, initially devised for image synthesis, now incorporates temporal dimensions to animate still visuals, producing videos typically within the range of 2 to 5 seconds.\n\nStable Video Diffusion is available in two variants: the standard SVD, capable of generating videos at a resolution of 576×1024 pixels across 14 frames, and the enhanced SVD-XT, which can produce up to 25 frames. Both variants support adjustable frame rates from 3 to 30 frames per second, addressing diverse digital content creation requirements.\n\nThe training of the SVD model involves a three-stage process: starting with an image model, transitioning to a video model pre-trained with an extensive video dataset, and refining with a selection of high-quality video clips. This meticulous process highlights the significance of dataset quality in optimizing the model's video production capabilities.\n\nAt the heart of the Stable Video Diffusion model is the Stable Diffusion 2.1 image model, which acts as the foundational image backbone. The integration of temporal convolution and attention layers into the U-Net noise estimator evolves this into a powerful video model, interpreting latent tensors as video sequences. This model employs reverse diffusion to simultaneously denoise all frames, akin to the VideoLDM model.\n\nEquipped with 1.5 billion parameters and trained on a vast video dataset, the model undergoes further fine-tuning with a high-quality video dataset for peak performance. Two sets of SVD model weights are publicly accessible, designed for generating 14-frame and 25-frame videos at 576×1024 resolution, respectively.\n\n### 2.2. Key Features of Stable Video Diffusion (SVD)\n\nWhen using Stable video Diffusion in ComfyUI workflow, you can adjust the key parameters for video output customization include the motion **bucket id**, controlling the video's motion intensity; **frames per second (fps)**, determining the frame rate; and the **augmentation leve**l, adjusting the initial image's noise level for various transformation degrees. \n\n**2.2.1. Motion Bucket ID**: This feature offers users the ability to control the video's motion intensity. By tweaking this parameter, you can dictate the amount of movement observed in the video, ranging from subtle gestures to more pronounced action, depending on the desired visual effect.\n\n**2.2.2. Frames Per Second (fps)**: This parameter is crucial for determining the video's playback speed. Adjusting the frames per second allows you to produce videos that can either capture the swift dynamics of a scene or present a slow-motion effect, thereby enhancing the storytelling aspect of the video content. This flexibility is particularly beneficial for creating a wide range of video types, from fast-paced advertisements to more contemplative, narrative-driven pieces.\n\n**2.2.3.Augmentation Level Parameter**: This adjusts the initial image's noise level, enabling various degrees of transformation. By manipulating this parameter, you can control the extent to which the original image is altered during the video creation process. Adjusting the augmentation level allows for maintaining closer fidelity to the original image or venturing into more abstract and artistic interpretations, thus expanding creative possibilities."
    },
    {
        "id": "1021",
        "readme": "## 1. ComfyUI AnimateDiff, ControlNet and Auto Mask Workflow\n\nThis ComfyUI workflow introduces a powerful approach to video restyling, specifically aimed at transforming characters into an anime style while preserving the original backgrounds. This transformation is supported by several key components, including **AnimateDiff**, **ControlNet**, and **Auto Mask**.\n\n**AnimateDiff** is designed for differential animation techniques, enabling the maintenance of a consistent context within animations. This component focuses on smoothing transitions and enhancing the fluidity of motion in restyled video content. \n\n**ControlNet** serves a critical role in precise human pose replication and manipulation. It leverages advanced pose estimation to accurately capture and control the nuances of human movement, facilitating the transformation of characters into anime forms while preserving their original poses.\n\n**Auto Mask** is involved in automatic segmentation, adept at isolating characters from their backgrounds. This technology allows for selective restyling of video elements, ensuring that character transformations are executed without altering the surrounding environment, maintaining the integrity of the original backgrounds.\n\nThis ComfyUI workflow realizes the conversion of standard video content into stylized animations, focusing on efficiency and the quality of the anime-style character generation.\n\n## 2. Overview of AnimateDiff\n\n### 2.1. Introduction to AnimateDiff\n\nAnimateDiff emerges as an AI tool designed to animate static images and text prompts into dynamic videos, leveraging Stable Diffusion models and a specialized motion module. This technology automates the animation process by predicting seamless transitions between frames, making it accessible to users without coding skills or computing resources through a free online platform.\n\n### 2.2. Key Features of AnimateDiff\n\n**2.2.1. Comprehensive Model Support**: AnimateDiff is compatible with various versions, including AnimateDiff v1, v2, v3 for Stable Diffusion V1.5, and AnimateDiff sdxl for Stable Diffusion SDXL. It allows for the use of multiple motion models simultaneously, facilitating the creation of complex and layered animations.\n\n**2.2.2. Context Batch Size Determines Animation Length**: AnimateDiff enables the creation of animations of infinite length through the adjustment of the context batch size. This feature allows users to customize the length and transition of animations to suit their specific requirements, providing a highly adaptable animation process.\n\n**2.2.3. Context Length for Smooth Transitions**: The purpose of Uniform Context Length in AnimateDiff is to ensure seamless transitions between different segments of an animation. By adjusting the Uniform Context Length, users can control the transition dynamics between scenes—longer lengths for smoother, more seamless transitions, and shorter lengths for quicker, more pronounced changes.\n\n**2.2.4. Motion Dynamics**: In AnimateDiff v2, specialized motion LoRAs are available for adding cinematic camera movements to animations. This feature introduces a dynamic layer to animations, significantly enhancing their visual appeal.\n\n**2.2.5. Advanced Support Features**: AnimateDiff is designed to work with a variety of tools including ControlNet, SparseCtrl, and IPAdapter, offering significant advantages for users aiming to expand the creative possibilities of their projects.\n\n## 3. Overview of ControlNet\n\n### 3.1. Introduction to ControlNet\n\nControlNet introduces a framework for augmenting image diffusion models with conditional inputs, aiming to refine and guide the image synthesis process. It achieves this by duplicating the neural network blocks within a given diffusion model into two sets: one remains \"locked\" to preserve the original functionality, and the other becomes \"trainable,\" adapting to the specific conditions provided. This dual structure allows developers to incorporate a variety of conditional inputs by using models such as OpenPose, Tile, IP-Adapter, Canny, Depth, LineArt, MLSD, Normal Map, Scribbles, Segmentation, Shuffle, and T2I Adapter, thereby directly influencing the generated output. Through this mechanism, ControlNet offers developers a powerful tool to control and manipulate the image generation process, enhancing the diffusion model's flexibility and its applicability to diverse creative tasks.\n\n**Preprocessors and Model Integration**\n\n**3.1.1. Preprocessing Configuration**: Initiating with ControlNet involves selecting a suitable preprocessor. Activating the preview option is advisable for a visual understanding of the preprocessing impact. Post-preprocessing, the workflow transitions to utilizing the preprocessed image for further processing steps.\n\n**3.1.2. Model Matching**: Simplifying the model selection process, ControlNet ensures compatibility by aligning models with their corresponding preprocessors based on shared keywords, facilitating a seamless integration process.\n\n### 3.2. Key Features of ControlNet\n\n**In-depth Exploration of ControlNet Models**\n\n**3.2.1. OpenPose Suite**: Designed for precise human pose detection, the OpenPose suite encompasses models for detecting body poses, facial expressions, and hand movements with exceptional accuracy. Various OpenPose preprocessors are tailored to specific detection requirements, from basic pose analysis to detailed capture of facial and hand nuances.\n\n**3.2.2. Tile Resample Model**: Enhancing image resolution and detail, the Tile Resample model is optimally used alongside an upscaling tool, aiming to enrich image quality without compromising visual integrity.\n\n**3.2.3. IP-Adapter Model**: Facilitating the innovative use of images as prompts, the IP-Adapter integrates visual elements from reference images into the generated outputs, merging text-to-image diffusion capabilities for enriched visual content.\n\n**3.2.4. Canny Edge Detector**: Revered for its edge detection capabilities, the Canny model emphasizes the structural essence of images, enabling creative visual reinterpretations while maintaining core compositions.\n\n**3.2.5. Depth Perception Models**: Through a variety of depth preprocessors, ControlNet is adept at deriving and applying depth cues from images, offering a layered depth perspective in generated visuals.\n\n**3.2.6. LineArt Models**: Convert images into artistic line drawings with LineArt preprocessors, catering to diverse artistic preferences from anime to realistic sketches, ControlNet accommodates a spectrum of stylistic desires.\n\n**3.2.7. Scribbles Processing**: With preprocessors like Scribble HED, Pidinet, and xDoG, ControlNet transforms images into unique scribble art, offering varied styles for edge detection and artistic reinterpretation.\n\n**3.2.8. Segmentation Techniques**: ControlNet's segmentation capabilities accurately classify image elements, enabling precise manipulation based on object categorization, ideal for complex scene constructions.\n\n**3.2.9. Shuffle Model**: Introducing a method for color scheme innovation, the Shuffle model randomizes input images to generate new color patterns, creatively altering the original while retaining its essence.\n\n**3.2.10. T2I Adapter Innovations**: The T2I Adapter models, including Color Grid and CLIP Vision Style, propel ControlNet into new creative domains, blending and adapting colors and styles to produce visually compelling outputs that respect the original's color scheme or stylistic attributes.\n\n**3.2.11. MLSD (Mobile Line Segment Detection)**: Specializing in the detection of straight lines, MLSD is invaluable for projects focused on architectural and interior designs, prioritizing structural clarity and precision.\n\n**3.2.12. Normal Map Processing**: Utilizing surface orientation data, Normal Map preprocessors replicate the 3D structure of reference images, enhancing the generated content's realism through detailed surface analysis."
    },
    {
        "id": "1031",
        "readme": "## 1. ComfyUI AnimateDiff Workflow\n\nThis ComfyUI AnimateDiff workflow is designed for users to delve into the sophisticated features of AnimateDiff across **AnimateDiff V3**, **AnimateDiff SDXL**, and **AnimateDiff V2** versions. It facilitates exploration of a wide range of animations, incorporating various motions and styles. After creating animations with AnimateDiff, Latent Upscale is employed for achieving high-resolution outcomes using two approaches. The first leverages traditional latent upscaling within ComfyUI, with adjustments such as denoising strength and upscale factor. The second, Control Net Assisted Latent Upscale, offers more precise enhancements by integrating a line art preprocessor and the suitable control net model, significantly improving the artwork while maintaining its original essence.\n\n## 2. Overview of AnimateDiff\n\n### 2.1. Introduction to AnimateDiff\n\nAnimateDiff emerges as a pioneering framework, designed to breathe life into images generated by text-to-image models, such as Stable Diffusion. This tool allows users to seamlessly integrate motion into static images, transforming them into personalized animated visuals. The framework is built on the premise of enhancing existing text-to-image models with a motion modeling module, trained on video clips to capture realistic motion dynamics. This process eliminates the need for model-specific adjustments, offering a universal solution for animating personalized images.\n\n### 2.2. Different versions of AnimateDiff\n\n**2.2.1. AnimateDiff V3: Revolutionizing Motion with New Technologies**\n\nAnimateDiff V3 introduces a groundbreaking motion module, encapsulating the latest advancements in animation technology. At the heart of this evolution lies the Domain Adapter LoRA module, a mechanism that prepares the motion module by training on static video frames. This setup enables AnimateDiff to adeptly navigate the complexities of motion, ensuring animations are both nuanced and flexible. Unlike its predecessor, V3 doesn't surpass V2 across all dimensions but introduces varied motion capabilities, enriching the user's creative toolkit.\n\n**2.2.2. AnimateDiff SDXL: High-Resolution Video Animation**\n\nFor enthusiasts of high-definition visuals, AnimateDiff SDXL presents an enticing option. This Beta version supports the creation of high-resolution videos (1024x1024 resolution with 16 frames), accommodating various aspect ratios with or without personalized models. Despite being in Beta, SDXL promises to elevate the quality of animated content, with more refined versions anticipated shortly.\n\n**2.2.3. AnimateDiff V2: Classic Motion and Camera Movement Controls**\n\nAnimateDiff V2 stands as the foundational version that significantly improved sample quality through enhanced resolution and batch size training. It introduced MotionLoRA, facilitating control over eight fundamental camera movements, including Zoom In/Out, Pan Left/Right, Tilt Up/Down, and Rolling Clockwise/Anticlockwise. This version is ideal for users seeking to incorporate dynamic camera movements into their animations, offering a robust set of tools for creating dramatic visual narratives.\n"
    },
    {
        "id": "1032",
        "readme": "## 1. ComfyUI AnimateDiff and ControlNet Morphing Workflow\n\nThis ComfyUI workflow, which leverages AnimateDiff and ControlNet TimeStep KeyFrames to create morphing animations, offers a new approach to animation creation. **AnimateDiff** is dedicated to generating animations by interpolating between keyframes—defined frames that mark significant points within the animation. On the other hand, **ControlNet** enhances this process by providing precise control over the animation's details and movements through the use of \"Timestep KeyFrame\" and the \"ControlNet Tile\" model. These timestep keyframes pinpoint specific moments in the animation where changes occur, facilitating a high level of precision in the development of the animation over time. Collectively, AnimateDiff and ControlNet forge a robust methodology for generating morpging animations that are both dynamic and engaging, by synergizing their distinct functionalities to enhance the overall animation workflow.\n\n## 2. Overview of AnimateDiff\n\nPlease check out the details on [How to use AnimateDiff in ComfyUI](https://www.runcomfy.com/comfyui-workflows/transform-video-into-ceramic-art-style-using-animatediff-controlnet-within-comfyui#2-how-to-use-animatediff)\n\n## 3. Overview of ControlNet Tile Model\n\nThe ControlNet Tile model excels in refining image clarity by intensifying details and resolution, serving as a foundational tool for augmenting textures and elements within visuals. In the realm of morphing animations, it synergizes with ControlNet TimeStep KeyFrames to seamlessly blend noise augmentation with the meticulous enhancement of finer details. This integration not only sharpens and enriches the textures but also ensures that transitions between frames are smooth and cohesive, employing TimeStep KeyFrames for precise control over the animation's temporal and visual progression.\n\n## 4. Overview of ControlNet TimeStep KeyFrames\n\nControlNet TimeStep KeyFrames provide an advanced mechanism for manipulating the flow of AI-generated visuals, ensuring precise timing and progression in animations or dynamic imagery. \n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1032/readme01.webp\" alt=\"ControlNet TimeStep KeyFrames\" width=\"750\"/>\n\nThis overview presents the essential parameters for their optimal and intuitive application:\n\n### 4.1. prev_timestep_kf\nConsider the role of prev_timestep_kf as creating a bridge to the preceding keyframe in a sequence, thereby crafting a fluid transition or storyboard. This linkage aids in guiding the AI's generation process seamlessly from one phase to the next, underpinning a logical progression.\n\n### 4.2. cn_weights: \nThe cn_weights parameter plays a pivotal role in refining the output by modifying specific characteristics within ControlNet across various stages of content generation, enhancing the precision of Timestep KeyFrame application.\n\n### 4.3. latent_keyframe\nThrough latent_keyframe, you can dictate the extent of influence individual parts of the AI model have on the final product during specific phases. Whether aiming to intensify the detail in the foreground of an evolving image or to diminish certain elements over time, this parameter allows for dynamic adjustments. It's instrumental in generating visuals that require detailed evolution or precise timing and progression, showcasing the versatility of Timestep KeyFrames.\n\n### 4.4. mask_optional\nEmploying mask_optional offers a targeted approach, enabling the concentration of ControlNet's influence on selected image areas. This feature can be utilized to spotlight or accentuate elements, providing a nuanced control reminiscent of Timestep KeyFrame's detailed orientation.\n\n### 4.5. start_percent\nThe start_percent parameter essentially schedules the activation of your keyframe within the generation timeline, akin to cueing an actor's entrance in a play, ensuring timely appearances in sync with the narrative flow.\n\n### 4.6. strength\nOffering overarching control, the strength setting determines the influence magnitude of ControlNet on the output, embodying the granular control facilitated by Timestep KeyFrames.\n\n### 4.7. null_latent_kf_strength\nNull_latent_kf_strength serves as a guideline for any unaddressed components within a scene, ensuring even the background or less focused areas are cohesively integrated, a testament to the comprehensive control offered by Timestep KeyFrames.\n\n### 4.8. inherit_missing\nThe inherit_missing function ensures a smooth transition between keyframes by allowing the current frame to inherit any unspecified attributes from its predecessor, enhancing continuity without redundancy, a feature that underscores the efficiency of Timestep KeyFrame utilization.\n\n### 4.8. guarantee_usage\nWith guarantee_usage, you ensure the inclusion and impact of every keyframe in the creation process, affirming the value of each Timestep KeyFrame in the meticulous crafting of AI-generated content.\n\nControlNet Timestep KeyFrames are crucial for precisely directing the AI's creative process, facilitating the creation of narrative or visual journeys with exacting detail. They empower creators to orchestrate the evolution of visuals, especially in animations, from the initial scene to the conclusion, ensuring a cohesive and seamless transition throughout, all while emphasizing the critical role of Timestep KeyFrames in achieving artistic objectives.\n"
    },
    {
        "id": "1033",
        "readme": "## 1. ComfyUI AnimateDiff and Dynamic Prompts (Wildcards) Workflow\n\nThis workflow presents an approach to generating diverse and engaging content. By harnessing the power of Dynamic Prompts, users can employ a small template language to craft randomized prompts through the innovative use of wildcards. With the addition of AnimateDiff and the IP-Adapter, this workflow extends its capabilities to producing dynamic videos or GIFs, tailored to the input images.\n\n## 2. Overview of Dynamic Prompts (Wildcards)\n\n### 2.1. Introduction to Dynamic Prompts (Wildcards)\n\nDynamic Prompts (Wildcards) allow for the dynamic and random generation of prompts through the use of templating languages. This method enables you to create a vast array of prompt variations from a single template by incorporating placeholders that can randomly select from a predefined set of options.\n\nThe basic concept involves defining a template with variable placeholders (wildcards) that are replaced with random selections from specified options. This allows for the generation of a large number of unique prompts, which can be especially useful in creative tasks like generating diverse images, texts, or ideas without manually creating each prompt.\n\nFor instance, a templating language for generating such dynamic prompts might allow you to specify a prompt template like \"A `{color}` `{animal}` in the `{location}`,\" where each placeholder (enclosed in braces) is replaced with a random selection from a list of options for colors, animals, and locations. This could result in a variety of prompts like \"A blue dog in the forest\" or \"A red cat in the city,\" generated from the same template.\n\nAdditionally, this method can include advanced features such as nesting wildcards, using wildcards within wildcards for even more dynamic combinations, setting variables for reuse within a prompt, and even specifying the number of items to select or the use of weights to influence the selection frequency of certain options. These features enable the creation of highly complex and varied prompts with minimal manual effort.\n\nFor more information, check it on [Github](https://github.com/adieyal/sd-dynamic-prompts)\n\n## 3. Overview of AnimateDiff\n\nPlease check out the details on [Overview of AnimateDiff](https://www.runcomfy.com/comfyui-workflows/comfyui-animatediff-controlnet-and-auto-mask-workflow-video2video#2-overview-of-animatediff)\n"
    },
    {
        "id": "1034",
        "readme": "🌟🌟🌟 Update: Check out the latest version of the [ComfyUI Reactor for Face Swap](https://www.runcomfy.com/comfyui-workflows/comfyui-reactor-face-swap-professional-ai-face-animation) workflow. Make sure to use this updated version for even better results! 🌟🌟🌟\n\n## 1. ComfyUI ReActor - Face Swap Workflow\n\nThis ComfyUI workflow is designed for advanced face swapping in images, videos or animations. It operates through nodes like \"ReActorFaceSwap,\" leveraging models such as \"inswapper_128.onnx,\" \"retinaface_resnet50,\" and \"codeformer.pth\" for precise face detection and swapping. This tool allows for swapping faces on both single and multiple characters, supporting different configurations for source and input images, and adjusting parameters like model strength and swap settings. Essential for creating realistic and seamless face swaps in visual content.\n\n## 2. Overview of **ReActor**\n\n### 2.1. Introduction to Comfyui ReActor\n\nComfyUI ReActor introduces a fast and simple face swap extension node, leveraging the capabilities of ReActor SD-WebUI face swap extension. It leverages multiple models to facilitate face detection, face swapping, and face restoration.\n\n### 2.2. Key Features of Comfyui ReActor\n\n**2.2.1. Comprehensive Node System**: ComfyUI ReActor comprises several key nodes\n\n- ReActorFaceSwap (Main Node)**: Facilitates the primary face swapping functionality.\n- ReActorLoadFaceModel: Enables loading of face models.\n- ReActorSaveFaceModel: Allows saving of face models for future use.\n\nThese nodes can be interconnected to execute complex face swapping tasks with ease.\n\n**2.2.2. Versatile Input and Output Compatibility**:\n\n- Inputs: The main node accepts various inputs such as input_image for the target_image and source_image for the face(s) to be swapped.\n- Outputs: Generates two types of outputs - the swapped image and the source face's model (face_model), compatible with any nodes requiring images as inputs or those specific for saving face models.\n\n**2.2.3. Advanced Face Restoration**: ComfyUI ReActor includes built-in face restoration, enhancing the quality of swapped faces by improving detail accuracy. \n\n**2.2.4. Intuitive Face Indexing**: The tool offers a straightforward way to detect and index faces within images, sorting them from left to right and top to bottom. This feature simplifies the process of specifying which faces to swap by setting indexes for both source and input images.\n\n**2.2.5. Gender Specification**: ComfyUI ReActor supports gender-based face swapping, allowing users to define the gender for more precise and relevant face swap results.\n\n**2.2.6. Efficient Face Model Management**: Users can save and load lightweight face models as \"safetensors\" files, enhancing the tool's efficiency and flexibility in managing face models for different scenarios.\n\nFor more information, check it on [Github](https://github.com/Gourieff/comfyui-reactor-node)\n"
    },
    {
        "id": "1035",
        "readme": "## 1. ComfyUI Face Detailer Workflow\n\nThis ComfyUI workflow incorporates the **Impact Pack-Face Detailer** and the **Upscale (4x UltraSharp Model)** to enhance image and video quality. The Face Detailer component refines facial features in images, with a detailed processing pipeline for face detection, facial detail enhancement, and adjustable parameters for specific effects, aimed at face restoration. The 4x UltraSharp Model upscales images to significantly improve resolution without compromising quality, sharpening details and reducing artifacts, ideal for generating high-resolution images and videos. These components form a powerful toolkit for enhancing visual quality in images, videos, and animations, especially for applications requiring facial fixes and overall clarity improvement.\n\n## 2. Overview of Face Detailer\n\n### 2.1. Introduction to FaceDetailer\n\nComfyUI Face Detailer is part of the ComfyUI Impact Pack, a suite of custom nodes that makes enhancing images more efficient and easier than ever through the integration of sophisticated tools like the Detector, Detailer, Upscaler, and Pipe.\n\n**2.1.1. FaceDetailer**: Revolutionize face enhancement with FaceDetailer. This tool excels in detecting faces within images and significantly improving their appearance with precision.\n\n**2.1.2. FaceDetailer (pipe)**: Tailored for multipass operations, FaceDetailer (pipe) maintains the core functionality of detecting and enhancing faces, optimizing the process for complex image refinement tasks.\n\n### 2.2 Key Features of FaceDetailer\n\n**2.2.1. Basic Auto Face Detection and Refinement**: Utilize FaceDetailer for straightforward, automatic face detection and enhancement. This feature is particularly effective in restoring faces marred by low resolution, employing high-resolution synthesis to rejuvenate lost details. Combining Detector and Detailer nodes, FaceDetailer offers an advanced solution for face detection and image enhancement.  The MASK output visualizes the enhancement areas, showcasing the before and after effects vividly.\n\n**2.2.2. 2Pass Refine**: Address severely damaged faces with a 2-pass approach using two FaceDetailers. This setup allows for efficient workflow customization through **FaceDetailer (pipe)**, enabling simple configuration of various common inputs. The initial pass focuses on restoring the basic outline with moderate resolution and settings, while selective dilation helps include adjacent areas for comprehensive face reconstruction.\n\n**2.2.3. Face Bbox + Person Silhouette Segmentation**: This innovative combination prevents background distortion during face enhancement. By utilizing BBoxDetectorForEach for face detection and SAMDetectorCombined for segment identification, this approach ensures detailed enhancements aligned perfectly with face contours, without impacting the surrounding image areas.\n\nFor more information, check it on [GitHub](https://github.com/ltdrdata/ComfyUI-Impact-Pack)\n\n## 3. Overview of 4x-Ultrasharp Upscaler\n\n### 3.1. Introduction to 4x-Ultrasharp Upscaler\n\nThe 4x-Ultrasharp Upscaler stands out for its ability to transform images into higher resolutions with remarkable clarity and detail, addressing the common challenge of detail loss in upscaling processes."
    },
    {
        "id": "1037",
        "readme": "## 1. Background Removal Workflow - BRIA AI RMBG 1.4 vs Segment Anything\n\nThis ComfyUI workflow is engineered to simplify the task of eliminating backgrounds by putting the RMBG 1.4 model from BRIA AI head-to-head with the Segment Anything model. This comparison allows for an effortless evaluation of the outcomes, enabling you to select the superior result for your creative projects.\n\n## 2. Overview of BRIA AI RMBG 1.4\n\nBRIA AI RMBG v1.4 represents a revolutionary advance in background removal technology, carefully crafted to accurately differentiate the foreground from the background across a multitude of image types. Developed with intensive training on a meticulously chosen dataset that includes general stock images, e-commerce, gaming, and advertising content, the model also covers a broad spectrum of subjects, such as objects, individuals, people with objects, and complex scenes involving people, animals, and objects alongside text. This wide-ranging applicability ensures thorough coverage across diverse scenarios, resulting in accuracy, efficiency and versatility.\n\n## 3. Overview of Segment Anything\n\nSegment Anything Model (SAM) is a great AI model from Meta AI designed to \"cut out\" or segment any object within an image with just a single click. What sets SAM apart is its method of generating high-quality object masks based on simple input prompts, such as points or boxes. This feature enables the creation of precise masks for every object within an image, facilitating a wide range of segmentation tasks. The model's effectiveness is underpinned by its training on an extensive dataset comprising 11 million images and 1.1 billion masks. This extensive training has endowed SAM with robust zero-shot performance, making it highly versatile and effective across various segmentation challenges. You can also control the object you want to seperate by prompts input.\n\n## 4. BRIA AI RMBG 1.4 VS. Segment Anything\n\nWhile both models excel at separating foreground elements from the background in images, BRIA AI RMBG appears to handle details better. The most notable difference between the two models lies in the level of user control over the segmentation process. RMBG 1.4 offers users limited options for selecting specific parts of an image for isolation, a feature that is essential for tasks requiring detailed control. Consequently, Segment Anything stands out in applications that demand a high degree of customization and user involvement, offering superior precision and flexibility for those seeking intricate operations in their segmentation projects.\n"
    },
    {
        "id": "1038",
        "readme": "## 1. SVD and IPAdapter Workflow\n\nThe ComfyUI workflow is designed to efficiently blend two specialized tasks into a coherent process. In its first phase, the workflow takes advantage of IPAdapters, which are instrumental in fabricating a composite static image. This is achieved by amalgamating three distinct source images, using a specifically designed mask image as the blueprint for their integration. This process ensures that the resulting image is a precise and harmonious blend of the source materials.\n\nFollowing the creation of the static image, the workflow transitions to its second phase, where it leverages the capabilities of Stable Video Diffusion (SVD). SVD is a cutting-edge technique that is adept at transforming static images into engaging dynamic videos. This transformation is not merely a conversion but an enhancement that imbues the static image with life, creating a seamless video sequence. \n\n## 2. Overview of IP-Adapter\n\nPlease check out the details in the [Introduction to IPAdapter](https://www.runcomfy.com/comfyui-workflows/comfyui-animatediff-and-ipadapter-workflow-stable-diffusion-animation#3-overview-of-ip-adapter)\n\n## 3. Overview of SVD\n\nPlease check out the details on this [Introduction to SVD (Stable Video Diffusion)](https://www.runcomfy.com/comfyui-workflows/comfyui-stable-video-diffusion-svd-workflow-text2video#2-overview-of-stable-video-diffusion-svd)\n"
    },
    {
        "id": "1039",
        "readme": "## 1. Stable Cascade ComfyUI Workflow\n\nIn this ComfyUI workflow, we leverage Stable Cascade, a superior text-to-image model noted for its prompt alignment and aesthetic excellence. Unlike other Stable Diffusion models, Stable Cascade utilizes a three-stage pipeline (Stages A, B, and C) architecture. This design enables hierarchical image compression in a highly efficient latent space, resulting in exceptional image quality.\n\n## 2. Overview of Stable Cascade\n\nStable Cascade emerges as a groundbreaking text-to-image model, leveraging the innovative [Würstchen](https://openreview.net/forum?id=gU58d5QeGv) architecture. This model distinguishes itself through its  higher quality images, faster speeds, lower costs, and easier customization. \n\n### 2.1. A Three-Stage Process Structure\n\n**Stable Cascade Stage A**: Stage A of Stable Cascade utilizes a Vector-Quantized Generative Adversarial Network (VQGAN) to achieve image compression by a factor of four. This stage innovatively quantizes values into one of 8,192 unique entries from a learned codebook, akin to selecting colors from a palette. This quantization not only spatially compresses the image 4:1 but also significantly reduces the data size by representing images with discrete tokens. This method stands in contrast to Stable Diffusion's use of floating point values, offering a more compact and efficient compression technique.\n\n**Stable Cascade Stage B**: Moving forward to Stage B, Stable Cascade showcases its prowess in refining image data. Here, the discrete tokens from Stage A undergo transformation through a latent diffusion model, ingeniously integrating the principles of an IP Adapter with diffusion techniques to guide the creation of similar output images. Stage B shines in its ability to transform tokenized data back into rich, detailed floating-point values, enhancing the image's semantic quality. This stage is designed for efficiency, focusing on creating denoised latents that perfectly match the input, thereby making the training process more streamlined and reducing computational demands.\n\n**Stable Cascade Stage C**: Stage C introduces a novel approach by adding noise to the semantic output from Stage B, then meticulously denoising it using a sequence of ConvNeXt blocks. The aim is to precisely replicate the semantic content, bypassing the need for downsampling. This stage plays a pivotal role in transforming a semantic blob into a coherent piece that Stage B can further refine, culminating in the generation of high-quality images. Stage C's strategic use of ConvNeXt blocks highlights its commitment to delivering top-notch performance efficiently, sidestepping the hefty computational costs typically involved in achieving such advanced results.\n\n### 2.2. Why Stable Cascade Stands Out\n\n**Superior Aesthetic Quality**: Evaluations reveal that Stable Cascade significantly surpasses Stable Diffusion XL in delivering visually stunning images. It achieves 2.5 times the aesthetic quality of SDXL and astonishingly outperforms SDXL Turbo by 5.5 times, showcasing its exceptional capability in producing high-quality visuals.\n\n**Enhanced Inference Speed**: Thanks to its innovative architecture, Stable Cascade offers a more efficient inference process, utilizing resources more effectively than its predecessors. With a remarkable compression factor of 42, it can transform 1024x1024 images into compact 24x24 dimensions. This efficiency does not compromise image quality but rather speeds up the generation process, making it a game-changer for generating images quickly.\n\n**Improved Prompt Understanding**: Stable Cascade also shines in its ability to understand and align with user prompts, whether they are brief or detailed. Human evaluations have demonstrated that it outperforms other models in accurately interpreting prompts, ensuring that the generated images closely match the user's vision.\n"
    },
    {
        "id": "1042",
        "readme": "## 1. ComfyUI InstantID Workflow: Face Sticker Generation\n\nIn this workflow, we utilize InstantID along with IPAdapter Plus Face, making it super easy to keep all those important facial details sharp in your face sticker. Plus, we're using ControlNet depth model to make sure the head pose is just right, keeping everything looking natural in your face sticker.\n\nBy bringing together InstantID and IPAdapter Plus Face, we're making sure every face sticker not only looks great but also stays true to the original expressions and features—so you get a top-quality, durable face sticker every time.\n\nAnd there's more! This setup is super flexible, allowing you to play around with different prompts and unleash your creativity. You can explore a whole range of styles and expressions to create a variety of face stickers. Whether you're after something fun and cartoon-like or more polished and refined, this process lets you customize your face stickers to match any vibe you're going for.\n\n## 2. Overview of InstantID\n\nInstantID is designed to transform a single reference ID image into a variety of customized images, capturing different poses or styles, while maintaining high fidelity to the original. Here's a breakdown of its methodology for a clearer understanding.\n\n**ID Embedding**: This component is crucial for capturing the semantic face information accurately. It ensures that the system understands the key aspects of the face in the reference image, enabling it to preserve the identity across various generated images. \n\n**Lightweight Adapted Module**: This module enhances the system's ability to use the reference image as a visual prompt efficiently. It employs a decoupled cross-attention mechanism, which allows for a focused interpretation of the visual information, aiding in the generation of images that closely adhere to the reference's style and pose.\n\n**IdentityNet**: This network is responsible for encoding the detailed features of the facial image from the reference. It provides additional spatial control, ensuring that the nuances and specific characteristics of the original identity are accurately reflected in the generated images.\n\nOverall, InstantID combines these components to provide a sophisticated yet accessible solution for generating diverse images from a single photo, ensuring consistent representation of the identity's core attributes across all variations. This integration is why InstantID effectively preserves facial identity when creating face stickers.\n\nFor further information on utilizing Instant ID within ComfyUI, please visit [How to use Instant ID in ComfyUI](https://www.runcomfy.com/comfyui-workflows/comfyui-instantid-workflow#2-overview-of-instantid).\n"
    },
    {
        "id": "1044",
        "readme": "## 1. ComfyUI AnimateLCM Workflow\n\nThe ComfyUI AnimateLCM Workflow is designed to enhance AI animation speeds. Building on the foundations of ComfyUI-AnimateDiff-Evolved, this workflow incorporates AnimateLCM to specifically accelerate the creation of text-to-video (t2v) animations. You can experiment with various prompts and steps to achieve desired results.\n\n## 2. Overview of AnimateLCM\n\nThe advent of technologies like SDXL, LCM, and SDXL Turbo has significantly boosted the pace of image generation. AnimateLCM further propels the progress in AI animation. It supports fast image-to-video generation and is aimed at enhancing the speed and efficiency of producing animated videos from static images or text descriptions, making it particularly useful for creating personalized animation effects quickly\n\n### 2.1. Introduction to AnimateLCM\n\nAnimateLCM is designed to accelerate the animation of personalized diffusion models and adapters through decoupled consistency learning. It is inspired by the Consistency Model (CM), which distills pretrained image diffusion models to accelerate the sampling process, and its extension, the Latent Consistency Model (LCM), which focuses on conditional image generation. AnimateLCM leverages these foundations to enable the creation of videos with high fidelity in a few steps, building on the success of image diffusion and generation techniques to expand their capabilities into the video domain.\n\n### 2.2. How to use AnimateLCM in Your ComfyUI Workflow\n\nThis workflow builds on the [ComfyUI-AnimateDiff-Evolved](https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved) Workflow. The following are the configuring parameters in the \"Use Evolved Sampling\" node.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1044/readme01.webp\" alt=\"ComfyUI AnimateLCM workflow\" height=\"512\" />\n\n**Models**: Set checkpoint model and LoRA model.\n- Checkpoint: This pertains to the StableDiffusion (SD) model inputs utilized for animation creation. Depending on the chosen motion models, compatibility may vary across SD versions, such as SD1.5 or SDXL.\n- LoRA: Incorporating the **AnimateLCM LoRA model** to seamlessly integrate existing adapters for a variety of functions, enhancing efficiency and output quality with a focus on consistency learning without sacrificing sampling speed.\n\n**Motion Models (M Models)**: These are the outputs from the Apply AnimateDiff Model process, enabling the use of AnimateLCM Motion Model.\n\n**Context Options**: These settings adjust AnimateDiff's operation during animation production, allowing for animations of any length via sliding context windows across the entire Unet or within the motion module specifically. They also enable timing adjustments for complex animation sequences. Refer here for [a comprehensive guide on AnimateDiff](https://www.runcomfy.com/tutorials/how-to-use-animatediff-to-create-ai-animations-in-comfyui)\n\n**Beta Schedule in Sample Settings**: Opt for LCM. Within AnimateDiff’s Sample Settings in ComfyUI, the \"beta schedule\" selection, including options like \"lcm,\" \"lineart,\" etc., fine-tunes the beta values that regulate noise levels throughout the diffusion process. This customization affects the visual style and fluidity of the animation. Each setting caters to specific animation or motion model needs.\n- LCM (Latent Consistency Module): The \"lcm\" setting is tailored for LCM LoRAs, enhancing animation uniformity and reducing creation time. It achieves quicker convergence with fewer steps, requiring step reduction (to a minimum of ~4 steps) and other parameter adjustments for optimal detail retention.\n- Linear (AnimateDiff-SDXL): Recommended for AnimateDiff with SDXL motion modules, this option strikes a balance between detail preservation and smooth motion, indicating compatibility with specific motion model versions.\n- Sqrt_lineart (AnimateDiff): Similar to \"lineart,\" this variant is designed for AnimateDiff processes, especially with V2 motion modules, modifying noise levels to complement the motion model’s output for more fluid transitions or movements.\n"
    },
    {
        "id": "1047",
        "readme": "## 1. AnimateDiff + Batch Prompt Schedule Workflow\n\nThe AnimateDiff and Batch Prompt Schedule workflow enables the dynamic creation of videos from textual prompts. By allowing scheduled, dynamic changes to prompts over time, the Batch Prompt Schedule enhances this process, offering intricate control over the narrative and visuals of the animation and expanding creative possibilities for storytelling.\n\n## 2. Overview of AnimateDiff\n\nPlease check out the details on [How to use AnimateDiff in ComfyUI](https://www.runcomfy.com/comfyui-workflows/transform-video-into-ceramic-art-style-using-animatediff-controlnet-within-comfyui#2-how-to-use-animatediff)\n\n## 3. Overview of Batch Prompt Schedule\n\n### 3.1. Introduction to Batch Prompt Schedule\n\nThe Batch Prompt Schedule in ComfyUI is a powerful node designed for efficiently managing and scheduling complex prompts across a series of frames or iterations. It enables creators to dynamically adjust text and parameters over time, allowing for detailed control in animation and other time-based media projects. \n\n### 3.2 How to Use Batch Prompt Schedule\n\nFirstly, it's important to note the three text fields for prompts in the Batch Prompt Schedule node: **Initial Text Field, Pretext Field, and Posttext Field.**\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1047/readme01.webp\" alt=\"Batch Prompt Schedule\" width=\"600\"/>\n\nTo compose an **Initial Text Field** in a ComfyUI Batch Prompt Schedule, you essentially define the core or base prompt that will be applied across all frames or iterations in your schedule. This is accomplished using the **Pretext Field** or **Posttext Field**, which respectively allows you to prepend or append text to every scheduled prompt.\n\n**Initial Text Field**: This is the main text input where you enter the prompts you want processed by the Batch Prompt Schedule. Specify the prompt for keyframes here. For example:\n    \n```json\n\"0\": \"Grim Reaper, standing among the graves, carrying a scythe, in a misty graveyard at night, eerie fog, full moon, bats flying around, Halloween pumpkins\",\n\"25\": \"Male zombie, open mouth, eating a brain, in an abandoned post-apocalyptic cityscape, graffiti walls, flickering streetlights\",\n\"50\": \"Werewolf howling, his eyes glowing red, in a dense eerie forest, full bright moon, pack of wolves in the background\"\n```\n**Pretext Field**: Text that is prepended at the beginning of each prompt in the schedule, allowing for a consistent base across all scheduled prompts.\n\n**Posttext Field**: Text that is appended at the end of each prompt in the schedule, enabling a uniform conclusion to each prompt or adding consistent elements across prompts.\n\nAdditionally, there are some other parameters to help control the prompts:\n\n**Max Frames**: Determines the maximum number of frames for which the batch prompt schedule is applied. It is crucial to align this number with the number of latents in your batch or the total animation frames you intend to produce.\n\n**Print Output**: A boolean parameter that, when enabled, allows for the output to be printed for debugging or verification purposes.\n\n**Start Frame**: Specifies the starting frame for the batch prompt schedule.\n\n**PW Fields**: These fields (`pw_a`, `pw_b`, `pw_c`, `pw_d`) facilitate the dynamic adjustment of prompt weights over time. By manipulating these weights through expressions and linking them to other scheduled values, creators gain nuanced control over how different aspects of the prompt influence the batch schedule's output. For instance, you can incorporate `pw_a`, `pw_b`, `pw_c`, and `pw_d` into the Initial Text Field as follows:\n    \n```json\n\"0\": \"A joyful dancer in the spotlight, smiling broadly (happiness:`pw_a`)\",\n\"24\": \"A contemplative dancer under the moonlight, lost in thought (contemplation:`pw_b`)\",\n\"48\": \"A sorrowful figure in the shadows, tears glistening in the moonlight (sorrow:`pw_c`)\"\n```\n\n\nThis workflow is inspired by [MDMZ](https://www.youtube.com/@MDMZ). For more information, please visit his YouTube channel.\n"
    },
    {
        "id": "1048",
        "readme": "## 1. ComfyUI Workflow: AnimateDiff + ControlNet + IPAdapter | Flat Anime Style\n\nThis ComfyUI workflow utilizes AnimateDiff, ControlNet featuring on Depth, Softedge, etc., IPAdapter, and FaceRestore to transform original video content into a distinctive Flat Anime Style. After obtaining the result, you can activate the upscale nodes to enhance your video's resolution.\n\n## 2. Overview of AnimateDiff\n\nPlease check out the details on [How to use AnimateDiff in ComfyUI](https://www.runcomfy.com/comfyui-workflows/transform-video-into-ceramic-art-style-using-animatediff-controlnet-within-comfyui#2-how-to-use-animatediff)\n\n## 3. Overview of ControlNet\n\nPlease check out the details on [How to use ControlNet in ComfyUI](https://www.runcomfy.com/comfyui-workflows/convert-video-to-japanese-anime-style-using-animatediff-controlnet-ipadapter-in-comfyui#3-how-to-use-controlnet)\n\n## 4. How to Use Face Restore\n\n\"FaceRestore\" in ComfyUI is a custom extension designed for restoring faces in images. It leverages the capabilities of the CodeFormer model to enhance image fidelity. Here are the detailed explanations.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1048/readme01.webp\" alt=\"Face Restore Model in ComfyUI\" width=\"450\"/>\n\n### 4.1. Input of \"Face Restore CF With Model\" node\n\n**facerestore_model**: Specify the face restoration model to use. This is essential for defining the algorithm that will be applied to enhance the faces in your images.\n\n**image**: This is the input image that contains faces you wish to restore. The node will process this image and apply face restoration on detected faces.\n\n**facedetection**: Choose the face detection model from the following options. This model is responsible for identifying and cropping faces from the input image: Each of these options has its strengths, with some being more accurate while others are faster or lighter in terms of computational resources required.\n\n- retinaface_resnet50\n- retinaface_mobile0.25\n- YOLOv5l\n- YOLOv5n\n\n**codeformer_fidelity (FLOAT)**: A critical parameter that allows you to adjust the fidelity of the CodeFormer model. This setting determines the balance between restoring the face with high fidelity to the original and enhancing the image. A higher value might retain more original features, while a lower value may result in a more 'idealized' restoration.\n\n### 4.2. Output of \"Face Restore CF With Model\" node\n\n**IMAGE**: The output is the processed image where the faces have been restored. This image is the result of the face restoration process, showcasing enhanced clarity, details, and overall improved visual quality of the faces detected in the input image.\n"
    },
    {
        "id": "1050",
        "readme": "## 1. ComfyUI Workflow: AnimateDiff + ControlNet | Ceramic Art Style\n\nThis workflow utilizes AnimateDiff, ControlNet focusing on depth, and specific Lora to skillfully transform videos into a Ceramic Art Style. You are encouraged to use different prompts to achieve various art styles, turning your ideas into reality.\n\n## 2. How to Use AnimateDiff\n\nAnimateDiff is designed to animate static images and text prompts into dynamic videos, leveraging Stable Diffusion models and a specialized motion module. It automates the animation process by predicting seamless transitions between frames, making it accessible to users without coding skills .\n\n### 2.1 AnimateDiff Motion Modules\n\nTo begin, select the desired AnimateDiff motion module from the model_name dropdown:\n\n- Use v3_sd15_mm.ckpt for AnimateDiff V3\n- Use mm_sd_v15_v2.ckpt for AnimateDiff V2\n- Use mm_sdxl_v10_beta.ckpt for AnimateDiff SDXL\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1050/readme01.webp\" alt=\"AnimateDiff Motion Modules in ComfyUI\" width=\"750\"/>\n\n### 2.2 Beta Schedule\n\nThe Beta Schedule in AnimateDiff is crucial for adjusting the noise reduction process throughout animation creation.\n\nFor versions V3 and V2 of AnimateDiff, the sqrt_linear setting is recommended, although experimenting with the linear setting can yield unique effects.\n\nFor AnimateDiff SDXL, the linear setting (AnimateDiff-SDXL) is advised.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1050/readme02.webp\" alt=\"AnimateDiff Beta Schedule in ComfyUI\" width=\"750\"/>\n\n### 2.3 Motion Scale\n\nThe Motion Scale feature in AnimateDiff allows for the adjustment of motion intensity in your animations. A Motion Scale under 1 results in more subtle motion, whereas a scale over 1 amplifies movement.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1050/readme03.webp\" alt=\"AnimateDiff Motion Scale in ComfyUI\" width=\"750\"/>\n\n### 2.4 Context Length\n\nThe Uniform Context Length in AnimateDiff is essential for ensuring seamless transitions between scenes defined by your Batch Size. It acts like an expert editor, seamlessly connecting scenes for fluid narration. Setting a longer Uniform Context Length ensures smoother transitions, while a shorter length offers quicker, more distinct scene changes, beneficial for certain effects. The standard Uniform Context length is set to 16.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1050/readme04.webp\" alt=\"AnimateDiff Context Length in ComfyUI\" width=\"750\"/>\n\n### 2.5 Utilizing Motion LoRA for Enhanced Camera Dynamics (Specific to AnimateDiff v2)\n\nMotion LoRAs, compatible solely with AnimateDiff v2, introduce an additional layer of dynamic camera movement. Achieving the optimal balance with the LoRA weight, typically around 0.75, ensures smooth camera motion free from background distortions.\n\nMoreover, chaining various Motion LoRA models allows for complex camera dynamics. This enables creators to experiment and discover the ideal combination for their animation, elevating it to a cinematic level.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1050/readme05.webp\" alt=\"AnimateDiff Motion LoRA in ComfyUI\" width=\"750\"/>\n\n## 3. How to Use ControlNet\n\nControlNet enhances image generation by introducing precise spatial control to text-to-image models, allowing users to manipulate images in sophisticated ways beyond just text prompts, by utilizing vast libraries from models like Stable Diffusion for intricate tasks such as sketching, mapping, and segmenting visuals.\n\nThe following is the simplest workflow using ControlNet.\n\n### 3.1 Loading the \"Apply ControlNet\" Node\n\nStart your image crafting by loading the \"Apply ControlNet\" Node in ComfyUI, setting the stage for combining visual and textual elements in your design.\n\n### 3.2 Inputs of \"Apply ControlNet\" Node\n\nUse Positive and Negative Conditioning to shape your image, select a ControlNet model to define style traits, and preprocess your image to ensure it matches the ControlNet model requirements, thereby making it ready for transformation.\n\n### 3.3 Outputs of \"Apply ControlNet\" Node\n\nThe node outputs guide the diffusion model, offering a choice between refining the image further or adding more ControlNets for enhanced detail and customization based on the interaction of ControlNet with your creative inputs.\n\n### 3.4 Tuning \"Apply ControlNet\" for Best Results\n\nControl the influence of ControlNet on your image through settings like Determining Strength, Adjusting Start Percent, and Setting End Percent to finely tune the image's creative process and outcome.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1050/readme06.webp\" alt=\"ControlNet in ComfyUI\" width=\"750\"/>\n\nFor more detailed information, please check out [How to use ControlNet in ComfyUI](https://www.runcomfy.com/comfyui-workflows/convert-video-to-japanese-anime-style-using-animatediff-controlnet-ipadapter-in-comfyui)\n\n\nThis workflow is inspired by [MDMZ](https://www.youtube.com/@MDMZ) with some modifications. For more information, please visit his YouTube channel.\n"
    },
    {
        "id": "1051",
        "readme": "## 1. ComfyUI Workflow: AnimateDiff + ControlNet | Marble Sculpture Style\n\nThis workflow leverages AnimateDiff, ControlNet with an emphasis on depth, and specialized Lora techniques to masterfully convert videos into a Marble Sculpture Style. You're invited to experiment with various prompts to explore different artistic styles, bringing your creative visions to life.\n\n## 2. Overview of AnimateDiff\n\nPlease check out the details on [How to use AnimateDiff in ComfyUI](https://www.runcomfy.com/comfyui-workflows/transform-video-into-ceramic-art-style-using-animatediff-controlnet-within-comfyui#2-how-to-use-animatediff)\n\n## 3. Overview of ControlNet\n\nPlease check out the details on [How to use ControlNet in ComfyUI](https://www.runcomfy.com/comfyui-workflows/convert-video-to-japanese-anime-style-using-animatediff-controlnet-ipadapter-in-comfyui#3-how-to-use-controlnet)\n\n\nThis workflow is inspired by [MDMZ](https://www.youtube.com/@MDMZ) with some modifications. For more information, please visit his YouTube channel.\n"
    },
    {
        "id": "1052",
        "readme": "## 1. Jellyfish Ballerina Workflow\n\nThis workflow creates an enchanting animation of a ballerina dancing gracefully, mimicking the movements of a jellyfish underwater. Developed by [Matt3o](https://github.com/cubiq/ComfyUI_IPAdapter_plus), this captivating project is showcased in his [YouTube](https://www.youtube.com/watch?v=_f-jv311w-g), providing ample inspiration.\n\nWe break down the essential components of this workflow to make it easy for you to grasp. Plus, you see how you can apply this method to create a variety of animated artworks, broadening your creative scope.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1052/readme01.webp\" alt=\"ComfyUi Jellyfish Ballerina Workflow\" width=\"750\"/>\n\n## 2. Prepare Input\n\nThe input materials include a video of a ballerina and a photo of a jellyfish as the starting points. The aim is to craft an animation in which the ballerina's movements echo those of a jellyfish gliding through water.\n\n## 3. IPAdapter: Implementing Three IPAdapters\n\nThis workflow incorporates three distinct IPAdapters, each with a unique role.\n\nFirstly, it's crucial to understand that IPAdapters play a significant role in refining and enhancing animations. They add depth, detail, and stability. However, a notable limitation is their requirement for input images to be square (e.g., 224x224 pixels). This constraint does not align well with all types of video content, particularly those in portrait or landscape formats.\n\n**To navigate this, the workflow employs two IPAdapters for the Upper and Lower Body of the ballerina.** This strategy overcomes the input size limitation by allowing independent resizing of each body section into the square format, preserving detail and proportion.\n\n**First IPAdapter: Stabilizing Upper Body and Head Movements**\nThe initial IPAdapter is dedicated to refining the upper body and head animations of the ballerina, ensuring they are expressed with clarity and grace.\n\n**Second IPAdapter: Refining Lower Body Movements**\nThe second IPAdapter focuses on the ballerina's lower body, including the legs, enhancing fluidity and elegance to match the upper body's movements.\n\n**Third IPAdapter: Integrating the Jellyfish Element**\nThe final IPAdapter introduces the jellyfish motif, acting as a thematic anchor for the entire animation. It employs a jellyfish image to imbue the animation with the creature's ethereal and flowing qualities, merging them seamlessly with the ballerina's dance.\n\n## 4. Mask: Eliminating the Background\n\nAn important detail in the creation of this artwork is dealing with the potential distractions from the ballerina's original dance background. To tackle this, masks are created to isolate the ballerina from the rest of the video. Advanced segmentation preprocessors are employed, which effectively isolate the dancer, ensuring a more focused and precise animation.\n\n## 5. ControlNet: Ensuring Pose Accuracy\n\nGetting the ballerina's pose just right is crucial. This is where ControlNet comes in, mimicking the dancer's movements with precision to keep the animation in line with the intended choreography. It is instrumental in capturing the essence of the dancer's movements, ensuring each frame reflects the grace and precision of a real-life ballerina.\n\n## 6. AnimateDiff: Ensuring Fluidity\n\nIn the Jellyfish Ballerina project, AnimateDiff is crucial for crafting smooth transitions between frames, mirroring the seamless flow of a jellyfish's movements. It plays a vital role in ensuring the animation's fluidity, making each motion blend effortlessly into the next and capturing the essence of elegance in motion.\n\nLeveraging the components outlined above will enable you to recreate the mesmerizing dance of a ballerina as a jellyfish. Experimenting with different input videos and reference images will further unlock creative possibilities in motion art creation!\n"
    },
    {
        "id": "1053",
        "readme": "## 1. ComfyUI Workflow：AnimateDiff + AutoMask + ControlNet | Visual Effects (VFX)\n\nThis workflow harnesses the capabilities of AnimateDiff, ControlNet, and AutoMask to create stunning visual effects with precision and ease. The core of this process lies in the strategic use of AutoMask, which plays a crucial role in defining and isolating the specific area for the visual transformation.\n\n## 2. Precise Transformations with AutoMask\n\nThe initial step in creating detailed and impactful visual effects, such as transforming a human face into a zombie, involves the use of AutoMask. AutoMask plays a crucial role in this process by accurately identifying and isolating the specific area for enhancement. For effects that demand a high degree of realism and precision, such as morphing into a zombie, focusing the transformation on the facial area is essential. AutoMask excels in this by generating a highly accurate mask that envelops the face. This precision ensures that the transformative effect is confined strictly to the face, maintaining the integrity of the surrounding areas of the image without alteration. In this workflow, we use the BatchCLIPSeg Node and the GrowMaskWithBlur node to realize it automatically.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1053/readme01.webp\" alt=\"Mask in ComfyUI\" width=\"750\"/>\n\n## 3. Enhancing Realism with the Fade Effect\n\nTo elevate the visual fidelity of the transformation and ensure a natural transition between the applied effects and the original footage, incorporating a Fade effect is paramount. This technique is designed to blend the edges of the visual effects seamlessly into the untouched parts of the video, creating a gradual shift rather than an abrupt change. Implementing the Fade effect contributes to the realism of the transformation, making the transition from human to zombie (or any other effect) appear more fluid and believable. By carefully adjusting the intensity and duration of the Fade, you can achieve a perfect balance that enhances the overall impact of the visual transformation, providing viewers with a captivating and immersive experience.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1053/readme02.webp\" alt=\"Fade Effect in ComfyUI\" width=\"750\"/>\n\nThis workflow also uses AnimateDiff and ControlNet; for more information about how to use them, please check the following link.\n\n## 4. Overview of AnimateDiff\n\nPlease check out the details on [How to use AnimateDiff in ComfyUI](https://www.runcomfy.com/comfyui-workflows/transform-video-into-ceramic-art-style-using-animatediff-controlnet-within-comfyui#2-how-to-use-animatediff)\n\n## 5. Overview of ControlNet\n\nPlease check out the details on [How to use ControlNet in ComfyUI](https://www.runcomfy.com/comfyui-workflows/convert-video-to-japanese-anime-style-using-animatediff-controlnet-ipadapter-in-comfyui#3-how-to-use-controlnet)\n\nThis workflow is inspired by **[Mickmumpitz](https://www.youtube.com/@mickmumpitz).** For more information, please visit his YouTube channel.\n"
    },
    {
        "id": "1057",
        "readme": "## 1. ComfyUI Workflow: Mesh Graphormer ControlNet | Fix Hands\n\nEnhancing the realism of hands in AI-generated images can be a challenge. However, the introduction of the MeshGraphormer ControlNet technique has greatly streamlined this task. This method specifically targets the common issues associated with the depiction of hands in AI-generated artwork, ensuring they are represented accurately and appear lifelike.\n\nCentral to this approach is a node called \"MeshGraphormer Hand Refiner.\" This node is crucial in generating a depth map that outlines where the hands should be positioned and how they should look, correcting the initial image's inaccuracies.\n\nThe depth map acts as a critical preprocessing step for the ControlNet. Once the depth map has been created, it is utilized in tandem with a specific \"inpainting model\" within the ControlNet to refine the hands' appearance in detail.\n\nSimultaneously, the \"MeshGraphormer Hand Refiner\" node also produces an exact mask. This mask operates like a targeted overlay that singles out the hands, ensuring that refinements are applied solely to the hands, thereby preventing any unintended alterations to the rest of the image. Consequently, while the overall image remains intact, the hands are given the focused attention they need to achieve a natural and believable appearance.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1057/readme01.webp\" alt=\"Mesh Graphormer ControlNet\" width=\"750\"/>\n"
    },
    {
        "id": "1058",
        "readme": "## 1. ComfyUI Outpainting Workflow\n\nThis image outpainting workflow is designed for extending the boundaries of an image, incorporating four crucial steps:\n\n### 1.1. ComfyUI Outpainting Preparation: \nThis step involves setting the dimensions for the area to be outpainted and creating a mask for the outpainting area. It's the preparatory phase where the groundwork for extending the image is laid out.\n\n### 1.2. ComfyUI Outpainting Process (Use Inpainting ControlNet model): \nThe actual outpainting process is executed through the inpainting model, specifically using ControlNet's inpainting module. In this phase, only the region designated by the previously created mask is addressed. This approach utilizes the inpainting model to generate the additional content required for the outpainting area. It’s crucial to understand that although we are extending the image (outpainting), the technique applied is derived from inpainting methodologies, governed by the ControlNet module that intelligently fills in the designated area based on the context provided by the surrounding image.\n\n### 1.3. ComfyUI Outpainting Initial Output: \nHere we obtain the initial version of the image with the newly outpainted area. This stage showcases how the inpainting model has extended the image boundaries. However, at this point, there may be noticeable distinctions between the edges of the original image and the newly extended parts. So the subsequent step is crucial for repair it.\n\n### 1.4. ComfyUI Outpainting Edge Repair: \nThe final step focuses on refining the integration between the original image and the newly added sections. This involves specifically targeting and enhancing the edges to ensure a seamless transition between the original and extended parts of the image.\n\n## 2. Detailed Introduction to ComfyUI Outpainting/Inpainting Process\n\n### 2.1. ComfyUI Outpainting Preparation\n\nHere are the key nodes involved in this step:\n\n**2.1.1. Image Scale to Side**: Scale images based on specified parameters. You can set a target side length and choose which side (longest, width, or height) to scale. It offers several scaling methods (nearest-exact, bilinear, area) and an optional crop feature for maintaining aspect ratio.\n- Side Length: Define the target side length for scaling\n- Side: Choose the side of the image to scale (longest, width, or height)\n- Upscale Method: Select the preferred method for scaling\n- Crop: Enable cropping to maintain the original image's aspect ratio during scaling\n\n**2.1.2. Pad Image for Outpainting**: Prepares images for outpainting by adding padding around the borders. This node allows specification of padding amounts for each side of the image and includes a \"feathering\" option to seamlessly blend the original image into the padded area.\n\n**2.1.3. Convert Image to Mask**: Transforms a selected channel (red, green, blue, alpha) of an image into a mask, isolating a portion of the image for processing.\n\nIn this phase, the padded and masked images are prepared.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1058/readme01.webp\" alt=\"ComfyUI Inpainting ControlNet\" width=\"700\"/>\n\n### 2.2. ComfyUI Outpainting Process (Use Inpainting ControlNet model)\n\nHere are the key nodes involved in this step:\n\n**2.2.1. Apply Advanced ControlNet**: Apply the ControlNet node to meticulously guide the inpainting process, targeting the area outlined by the mask prepared in the first step. \n\n**2.2.2. Load ControlNet Model**: Selects and loads the inpainting ControlNet model.\n\n**2.2.3. Inpainting Preprocessor**: Send the padded and masked images, which were prepared in the first step, to the inpainting preprocessor.\n\n**2.2.4. Scaled Soft Weights**: Adjusts the weights in the inpainting process for nuanced control, featuring parameters like base_multiplier for adjusting weight strength and flip_weights to inverse the effect of weights.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1058/readme02.webp\" alt=\"ComfyUI Outpainting Preparation\" width=\"700\"/>\n\n### 2.3. ComfyUI Outpainting Initial Output\n\nAt this stage, the initial outpainted image is generated. However, noticeable edges around the original image may be visible.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1058/readme03.webp\" alt=\"ComfyUI Outpainting Initial Output\" width=\"700\"/>\n\n### 2.4. ComfyUI Outpainting Repair Edge\n\nThis final step involves masking the edge area for regeneration, which improves the overall look of the outpainted area.\n\nHere are the essential nodes involved in incorporating noticeable edges into the mask:\n\n**2.4.1. Mask Dilate Region**: Expands the mask's boundaries within an image, useful for ensuring complete coverage or creating a larger boundary for processing effects.\n\n**2.4.2. Mask Contour**: Involves identifying and outlining the edges within a mask, aiding in the distinction between different elements in an image.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1058/readme04.webp\" alt=\"ComfyUI Outpainting Repair Edge\" width=\"700\"/>\n\n\n<p> This workflow is inspired by <a href=\"https://openart.ai/workflows/hornet_splendid_53/extended-image-no-prompts-needed/jA3L4h2zsZaaZ4qaG4jA\" target=\"_blank\" rel=\"nofollow noopener  noreferrer\">Ning</a></p>\n\n"
    },
    {
        "id": "1063",
        "readme": "## 1. Consistent Character **Workflow**\n\nThis workflow is all about crafting characters with a consistent look, leveraging the IPAdapter Face Plus V2 model. Simply start by uploading some reference images, and then let the Face Plus V2 model work its magic, creating a series of images that maintain the same facial features. Feel free to mix things up with different checkpoints or LoRA models to explore a variety of styles, all while keeping your character's appearance consistent.\n\n## 2. Overview of IPAdapter FaceID/FaceID Plus\n\n### v1.5 FaceID\n\nThis model is the base version for face identification, allowing for variations augmented by text prompts, control nets, and masks. It's noted for its average strength in conditioning, making it suitable for general face conditioning tasks. The base FaceID model does not utilize a CLIP vision encoder, which implies a simpler setup without the need for complex encoder configurations.\n\n### v1.5 FaceID Plus\n\nThe FaceID Plus model is a more potent variant, designed for stronger image-to-image conditioning effects. It requires the use of the **ViT-H image encoder**, indicating its need for higher processing capabilities for detailed face modeling.\n\n### v1.5 FaceID Plus v2\n\nAn iteration over the FaceID Plus, this model introduces enhancements for even more detailed face conditioning. Similar to FaceID Plus, it utilizes the **ViT-H image encoder**. This model aims at providing an increased quality in face modeling, catering to more nuanced requirements.\n\n### v1.5 FaceID Portrait\n\nDesigned specifically for portraits, this model does not use a CLIP vision encoder. It focuses on generating high-quality facial images within portrait settings, potentially offering a specialized approach for portrait image generation.\n\n### SDXL FaceID\n\nThe SDXL variant of FaceID is tailored for use with the SDXL architecture, not employing a CLIP vision encoder. It represents a base model within the SDXL suite, designed for scalable deep learning architectures, focusing on face identification tasks.\n\n### SDXL FaceID Plus v2\n\nThis is a stronger version of the FaceID model for the SDXL architecture, utilizing the **ViT-H image encoder**. It's designed to offer enhanced face conditioning effects within the SDXL framework, aimed at high-quality image generation tasks.\n\n## 3. How to use IPAdapter FaceID/FaceID Plus\n\n### 3.1. Choose FaceID/FaceID Plus model\n\nSelect your preferred FaceID or FaceID Plus model to start crafting your images. Within the settings, you'll find options to adjust both the weights and the noise. These adjustments are key to fine-tuning the appearance of your generated images, allowing you to achieve the precise look you're aiming for.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1063/readme01.webp\" alt=\"FaceID Plus Model\" height=\"800\"/>\n\n### 3.2. Preparing the reference image\n\nWhen using IPAdapter FaceID nodes, the CLIP vision model processes your reference image by resizing and centering it to a dimension of 224x224 pixels. This automatic adjustment focuses on the image's center, making it crucial for the main subject of your image, like a character's face, to be positioned centrally. If the subject is off-center, especially in portrait or landscape images, the results might not meet your expectations. **For best outcomes, it's highly recommended to use square images with the subject centered.**\n"
    },
    {
        "id": "1064",
        "readme": "## 1. ComfyUI Portrait Master Workflow\n\nThis workflow uses SDXL checkpoints and the Portrait Master node for simple portrait generation. With Portrait Master node verssion 2.3, you have enhanced control in creating portraits, allowing you to produce images that are either realistic or stylized according to your needs.\n\n## 2. Overview of Portrait Master\n\nThe Portrait Master node within ComfyUI acts as an alternative to traditional prompt-based image generation. It streamlines the portrait creation process by enabling direct manipulation of key parameters such as pose, expression, lighting, and background, eliminating the reliance on complex textual prompts.\n\n## 3. Available Options of Portrait Master\n\n- **shot**: Specifies the type of camera shot.\n- **shot_weight**: Determines the importance or influence of the shot type.\n- **gender**: Indicates the character's gender.\n- **androgynous**: Adjusts the character's appearance to be more androgynous, with a value representing the extent of the adjustment.\n- **age**: Specifies the character's age.\n- **nationality_1**: Chooses the first ethnicity of the character.\n- **nationality_2**: Chooses the second ethnicity of the character.\n- **nationality_mix**: Blends the first and second ethnicities, using the format [nationality_1: nationality_2: nationality_mix]. Note: This format is not directly supported by ComfyUI; use [comfyui-prompt-control](https://github.com/asagi4/comfyui-prompt-control) for better integration. This feature is experimental.\n- **body_type**: Determines the character's body shape.\n- **body_type_weight**: Sets the prominence of the chosen body type.\n- **model_pose**: Selects the character's pose from a predefined list.\n- **eyes_color**: Sets the color of the eyes.\n- **eyes_shape**: Chooses the shape of the eyes.\n- **lips_color**: Defines the color of the lips.\n- **lips_shape**: Chooses the shape of the lips.\n- **makeup**: Applies makeup to the character.\n- **clothes**: Dresses the character.\n- **facial_expression** / **facial_expression_weight**: Applies and fine-tunes the character's facial expression.\n- **face_shape** / **face_shape_weight**: Applies and adjusts the shape of the character's face.\n- **facial_asymmetry**: Adjusts the asymmetry of the face.\n- **hair_color**: Sets the color of the hair.\n- **hairs_style**: Chooses a hairstyle.\n- **hairs_length**: Selects the length of the hair.\n- **disheveled**: Adds a tousled effect to the hair.\n- **natural_skin**: Enhances the skin to appear more natural.\n- **bare_face**: Adjusts the level of makeup or lack thereof to achieve a bare-faced look.\n- **washed_face**: Adds a fresh, clean look to the face.\n- **dried_face**: Simulates a dry skin appearance.\n- **skin_details**: Adjusts the level of detail visible on the skin.\n- **skin_pores**: Controls the visibility of skin pores.\n- **dimples**: Adds or adjusts the presence of facial dimples.\n- **freckles**: Controls the appearance of freckles.\n- **moles**: Adds moles to the skin.\n- **skin_imperfections**: Introduces imperfections to the skin.\n- **eyes_details**: Adjusts the level of detail in the eyes.\n- **iris_details**: Fine-tunes the details of the iris.\n- **circular_iris**: Enhances the iris to be more circular.\n- **circular_pupil**: Makes the pupil more circular.\n- **light_type**: Sets the overall lighting.\n- **light_direction**: Specifies the direction of lighting. This feature is experimental.\n- **photorealism_improvement**: An experimental setting to enhance photorealism.\n- **prompt_start**: Inserts a specific text at the beginning of the prompt.\n- **prompt_additional**: Inserts text at a midpoint in the prompt.\n- **prompt_end**: Adds text at the end of the prompt.\n- **negative_prompt**: Integrates a negative prompt for finer control based on settings.\n- **style_1** / **style_1_weight**: Applies and adjusts a specific style.\n- **style_2** / **style_2_weight**: Applies and adjusts a second style.\n- **random_**: Toggles the randomization of some options.\n\nNote: Parameters set to a null value or 0.00 will not be included in the generated prompt. The randomizer option, when enabled, ignores manually entered values, opting for random selection instead. This node generates two outputs: a positive and a negative prompt, based on the settings provided.\n\n## 4. Usage Tips of Portrait Master\n\n**Experiment with Different Models**: Given the varied styles and qualities of portraits produced by different models, experimenting with them is crucial to identify the most suitable one for your intended outcome.\n\n**Customizing Attributes**: The Portrait Master node provides numerous customizable attributes including shot type, gender, nationality, body type, model pose, eye color, facial expression, hairstyle, and lighting. Setting these attributes to randomize can yield a wide range of results with each iteration.\n\n**Fine-Tuning the Output**: Detailed adjustments are possible with Portrait Master, including light direction and facial details. This level of manipulation helps you fine-tune the portrait to their exact preferences.\n\n**Using Text for Additional Customization**: For more advanced customization, you can input text directly to specify or refine features that are not available through the pre-defined attributes. This feature adds an extra layer of personalization to the portrait creation process.\n\nFor more information, check it on [github](https://github.com/florestefano1975/comfyui-portrait-master)\n"
    },
    {
        "id": "1067",
        "readme": "## 1. ComfyUI SDXL Turbo Workflow\n\nSDXL Turbo synthesizes image outputs in a single step and generates real-time text-to-image outputs. The quality of SDXL Turbo is relatively good, though it may not always be stable. To enhance results, incorporating a face restoration model and an upscale model for those seeking higher quality outcomes.\n\n## 2. Overview of SDXL Turbo\n\nSDXL Turbo is a generative text-to-image model that efficiently converts text prompts into photorealistic images in just one network evaluation. Leveraging a technique called Adversarial Diffusion Distillation (ADD), developed by Stability AI, it drastically shortens the image synthesis process to 1 to 4 steps—far fewer than the traditional 50 steps required by earlier models. This model, an advancement from SDXL 1.0, utilizes ADD to merge score distillation with an adversarial loss, optimizing the use of existing image diffusion models for higher quality with fewer sampling steps. The introduction of this distillation technique not only preserves image quality but also significantly cuts down on the computational effort needed for image generation.\n\n## 3. Limitations of SDXL Turbo\n\nDespite its advanced capabilities, SDXL Turbo has certain limitations. It generates images at a fixed resolution of 512x512 pixels and may struggle with rendering legible text, accurately depicting faces and people, and achieving perfect photorealism. These constraints underscore the model's intended use for research and exploration rather than factual or accurate representations of real-world entities.\n"
    },
    {
        "id": "1069",
        "readme": "## 1. ComfyUI Workflow: Face Restore + ControlNet + Reactor |  Restore Old Photos\n\nIn this workflow, transform your faded pictures into vivid memories involves a three-component approach: Face Restore, ControlNet, and ReActor. Face Restore sharpens and clarifies facial features, while ControlNet, incorporating OpenPose, Depth, and Lineart, offers precise control over the image generation for detailed scene reconstruction. OpenPose fixes poses, Depth adds three-dimensionality, and LineArt enhances outlines. Finally, the ReActor node specializes in face swaps, improving face detail and accuracy in restored photos, completing the restoration process with enhanced realism and fidelity to the original scene.\n\nFor more details about ControlNet, please refer to [How to use ControlNet in ComfyUI](https://www.runcomfy.com/comfyui-workflows/convert-video-to-japanese-anime-style-using-animatediff-controlnet-ipadapter-in-comfyui)\n\nFor more details about ReActor for Face Swap, please refer to [How to use Reactor for Face Swap in ComfyUI](https://www.runcomfy.com/comfyui-workflows/comfyui-reactor-workflow-fast-face-swap)\n"
    },
    {
        "id": "1070",
        "readme": "## 1. ComfyUI InstantID Workflow\n\nThe ComfyUI workflow enables the creation of images featuring two individuals by using two InstantIDs. To focus on a single person, simply omit one of the InstantIDs, upload a new reference image, and dispense with the mask to craft a fresh image. In the ensuing sections, we'll delve into the specific parameters to guide you in utilizing InstantID. This approach will help you generate captivating, stylized portraits that not only embody the individuals’ essence but also align with your artistic vision.\n\n## 2. Overview of InstantID\n\nInstantID is a groundbreaking style transfer model designed to enhance and transform people's portraits with a high degree of accuracy and aesthetic appeal. The ComfyUI InstantID node, created by the developer of the ComfyUI IPAdapter nodes, integrates seamlessly into the ComfyUI ecosystem. It offers users an advanced tool for artistic expression and digital identity creation.\n\n### 2.1. Introduction to InstantID\n\nInstantID transforms photographs into stylized portraits, it operates with an SDXL checkpoint and is designed to work best with images at a resolution of 1024x1024. That said, you can experiment with various sizes to find what works best for your project. Here are some recommended resolutions that also work well with SDXL, which you might want to try:\n\n- 1024x1024 (square)\n- 1152x896 (wider)\n- 896x1152 (taller)\n- 1216x832 (wider)\n- 832x1216 (taller)\n- 1344x768 (wider)\n- 768x1344 (taller)\n- 1536x640 (wider)\n- 640x1536 (taller)\n\nInstantID combines a ControlNet with a face ID model. This duo ensures that the style transfer not only captures your subject's details accurately but also applies the desired artistic effects effectively.\n\n### 2.2. Key Parameters for Using InstantID\n\nWhen using the \"Apply InstantID\" feature, you'll encounter several important settings:\n\n**instantid**: This selects the specific InstantID model for your image creation.\n\n**insightface**: This is a model used for recognizing and extracting features from faces in your reference image, helping to capture the essence of the portrait.\n\n**control net**: Works with the face ID model to encode the detailed features from your reference, enhancing the image's style and realism.\n\n**image**: The reference image you use, ideally a clear portrait that fits within a 640x640 box for the best style transfer results.\n\n**image_kps**: Short for image keypoints, these are used to define the facial orientation in your generated image, allowing for pose customization based on your reference image. You can even use a different image for this if you want to change the head position but keep the facial features.\n\n**mask**: This parameter lets you pinpoint which parts of your image should be influenced by InstantID. This can be particularly useful for focusing the identity-preserving aspects on specific parts of the image, such as the face, while allowing the rest of the image to be generated according to other conditions or prompts.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1070/readme01.webp\" alt=\"ComfyUI Apply InstantID\" height=\"700\"/>\n\n### 2.3. Additional Settings for Using InstantID\n\n**Prompt**: You can input specific prompts to guide the style of your creation, such as asking for an oil painting style, a comic book style, etc.\n\n**CFG in KSampler**: It is suggested to lower the CFG settings for better results; CFG 4.5 is recommended.\n\n**Noise Injection**: The standard InstantID might overly enhance your image, but you can soften this by adding noise to the negative embeds, improving likeness. Use the Advanced InstantID node for more precise adjustments.\n\n**Extra ControlNets**: You can Enhance your creation by incorporating additional ControlNets into the process.\n\nFor more information, check it on [Github](https://github.com/cubiq/ComfyUI_InstantID)\n"
    },
    {
        "id": "1074",
        "readme": "This workflow is inspired by [enigmatic_e](https://www.youtube.com/@enigmatic_e) with some modifications. For detailed tutorial, please visit his YouTube channel.\n\n## 1. ComfyUI Workflow: AnimateDiff + ControlNet + IPAdapter | Japanese Anime Style\n\nThis workflow lets you transform standard videos into enchanting Japanese anime creations using AnimateDiff, ControlNet, and IPAdapter. Feel free to experiment with various checkpoints, LoRA settings, and reference images for the IPAdapter to craft your unique style. It's a fun and creative way to bring your videos to life in the anime world!\n\n## 2. Overview of AnimateDiff\n\nPlease check out the details on [How to use AnimateDiff in ComfyUI](https://www.runcomfy.com/comfyui-workflows/transform-video-into-ceramic-art-style-using-animatediff-controlnet-within-comfyui#2-how-to-use-animatediff)\n\n## 3. How to Use ControlNet\n\n### 3.1. Understanding ControlNet\n\nControlNet revolutionizes the way we generate images by bringing a new level of spatial control to text-to-image diffusion models. This cutting-edge neural network architecture partners beautifully with giants like Stable Diffusion, harnessing their vast libraries—forged from billions of images—to weave spatial nuances directly into the fabric of image creation. From sketching out edges to mapping human stances, depth perception, or segmenting visuals, ControlNet empowers you to mold the imagery in ways that go far beyond the scope of mere text prompts.\n\n### 3.2. The Innovation of ControlNet\n\nAt its core, ControlNet is ingeniously straightforward. It starts by safeguarding the integrity of the original model's parameters—keeping the base training intact. Then, ControlNet introduces a mirrored set of the model's encoding layers, but with a twist: they're trained using \"zero convolutions.\" These zeros as a starting point mean the layers gently fold in new spatial conditions without causing a ruckus, ensuring that the model's original talents are preserved even as it embarks on new learning paths.\n\n### 3.3. Understand ControlNets and T2I-Adapters\n\nBoth ControlNets and T2I-Adapters play crucial roles in the conditioning of image generation, with each offering distinct advantages. T2I-Adapters are recognized for their efficiency, particularly in terms of speeding up the image generation process. Despite this, ControlNets are unparalleled in their ability to intricately guide the generation process, making them a powerful tool for creators.\n\nConsidering the overlap in functionalities between many T2I-Adapter and ControlNet models, our discussion will primarily focus on ControlNets. However, it's worth noting that the RunComfy platform has preloaded several T2I-Adapter models for ease of use. For those interested in experimenting with T2I-Adapters, you can seamlessly load these models and integrate them into your projects.\n\n**Choosing between ControlNet and T2I-Adapter models in ComfyUI does not affect the use of ControlNet nodes or the consistency of the workflow.** This uniformity ensures a streamlined process, allowing you to leverage the unique benefits of each model type according to your project needs.\n\n### 3.4. Use of ControlNet Nodes\n\n**3.4.1. Loading the \"Apply ControlNet\" Node**\n\nTo begin, you'll need to load the \"Apply ControlNet\" Node into your ComfyUI. This is your first step toward a dual-conditioned image crafting journey, blending visual elements with textual prompts.\n\n**3.4.2. Understanding the Inputs of \"Apply ControlNet\" Node**\n\n**Positive and Negative Conditioning**: These are your tools for shaping the final image—what it should embrace and what it should avoid. Connect these to the \"Positive prompt\" and \"Negative prompt\" slots to sync them with the text-based part of your creative direction.\n\n**Selecting the ControlNet Model**: You'll need to link this input to the \"Load ControlNet Model\" node's output. This is where you decide whether to use a ControlNet or a T2IAdaptor model based on the specific traits or styles you're aiming for. While we're focusing on ControlNet models, mentioning some sought-after T2IAdaptors is worthwhile for a well-rounded view.\n\n**Preprocessing Your Image**: Connect your image to a \"ControlNet Preprocessor\" node, which is vital to ensure your image is ControlNet-ready. It's essential to match the preprocessor to your ControlNet model. This step adjusts your original image to fit the model's needs perfectly—resizing, recoloring, or applying necessary filters—preparing it for use by ControlNet.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1074/readme01.webp\" alt=\"Apply ControlNet in ComfyUI\" width=\"750\"/>\n\n**3.4.3. Understanding the Outputs of \"Apply ControlNet\" Node**\n\nAfter processing, the \"Apply ControlNet\" node presents you with two outputs reflecting the sophisticated interplay of ControlNet and your creative input: **Positive and Negative Conditioning**. These outputs guide the diffusion model within ComfyUI, leading to your next choice: refine the image using the KSampler or dive deeper by stacking more ControlNets for those seeking unparalleled detail and customization.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1074/readme02.webp\" alt=\"Apply ControlNet in ComfyUI\" width=\"750\"/>\n\n**3.4.4. Tuning \"Apply ControlNet\" for Best Results**\n\n**Determining Strength**: This setting controls how much ControlNet sways the resulting image. A full-on 1.0 means ControlNet's input has the reins, while dialing down to 0.0 lets the model run without ControlNet's influence.\n\n**Adjusting Start Percent**: This tells you when ControlNet starts to pitch in during the diffusion process. For example, a 20% start means that from one-fifth of the way through, ControlNet begins to make its mark.\n\n**Setting End Percent**: This is the flip side of Start Percent, marking when ControlNet bows out. If you set it to 80%, ControlNet's influence fades away as the image nears its final stages, untouched by ControlNet in the last stretch.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1074/readme03.webp\" alt=\"Apply ControlNet in ComfyUI\" width=\"750\"/>\n\n### 3.5. Guide to ControlNet Model: Openpose, Depth, SoftEdge, Canny, Lineart, Tile\n\n**3.5.1. ControlNet Model: Openpose**\n\n- **Openpose (also referred to as Openpose body)**: This model acts as the cornerstone of ControlNet for identifying key points on the human body, such as the eyes, nose, neck, shoulders, elbows, wrists, knees, and ankles. It's perfect for replicating simple human poses.\n- **Openpose_face**: This version of Openpose takes it a step further by detecting facial key points, which allows for a nuanced analysis of facial expressions and the direction the face is pointing. If your project is centered around facial expressions, this model is vital.\n- **Openpose_hand**: This enhancement to the Openpose model focuses on the fine details of hand and finger movements, which is key for a detailed understanding of hand gestures and positioning. It broadens the scope of what Openpose can do within ControlNet.\n- **Openpose_faceonly**: Tailored for facial detail analysis, this model skips over body key points to zero in on facial expressions and orientation exclusively. When facial features are all that matter, this is the model to choose.\n- **Openpose_full**: This all-in-one model integrates the capabilities of Openpose, Openpose_face, and Openpose_hand for full-body, face, and hand keypoint detection, making it the go-to for comprehensive human pose analysis within ControlNet.\n- **DW_Openpose_full**: Building upon Openpose_full, this model introduces further enhancements for superior pose detection detail and accuracy. It's the most advanced version available in the ControlNet suite.\n\nPreprocessor options include: Openpose or DWpose\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1074/readme04.webp\" alt=\"ControlNet Openpose in ComfyUI\" width=\"750\"/>\n\n**3.5.2. ControlNet Model: Depth**\n\nDepth models use a 2D image to infer depth, representing it as a grayscale map. Each has its strengths in terms of detail or background focus:\n\n- **Depth Midas**: A balanced approach to depth estimation, Depth Midas offers a middle ground in detailing and background portrayal.\n- **Depth Leres**: Puts an emphasis on details while still capturing background elements more prominently.\n- **Depth Leres++**: Pushes the envelope for detail in depth information, which is particularly useful for complex scenes.\n- **Zoe**: Finds equilibrium between the detail levels of Midas and Leres models.\n- **Depth Anything**: An improved model for versatile depth estimation across various scenes.\n- **Depth Hand Refiner**: Specifically fine-tunes details of hands in depth maps, making it invaluable for scenes where precise hand placement is essential.\n\nPreprocessors to consider: Depth_Midas, Depth_Leres, Depth_Zoe, Depth_Anything, MeshGraphormer_Hand_Refiner. This model excels in robustness and compatibility with actual depth maps from rendering engines.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1074/readme05.webp\" alt=\"ControlNet Depth in ComfyUI\" width=\"750\"/>\n\n**3.5.3. ControlNet Model: SoftEdge**\n\nControlNet Soft Edge is crafted to produce images with gentler edges, enhancing detail while maintaining a natural look. It utilizes cutting-edge neural networks for refined image manipulation, offering extensive creative control and flawless integration.\n\nIn terms of robustness: SoftEdge_PIDI_safe > SoftEdge_HED_safe >> SoftEdge_PIDI > SoftEdge_HED\n\nFor the highest quality results: SoftEdge_HED > SoftEdge_PIDI > SoftEdge_HED_safe > SoftEdge_PIDI_safe\n\nAs a general recommendation, SoftEdge_PIDI is the go-to option since it typically delivers excellent results.\n\nPreprocessors include: SoftEdge_PIDI, SoftEdge_PIDI_safe, SoftEdge_HED, SoftEdge_HED_safe.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1074/readme06.webp\" alt=\"ControlNet Softedge in ComfyUI\" width=\"750\"/>\n\n**3.5.4. ControlNet Model: Canny**\n\nThe Canny model implements the Canny edge detection to spotlight a wide spectrum of edges within images. This model is excellent for maintaining the integrity of structural elements while simplifying the image's overall look, aiding in creating stylized art or preparing images for additional manipulation.\n\nPreprocessors available: Canny\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1074/readme07.webp\" alt=\"ControlNet Canny in ComfyUI\" width=\"750\"/>\n\n**3.5.5. ControlNet Model: Lineart**\n\nLineart models are your tools for transforming images into stylized line drawings, suitable for a variety of artistic applications:\n\n- **Lineart**: The standard choice for turning images into line drawings, providing a versatile starting point for different artistic or creative endeavors.\n- **Lineart anime**: Tailored for creating clean, precise anime-style line drawings, perfect for projects aiming for an anime-inspired look.\n- **Lineart realistic**: Aims to capture a more lifelike representation in line drawings, offering more detail for projects that require realism.\n- **Lineart coarse**: Emphasizes bolder, more pronounced lines for a striking visual impact, ideal for bold graphic statements.\n\nPreprocessors available can produce either detailed or more pronounced lineart (Lineart and Lineart_Coarse).\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1074/readme08.webp\" alt=\"ControlNet Lineart in ComfyUI\" width=\"750\"/>\n\n**3.5.6. ControlNet Model: Tile**\n\nThe Tile Resample model excels in bringing out details in images. It's especially effective when used in tandem with an upscaler to enhance image resolution and detail, often applied to sharpen and enrich image textures and elements.\n\nPreprocessor recommended: Tile\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1074/readme09.webp\" alt=\"ControlNet Tile in ComfyUI\" width=\"750\"/>\n\n### 3.6. Guide to use multiple ControlNet\n\nIncorporating multiple ControlNets or T2I-Adapters allows for the sequential application of different conditioning types to your image generation process. For example, you can combine Lineart and OpenPose ControlNets for enhanced detailing.\n\n**Lineart for Object Shape**: Start by integrating a Lineart ControlNet to add depth and detail to objects or elements in your imagery. This process involves preparing a lineart or canny map for the objects you wish to include.\n\n**OpenPose for Pose Control**: Following the lineart detailing, utilize the OpenPose ControlNet to dictate the pose of individuals within your image. You will need to generate or acquire an OpenPose map that captures the desired pose.\n\n**Sequential Application**: To effectively combine these effects, link the output from the Lineart ControlNet into the OpenPose ControlNet. This method ensures that both the pose of the subjects and the shapes of objects are simultaneously guided during the generation process, creating an outcome that harmoniously aligns with all input specifications.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1074/readme10.webp\" alt=\"Multiple ControlNet in ComfyUI\" width=\"750\"/>\n\n## 4. Overview of IPAdapter\n\nPlease check out the details on [How to use IPAdapter in ComfyUI](https://www.runcomfy.com/comfyui-workflows/comfyui-animatediff-and-ipadapter-workflow-stable-diffusion-animation#3-overview-of-ip-adapter)\n"
    },
    {
        "id": "1075",
        "readme": "This workflow is inspired by [enigmatic_e](https://www.youtube.com/@enigmatic_e) with some modifications. For detalied tutorial, please visit his YouTube channel.\n\n## 1. ComfyUI Workflow: AnimateDiff + ControlNet + IPAdapter | Adventure Game Style\n\nDive into the world of adventure games with this workflow, which employs the magic of AnimateDiff, ControlNet, and IPAdapter to turn ordinary videos into thrilling adventure game scenes. Explore different checkpoints, adjust LoRA settings, and choose various reference images for the IPAdapter to design a distinctive style. It's an exciting journey to reimagine your videos with the spirit of adventure gaming!\n\n## 2. Overview of AnimateDiff\n\nPlease check out the details on [How to use AnimateDiff in ComfyUI](https://www.runcomfy.com/comfyui-workflows/transform-video-into-ceramic-art-style-using-animatediff-controlnet-within-comfyui#2-how-to-use-animatediff)\n\n## 3. Overview of ControlNet\n\nPlease check out the details on [How to use ControlNet in ComfyUI](https://www.runcomfy.com/comfyui-workflows/convert-video-to-japanese-anime-style-using-animatediff-controlnet-ipadapter-in-comfyui#3-how-to-use-controlnet)\n\n## 4. Overview of IPAdapter\n\nPlease check out the details in the [Introduction to IPAdapter](https://www.runcomfy.com/comfyui-workflows/comfyui-animatediff-and-ipadapter-workflow-stable-diffusion-animation#3-overview-of-ip-adapter)\n"
    },
    {
        "id": "1078",
        "readme": "## 1. ComfyUI Workflow: LayerDiffuse + TripoSR ｜ Image to 3D\n\nIn the ComfyUI workflow, we harness the capabilities of LayerDiffuse to produce images with transparent backgrounds. Following this, both the image and its mask are passed on to TripoSR for the creation of 3D objects. The outcome is a rough yet quickly produced 3D model, showing promising potential for further refinement.\n\nFor those interested in obtaining the mesh file (.obj), you can find it in your file system's output section. This streamlined process offers a straightforward path from image to 3D model, combining the strengths of LayerDiffuse and TripoSR to enhance your 3D creation experience.\n\n## 2. Overview of LayerDiffuse\n\nPlease check out the details on [How to use LayerDiffuse in ComfyUI](https://www.runcomfy.com/comfyui-workflows/generating-transparent-images-with-layerdiffuse-in-comfyui) \n\n## 3. Overview of TripoSR\n\n### 3.1. Introduction to TripoSR\n\nTripoSR is a cutting-edge 3D reconstruction model that quickly turns single images into 3D objects with astonishing speed and precision. This innovation is a joint effort by Tripo AI and Stability AI. Utilizing a transformer architecture, TripoSR stands out for its ability to quickly process images into 3D forms. It builds on the Large Reconstruction Model (LRM) network architecture but brings in significant improvements in handling data, designing the model, and refining the training process. These advancements make TripoSR more accurate and efficient than other models available today.\n\n### 3.2. Technical Architecture of TripoSR\n\nThe core of TripoSR includes three main parts: an image encoder, an image-to-triplane decoder, and a triplane-based neural radiance field (NeRF). The image encoder uses a pre-trained vision transformer model to capture both the broad and specific details of an input image. These details are then turned into a detailed 3D model using the innovative triplane-NeRF setup. Uniquely, TripoSR can guess the camera's settings, making it versatile and efficient in different image conditions without needing exact camera information.\n\n### 3.3. TripoSR Performance Benchmarking\n\nThe performance of TripoSR stands out when compared with other leading models. It consistently exceeds in capturing the fine textures and complex shapes of objects swiftly. This exceptional performance, achieved quickly on standard computer hardware, showcases TripoSR's potential to change the 3D reconstruction landscape.\n"
    },
    {
        "id": "1079",
        "readme": "## 1. ComfyUI LayerDiffuse Workflow Overview\n\nThe ComfyUI LayerDiffuse workflow integrates three specialized sub-workflows: creating transparent images, generating background from the foreground, and the inverse process of generating foreground based on existing background. Each of these LayerDiffuse sub-workflows operates independently, providing you the flexibility to choose and activate the specific LayerDiffuse functionality that meets your creative needs.\n\n### 1.1. Creating Transparent Images with LayerDiffuse:\n\nThis workflow enables the direct creation of transparent images, providing you with the flexibility to generate images with or without specifying the alpha channel mask. \n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1079/readme01.webp\" alt=\"Prompt Schedule\" width=\"700\"/>\n\n### 1.2. Generating Background from the Foreground with LayerDiffuse:\n\nFor this LayerDiffuse workflow, start by uploading your foreground image and crafting a descriptive prompt. LayerDiffuse then blends these elements to produce your desired image. When drafting your prompt for LayerDiffuse, it's crucial to detail the complete scene (e.g., \"a car parked on the side of the street\") instead of merely describing the background element (e.g., \"the street\").\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1079/readme02.webp\" alt=\"Prompt Schedule\" width=\"700\"/>\n\n### 1.3. Generating Foreground Based on the Background:\n\nMirroring the previous workflow, this LayerDiffuse functionality reverses the focus, aiming to merge foreground elements with an existing background. Therefore, you need to upload the background image and describing the envisioned final image in your prompt, emphasizing the complete scene (e.g., \"a dog walking on the street\") over individual elements (e.g., \"the dog\").\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1079/readme03.webp\" alt=\"Prompt Schedule\" width=\"700\"/>\n\nFor more LayerDiffuse workflows, check it on [github](https://github.com/huchenlei/ComfyUI-layerdiffuse)\n\n## 2. LayerDiffuse Workflow Efficacy\n\nWhile the process of creating transparent images is robust and reliably produces high-quality results, the workflows for blending backgrounds and foregrounds are more experimental. They may not always achieve a perfect blend, indicative of the innovative but developing nature of this technology.\n\n## 3. Technical introduction to LayerDiffuse\n\nLayerDiffuse is an innovative approach designed to enable large-scale pretrained latent diffusion models to generate images with transparency. This technique introduces the concept of \"latent transparency,\" which involves encoding alpha channel transparency directly into the latent manifold of existing models. This allows for the creation of transparent images or multiple transparent layers without significantly altering the original latent distribution of the pretrained model. The goal is to maintain the high-quality output of these models while adding the capability to generate images with transparency.\n\nTo achieve this, LayerDiffuse fine-tunes pretrained latent diffusion models by adjusting their latent space to include transparency as a latent offset. This process involves minimal changes to the model, preserving its original qualities and performance. The training of LayerDiffuse utilizes a dataset of 1 million transparent image layer pairs, collected through a human-in-the-loop scheme to ensure a wide variety of transparency effects.\n\nThe method has been shown to be adaptable to various open-source image generators and can be integrated into different conditional control systems. This versatility allows for a range of applications, such as generating images with foreground/background-specific transparency, creating layers with joint generation capabilities, and controlling the structural content of the layers.\n\n"
    },
    {
        "id": "1080",
        "readme": "## 1. ComfyUI SUPIR for Image Resolution | ComfyUI Upscale Workflow\n\nThis ComfyUI Upscale workflow utilizes the SUPIR (Scaling-UP Image Restoration), a state-of-the-art open-source model designed for advanced image and video enhancement. In this workflow, you will experience how SUPIR restores and upscales images to achieve photo-realistic results.\n\n## 2. ComfyUI SUPIR Overview\n\nSUPIR, the forefront of image upscaling technology, is comparable to commercial software like Magnific and Topaz AI. Our tutorial encompasses the **SUPIR upscaler wrapper node** within the ComfyUI workflow, which is adept at upscaling and restoring realistic images and videos.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1080/readme01.webp\" alt=\"SUPIR upscaler vs. Magnific vs. Topaz AI\" width=\"800\"/>\n\nFor image upscaling, this workflow's default setup will suffice. To modify it for video upscaling, switch from \"load image\" to \"load video\" and alter the output from \"save image\" to \"combine video\" to cater to video files.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1080/readme02.webp\" alt=\"SUPIR upscaler\" width=\"800\"/>\n\n## 3. Introduction to SUPIR Model\n\nThe **Scaling-UP Image Restoration** technology is a groundbreaking enhancement and upscaling model introduced by the paper **[Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild](https://arxiv.org/abs/2401.13627)**. SUPIR innovates with a photo-realistic image restoration method using a generative prior coupled with model scaling, enriched by multimodal techniques allowing image restoration guided by textual prompts, which broadens its application spectrum significantly.\n\n## 4. How to Use ComfyUI SUPIR for Image Resolution\n\n### 4.1. SUPIR Compatible Models\n\nBefore diving into SUPIR’s usage, ensure the checkpoint models are accessible:\n\n- Two versions of SDXL CLIP Encoder from OpenAI and LAION, respectively.\n- SDXL and LLaVA base models, crucial for the initial stages of image processing.\n- Optional models like Juggernaut-XL versions which can replace the SDXL base in specific scenarios for enhanced photorealistic outcomes.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1080/readme03.webp\" alt=\"ComfyUI SUPIR upscaler\" width=\"500\"/>\n\n### 4.2. SUPIR Models\n\nTwo key versions of SUPIR are available:\n\n- **SUPIR-v0Q**: Optimized for high generalization and quality, suitable for a wide range of images.\n- **SUPIR-v0F**: Tailored for images with light degradation, preserving more details in such conditions\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1080/readme04.webp\" alt=\"ComfyUI SUPIR upscaler\" width=\"500\"/>\n\n### 4.3. Key Parameters of SUPIR\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1080/readme05.webp\" alt=\"ComfyUI SUPIR upscaler\" width=\"500\"/>\n\n- `scale_by`: The upscaling ratio for given inputs determines how much the image size is increased during restoration.\n- `steps`: This parameter specifies the number of steps for the EDM Sampling Scheduler, likely influencing the detail and quality of the restoration process.\n- `cfg_scale`: This is the classifier-free guidance scale for prompts, affecting how strongly the output adheres to the provided textual prompts.\n- `positive-prompt` & `negative_prompt`: These parameters allow users to guide the restoration towards desired qualities (positive prompt) and away from undesired characteristics (negative prompt).\n- `s_churn` & `s_noise`: Representing original hyperparameters of EDM, this controls aspects of the noise model within the diffusion process, impacting the final image texture and clarity.\n- `color_fix_type`: This parameter allows for the selection of color correction methods post-restoration, with options including 'None', 'AdaIn', and 'Wavelet'.\n\n### 4.4. Performance Tips for SUPIR\n\n- **Hardware Requirements**: To achieve optimal results in higher resolution upscaling with the SUPIR upscaler, it is essential to have a sufficiently powerful hardware setup. We recommend using a machine equipped with at least 48GB of VRAM, such as the Extra Large Machine available at RunComfy, to handle the intensive computational demands of advanced image detailing.\n- **Maximizing Image Detail with Text Prompts**: Furthermore, to maximize the potential of the SUPIR's advanced AI algorithms, make full use of the detailed prompt feature. This allows you to guide the restoration process more precisely, enhancing the detailing and realism of the upscaled images. By effectively leveraging these prompts, SUPIR can produce outputs that are not only larger in size but also superior in quality.\n\n## 5. More Details About the SUPIR\n\nImage restoration technology has grown tremendously, now delivering results that are visually stunning and smarter. This growth is largely due to the introduction of the SUPIR Upscaler, which utilizes advanced generative models to enhance images.\n\n### 5.1. Core Capabilities of the SUPIR model\n\n- **Robust Models**: The heart of the SUPIR Upscaler is the StableDiffusion-XL (SDXL), a powerful generative model with 2.6 billion parameters. It's supported by an adaptor model adding another 600 million parameters, enabling the SUPIR Upscaler to restore images with exceptional detail and fidelity.\n\n### 5.2. Data-Driven Excellence\n\n- **Extensive Training Data**: The SUPIR Upscaler is trained on a dataset of over 20 million high-quality images, each annotated with detailed descriptions. This dataset trains a 13-billion-parameter multi-modal language model, enhancing the SUPIR Upscaler’s capability to produce precise content prompts for targeted image restoration.\n\n### 5.3. Innovative Technology and Strategic Implementation\n\n- **Advanced Design**: The SUPIR Upscaler includes several strategic enhancements like the ZeroSFT connector, which improves efficiency and reduces computational demands. Additionally, its image encoder is fine-tuned to better handle image degradation, increasing the accuracy of restoration results.\n- **Comprehensive Training**: Beyond high-quality images, the dataset also includes lower-quality, negative examples. This helps the SUPIR Upscaler learn to identify and correct visual flaws, enhancing the overall restoration quality.\n\n### 5.4. Balancing Enhancement and Fidelity\n\n- **Sophisticated Techniques**: Despite using generative models, the SUPIR Upscaler employs a novel sampling technique to balance the enhancement quality with the fidelity of the original images. This ensures that while the visual quality is boosted, the authenticity of the original images is preserved.\n\nFor a deeper dive into the SUPIR Upscaler’s capabilities and more technical details, explore resources on its [GitHub page](https://github.com/kijai/ComfyUI-SUPIR/tree/main) or the foundational [research paper](https://arxiv.org/abs/2401.13627). These resources provide comprehensive insights into the technologies and strategies that establish the SUPIR Upscaler as a leader in image restoration.\n"
    },
    {
        "id": "1081",
        "readme": "## 1. ComfyUI CCSR | ComfyUI Upscale Workflow\n\nThis ComfyUI workflow incorporates the CCSR (Content Consistent Super-Resolution) model, designed to enhance content consistency in super-resolution tasks. Following the application of the CCSR model, there's an optional step that involves upscaling once more by adding noise and utilizing the ControlNet recolor model. This is an experimental feature for users to explore.\n\nBy default, this workflow is set up for image upscaling. To upscale videos, simply replace \"load image\" with \"load video\" and change \"save image\" to \"combine video.\"\n\n## 2. Introduction to CCSR\n\nPre-trained latent diffusion models have been recognized for their potential in improving the perceptual quality of image super-resolution (SR) outcomes. However, these models often produce variable results for identical low-resolution images under different noise conditions. This variability, though advantageous for text-to-image generation, poses challenges for SR tasks, which demand consistency in content preservation.\n\nTo enhance the reliability of diffusion prior-based SR, CCSR (Content Consistent Super-Resolution) uses a strategy that combines diffusion models for refining image structures with generative adversarial networks (GANs) for improving fine details. It introduces a non-uniform timestep learning strategy to train a compact diffusion network. This network efficiently and stably reconstructs the main structures of an image, while the pre-trained decoder of a variational auto-encoder (VAE) is fine-tuned through adversarial training for detail enhancement. This approach helps CCSR to notably reduce the stochasticity associated with diffusion prior-based SR methods, thereby enhancing content consistency in SR outputs and accelerating the image generation process.\n\n## 3. How to Use ComfyUI CCSR for image Upscaling\n\n### 3.1. CCSR Models\n\n`real-world_ccsr.ckpt`: CCSR model for real-world image restoration.\n\n`bicubic_ccsr.ckpt`: CCSR model for bicubic image restoration.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1081/readme01.webp\" alt=\"Prompt Schedule\" width=\"700\"/>\n\n### 3.2. Key Parameters in CCSR\n\n`-scale_by`: This parameter specifies the super-resolution scale, determining how much the input images or videos are enlarged.\n\n`-steps`: Refers to the number of steps in the diffusion process. It controls how many iterations the model goes through to refine the image details and structures.\n\n`-t_max` and `-t_min`: These parameters set the maximum and minimum thresholds for the non-uniform timestep learning strategy used in the CCSR model. \n\n`-sampling_method`:\n\n - `CCSR (Normal, Untiled):` This approach utilizes a normal, untiled sampling method. It's straightforward and does not divide the image into tiles for processing. While this can be effective for ensuring content consistency across the entire image, it's also heavy on VRAM usage. This method is best suited for scenarios where VRAM is plentiful, and the highest possible consistency across the image is required.\n - `CCSR_Tiled_MixDiff:` This tiled approach processes each tile of the image separately, which helps manage VRAM usage more efficiently by not requiring the entire image to be in memory at once. However, a notable drawback is the potential for visible seams where tiles meet, as each tile is processed independently, leading to possible inconsistencies at the tile borders.\n - `CCSR_Tiled_VAE_Gaussian_Weights`: This method aims to fix the seam issue seen in the CCSR_Tiled_MixDiff approach by using Gaussian weights to blend the tiles more smoothly. This can significantly reduce the visibility of seams, providing a more consistent appearance across tile borders. However, this blending can sometimes be less accurate and might introduce extra noise into the super-resolved image, affecting the overall image quality.\n\n`-tile_size`, and `-tile_stride`: These parameters are part of the tiled diffusion feature, which is integrated into CCSR to save GPU memory during inference. Tiling refers to processing the image in patches rather than whole, which can be more memory-efficient. `-tile_size` specifies the size of each tile, and `-tile_diffusion_stride` controls the stride or overlap between tiles.\n\n`-color_fix_type`: This parameter indicate the method used for color correction or adjustment in the super-resolution process. `adain` is one of the methods employed for color correction to ensure that the colors in the super-resolved image match the original image as closely as possible.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1081/readme02.webp\" alt=\"Prompt Schedule\" width=\"700\"/>\n\n## 4. More Details about CCSR\n\nImage super-resolution, aimed at recovering high-resolution (HR) images from low-resolution (LR) counterparts, addresses the challenge posed by quality degradation during image capture. While existing deep learning-based SR techniques have primarily focused on neural network architecture optimization against simple, known degradations, they fall short in handling the complex degradations encountered in real-world scenarios. Recent advancements have included the development of datasets and methods simulating more complex image degradations to approximate these real-world challenges.\n\nThe study also highlights the limitations of traditional loss functions, such as ℓ1 and MSE, which tend to produce overly smooth details in SR outputs. Although SSIM loss and perceptual loss mitigate this issue to some extent, achieving realistic image detail remains challenging. GANs have emerged as a successful approach for enhancing image details, but their application to natural images often results in visual artifacts due to the diverse nature of natural scenes.\n\nDenoising Diffusion Probabilistic Models (DDPMs) and their variants have shown significant promise, outperforming GANs in generating diverse and high-quality priors for image restoration, including SR. These models, however, have struggled to adapt to the complex and varied degradations present in real-world applications.\n\nThe CCSR approach seeks to address these challenges by ensuring stable and consistent super-resolution outcomes. It leverages diffusion priors for generating coherent structures and employs generative adversarial training for detail and texture enhancement. By adopting a non-uniform timestep sampling strategy and fine-tuning a pre-trained VAE decoder, CCSR achieves stable, content-consistent SR results more efficiently than existing diffusion prior-based SR methods.\n\nFor more information, check it on the [github](https://github.com/kijai/ComfyUI-CCSR) or [paper](https://arxiv.org/pdf/2401.00877.pdf)\n"
    },
    {
        "id": "1082",
        "readme": "## 1. APISR for Anime Image Resolution | ComfyUI Upscale Workflow\n\nThis ComfyUI upscale workflow integrates the APISR (Anime Production-oriented Image Super-Resolution) model for upscaling low-quality, low-resolution anime images and videos. Additionally, it incorporates the 4xAnimateSharp Model for comparison purposes.\n\nBy default, the workflow is configured for image upscaling. To upscale videos, simply replace \"load image\" node with \"load video\" node and replace \"save image\" node with \"combine video\" node for a seamless transition.\n\n## 2. Introduction to APISR\n\nAPISR (Anime Production-oriented Image Super-Resolution) is designed to restore and enhance low-quality, low-resolution anime images and video sources that have undergone various degradations in real-world scenarios. It caters to the increasing interest in anime super-resolution (SR) by deviating from the traditional application of photorealistic SR techniques, which may not adequately accommodate the distinctive characteristics of anime.\n\nTwo primary challenges are identified in anime SR: the degradation of hand-drawn lines due to compression or aging, and the presence of unwanted color artifacts. To tackle these, APISR introduces an image degradation model specifically designed to simulate compression effects and assist in the restoration of hand-drawn lines. Additionally, it proposes a technique for enhancing faint, aging hand-drawn lines by merging them from overly sharpened images.\n\nFor the issue of color artifacts, often arising from GAN-based SR networks, APISR introduces balanced twin perceptual loss. This method combines perceptual features from both photorealistic and anime domains, aiming to reduce color inconsistencies and improve the visual quality of super-resolved anime images.\n\nAPISR introduces a dataset curation pipeline specifically designed for anime, an image degradation model that targets the unique challenges of anime restoration, and an innovative approach for addressing color artifacts in GAN-based SR network training. These methods collectively focus on enhancing the super-resolution process for anime by selecting the most informative frames for datasets, restoring degraded hand-drawn lines, and improving visual quality by addressing color inconsistencies. The effectiveness of APISR's approach is validated through comprehensive evaluations on real-world anime SR datasets, showing significant advancements over existing techniques.\n\n## 3. How to Use ComfyUI APISR for Anime Image Resolution\n\nThe APISR model is very user-friendly; simply select the model corresponding to the upscale size you desire.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1082/readme01.webp\" alt=\"Prompt Schedule\" width=\"700\"/>\n\n\nFor more information, check it on the [github](https://github.com/Kiteretsu77/APISR) and [paper](https://arxiv.org/abs/2403.01598)\n"
    },
    {
        "id": "1084",
        "readme": "## 1. Introducing DynamiCrafter: Revolutionizing Open-domain Image Animation\nDynamiCrafter stands at the forefront of digital art innovation, transforming still images into captivating animated videos. This state-of-the-art tool leverages the power of video diffusion models, breaking free from the constraints of traditional animation techniques that often focus on specific domains or natural phenomena. With DynamiCrafter, artists can breathe life into a vast array of visual content, from portraits to panoramic landscapes, through intuitive and dynamic animation.\n\nFor more information, please visit [github DynamiCrafter](https://github.com/Doubiiu/DynamiCrafter) or [github ComfyUI DynamiCrafter](https://github.com/chaojie/ComfyUI-DynamiCrafter)\n\nFor more example and inspiration, visit [here](https://replicate.com/camenduru/dynami-crafter-576x1024/examples)\n\n## 2. Key Features\n\n**Open-domain Animation**: Unlike traditional methods that cater to specific motions or domains, DynamiCrafter excels in animating a wide range of images, offering unparalleled versatility.\n\n**Text-to-Video Diffusion Models**: By harnessing advanced text-to-video generative models, it enables the synthesis of animations that are not only visually stunning but also logically consistent and naturally flowing.\n\n**Dual-stream Injection Paradigm**: Incorporating both text-aligned context representation and visual detail guidance, DynamiCrafter ensures animations that are rich in detail and faithful to the original image's essence.\n\n**Intuitive Motion Control**: With the ability to interpret and animate based on textual prompts, artists have the creative freedom to guide the animation process, ensuring the final output aligns with their vision.\n\n## 3. How It Works\nDynamiCrafter integrates seamlessly into the creative workflow, starting with the projection of the still image into a text-aligned rich context space. This facilitates the understanding and preservation of the image's core details during the animation process. By feeding the full image alongside initial noises to the diffusion model, it crafts animations that maintain visual fidelity while introducing natural and logical motions.\n\n## 4. Applications and Versatility\nBeyond mere animation, DynamiCrafter opens up new avenues for creative expression:\n*Storytelling*: Transform static images into compelling narrative videos, each frame a chapter in the unfolding story.\n*Looping Videos*: Create mesmerizing looping animations, perfect for digital displays and social media content.\n*Generative Frame Interpolation*: Innovatively fill in the gaps between frames, producing smooth transitions and detailed motion paths.\n\n## 5. Why DynamiCrafter?\nDynamiCrafter is not just a tool; it's a new canvas for digital artists. By combining the detailed precision of still imagery with the dynamic allure of animation, it invites artists to explore new dimensions of creativity. Whether for enhancing digital portfolios, creating engaging content for social media, or pushing the boundaries of digital storytelling, DynamiCrafter is the companion for any artist looking to stand out in the digital age.\n\n## 6. Getting Started\nReady to transform your images into animated masterpieces? Simply start with RunComfy by \"Launching App\".\n"
    },
    {
        "id": "1085",
        "readme": "## 1. ComfyUI IPAdapter Plus Workflow for Image Merging\n\nExplore the advanced functionalities of IPAdapter Plus (IPAdapter V2), expertly developed by [Matteo](https://github.com/cubiq/ComfyUI_IPAdapter_plus). For an in-depth understanding of how to maximize the potential of IPAdapter Plus, don’t miss his [YouTube tutorial](https://www.youtube.com/watch?v=_JzDcgKgghY)—it's truly exceptional!\n\nIn this workflow, you can experiment with different methods of IPAdapter Plus (IPAdapter V2) for image merging. You can seamlessly blend multiple images using techniques like concatenation, addition, subtraction, averaging, and normalized averaging, each providing unique visual outcomes. These methods allow for the creation of engaging visual narratives, showcasing IPAdapter Plus as a powerful tool for sophisticated image generation and manipulation.\n\nHere’s a guide on [how to switch IPAdapter V2 and IPAdapter V1 smoothly at RunComfy](https://comfyui-guides.runcomfy.com/ultimate-comfyui-how-tos-a-runcomfy-guide/how-to-install-ipadapter-v2)\n\n## 2. Introduction to IPAdapter Plus (IPAdapter V2)\n\nIPAdapter Plus (IPAdapter V2) represents a significant update from its predecessor, enhancing your experience with image-to-image conditioning in ComfyUI. This tool excels in transferring styles and elements from reference images into your projects, streamlining complex imaging tasks. With its robust capabilities, IPAdapter Plus enables effortless integration of style transfers and powerful model options, making it an outstanding resource for elevating your creative image generation and manipulation endeavors.\n\n## 3. How to Merge Multiple Images with IPAdapter Plus (IPAdapter V2)\n\n### 3.1. Method 1: Merge Images Using the ComfyUI \"Batch Image\" Node\n\nTo begin, select the images you intend to combine and input them into the \"Batch Image\" node. This creates a batch that feeds into the IPAdapter pipeline, processing each image in sequence.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1085/readme01.webp\" alt=\"ComfyUI IPAdapter Plus\" width=\"750\"/>\n\n#### Using the \"IPAdapter Advanced\" Node to Choose Merging Options\n\nThe \"IPAdapter Advanced\" node provides several embedding combination methods:\n\n- **Concat**: This technique concatenates the embeddings from each image, maintaining and blending their attributes into one comprehensive set. This method is ideal for including a diverse range of image features, although it may increase the complexity of the processed information.\n- **Add**: By adding the embeddings together, this approach amplifies the common characteristics of the images, promoting a unified theme or feature across the merged image.\n- **Subtract**: In contrast, subtracting the embeddings can accentuate the distinctive qualities of each image, offering a unique way to explore and emphasize contrasts.\n- **Average**: This method equally blends the characteristics from each image, ensuring no single image dominates the outcome. It’s perfect for achieving a well-balanced and even representation of all images.\n- **Norm Average** (Normalized Average): This option scales each image's influence based on its relative importance in the batch, allowing for detailed adjustments to the final image, making sure it matches your vision.\n\nExperimenting with different combination techniques unlocks a world of possibilities, allowing you to discover the perfect blend that brings your creative vision to life. Dive into each method and watch as your images transform into captivating, unique visual narratives!\n\n### 3.2. Method 2: Adjust Image Weights using the ComfyUI \"IPAdapter Encoder\" Node\n\nFor those looking for precise control over the influence of each reference image, the \"IPAdapter Encoder\" node is your go-to tool within the IPAdapter framework.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1085/readme02.webp\" alt=\"ComfyUI IPAdapter Plus\" width=\"750\"/>\n\n#### Implementing \"IPAdapter Encoder\" Nodes\n\nEach image is processed through an \"IPAdapter Encoder\" node, which prepares them for merging by encoding their data. You can set specific weights for each image, refining how prominently each one impacts the combined outcome.\n\n#### Combining Images with the \"IPAdapter Combine Embeds\" Node\n\nOnce the images are encoded and weighted, the \"IPAdapter Combine Embeds\" node merges these encoded data points into a single, cohesive embedding, reflecting the intended influence of each image in the merged product.\n\n#### Finalizing the Image with the \"IPAdapter\" Node\n\nThe final step involves the \"IP Adapter\" processing the combined embedding to synthesize the new image. Though connecting negative embeds is optional, doing so can help conserve computational resources. Linking even just one of the negative embeds from the encoder nodes to the IPAdapter node can optimize the merging process.\n\n## 4. Use IPAdapter Plus (IPAdapter V2) at RunComfy\n\nExperience the unmatched power of the ComfyUI IPAdapter Plus and take your image merging to the next level. Featuring cutting-edge encoding and merging technologies, the IPAdapter Plus simplifies intricate tasks. Let's begin your journey with IPAdapter Plus at RunComfy and transform your creative ideas into mesmerizing realities!\n\nHere’s a guide on [how to switch IPAdapter V2 and IPAdapter V1 smoothly at RunComfy](https://comfyui-guides.runcomfy.com/ultimate-comfyui-how-tos-a-runcomfy-guide/how-to-install-ipadapter-v2)\n"
    },
    {
        "id": "1086",
        "readme": "## 1. ComfyUI IPAdapter Plus for Style and Composition Transfer\n\nExplore the advanced functionalities of IPAdapter Plus (IPAdapter V2), expertly developed by [Matteo](https://github.com/cubiq/ComfyUI_IPAdapter_plus). For an in-depth understanding of how to maximize the potential of IPAdapter Plus, don’t miss his [YouTube tutorial](https://www.youtube.com/watch?v=czcgJnoDVd4&t=234s)—it's truly exceptional!\n\nUsing the ComfyUI IPAdapter Plus workflow, effortlessly transfer style and composition between images. The IPAdapter Plus enables precise control over merging the visual style and compositional elements from different images, facilitating the creation of new visuals. With the configurable settings in the IPAdapter Style & Composition SDXL node, you can finely adjust style and composition weights to achieve your desired results.\n\nHere’s a guide on [how to switch IPAdapter V2 and IPAdapter V1 smoothly at RunComfy](https://comfyui-guides.runcomfy.com/ultimate-comfyui-how-tos-a-runcomfy-guide/how-to-install-ipadapter-v2)\n\n## 2. Introduction to IPAdapter Plus (IPAdapter V2)\n\nIPAdapter Plus (IPAdapter V2) represents a significant update from its predecessor, enhancing your experience with image-to-image conditioning in ComfyUI. This tool excels in transferring styles and elements from reference images into your projects, streamlining complex imaging tasks. With its robust capabilities, IPAdapter Plus enables effortless integration of style transfers and powerful model options, making it an outstanding resource for elevating your creative image generation and manipulation endeavors.\n\n## 3. How to Use ComfyUI IPAdapter Plus for Style and Composition Transfer\n\n### 3.1. IPAdapter Plus for Style Transfer Only\n\nAre you ready to infuse the style of your chosen reference image into your project seamlessly? With ComfyUI IPAdapter Plus, you can easily achieve this sophisticated style transfer. Here's how you can incorporate this technique into your design.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1086/readme01.webp\" alt=\"ComfyUI IPAdapter Plus Style Transfer\" width=\"750\"/>\n\n**Integrating the IPAdapter Advanced Node**: Begin by adding an IPAdapter Advanced node to your workflow. This node is critical for facilitating the style transfer process.\n\n**Setting the Node for Style Transfer**: Within the IPAdapter Plus node settings, select the \"Style Transfer (SDXL)\" from the \"Weight Type\" dropdown. This configuration will focus the node’s efforts on capturing and applying the artistic style from your reference image to your target.\n\n**Customizing the Style Intensity**: Adjust the \"weight\" parameter to control how prominently you want the reference style to appear on your target image. Increasing the weight enhances the visual style’s impact, making it more pronounced and vibrant.\n\n### 3.2. IPAdapter Plus for Composition Transfer Only\n\nIf your goal is to maintain the composition of a reference image while creating a new one, the ComfyUI IPAdapter Plus offers a precise configuration to ensure this:\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1086/readme02.webp\" alt=\"ComfyUI IPAdapter Plus Composition Transfer\" width=\"750\"/>\n\n**Setting Up the IPAdapter Advanced Node Again**: Just like before, incorporate an IPAdapter Advanced node into your design setup.\n\n**Configuring the Node for Composition**: Choose \"Composition (SDXL)\" in the \"Weight Type\" menu. This selection instructs the node to concentrate on retaining the key compositional elements from your reference image in your new creation.\n\n**Adjusting the Composition Settings**: By modifying the \"weight\" setting, you determine the extent to which the new image adheres to the composition of the reference. A higher setting will more closely mimic the original composition’s structure.\n\n### 3.3. IPAdapter Plus for Both Style and Composition Transfer\n\nWhen your project requires a blend of both style and composition from different sources, the IPAdapter Plus Style & Composition SDXL node is ideally suited for the task.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1086/readme03.webp\" alt=\"ComfyUI IPAdapter Plus Style and Composition Transfer\" width=\"750\"/>\n\n**Choosing the IPAdapter Style & Composition SDXL Node**: Add this specialized node to your workflow to manage the dual transfer of style and composition effectively.\n\n**Loading Style and Composition References**: This node accommodates two separate inputs: one for the style you admire and another for the composition you wish to replicate. Ensure both are loaded appropriately.\n\n**Configure Weights for Style and Composition**: The IPAdapter Plus Style & Composition SDXL node allows for individual adjustments in the weights for style and composition. Tailor these settings to strike the perfect balance, ensuring that neither element overshadows the other in your final output.\n\n## 4. Use IPAdapter Plus (IPAdapter V2) at RunComfy\n\nThe ComfyUI IPAdapter Plus functions as a streamlined alternative to traditional LoRA training, which typically demands extensive data and considerable time. By concentrating on single-image transformations, the IPAdapter Plus facilitates quick style and composition adjustments without the need for large datasets or significant computational power. This efficiency makes the IPAdapter Plus an ideal method for rapid and effective artistic modifications. Explore this powerful feature at RunComfy and discover how easily it can elevate your creative projects by blending styles and compositions.\n\nHere’s a guide on [how to switch IPAdapter V2 and IPAdapter V1 smoothly at RunComfy](https://comfyui-guides.runcomfy.com/ultimate-comfyui-how-tos-a-runcomfy-guide/how-to-install-ipadapter-v2)\n"
    },
    {
        "id": "1087",
        "readme": "## 1. ComfyUI IPAdapter Plus Workflow for Style Transfer\n\nIPAdapter Plus can significantly enhance your style transfer projects. [Matteo](https://github.com/cubiq/ComfyUI_IPAdapter_plus), the creator of IPAdapter, shares a detailed tutorial to help you set up and use IPAdapter Plus to infuse various artistic styles into your images. You can check out Matteo’s youtube channel [here](https://www.youtube.com/watch?v=gmwZGC8UVHE).\n\nIn this workflow, you'll have fun learning how to perform style transfers using just one reference image with IPAdapter Plus (IPAdapter V2). This process allows you to experiment with IPAdapter Plus and apply different types of ControlNets. Feel free to choose the ControlNet model that best fits your reference image. A key part of mastering this workflow is adjusting the IPAdapter settings, such as weight, and fine-tuning the ControlNet settings, such as strength, to strike just the right balance for results that are not only precise but also stunningly beautiful. Give it a go and see how IPAdapter Plus can transform your image into another incredible style using only one image reference!\n\nHere’s a guide on [how to switch IPAdapter V2 and IPAdapter V1 smoothly at RunComfy](https://comfyui-guides.runcomfy.com/ultimate-comfyui-how-tos-a-runcomfy-guide/how-to-install-ipadapter-v2)\n\n## 2. Introduction to IPAdapter Plus\n\nIPAdapter Plus, also known as IPAdapter V2, marks a substantial upgrade over its predecessor, specifically designed to enhance image-to-image conditioning within ComfyUI. This advanced node is adept at incorporating styles and elements from reference images directly into your projects, simplifying intricate imaging operations. With IPAdapter Plus, you can easily integrate style transfers and utilize its robust model options, significantly enhancing your capabilities in creative image generation and manipulation. This makes IPAdapter Plus an invaluable asset for anyone looking to elevate their imaging work.\n\n## 3. How to use ComfyUI IPAdapter Plus for Style Transfer\n\nFirstly, it's important to highlight that the IPAdapter Plus Style Transfer is specifically designed for compatibility with the SDXL model. To begin, select the SDXL model as the checkpoint model and also adjust the \"weight type\" in the \"IPAdapter Advanced\" node to \"Style Transfer (SDXL)\". You can experiment with different \"weight\" values to get different results from IPAdapter’s effects. The input image for IPAdapter Plus should be the reference image whose style you aim to emulate; you can pick any image that captivates you with its style.\n\nThe second part is to utilize ControlNet models to ensure the consistency and precision of the output image with your input image. The choice of ControlNet models depends on the characteristics of your input image. For a comprehensive guide on selecting and using different ControlNet models, please refer to the [Mastering ControlNet in ComfyUI](https://www.runcomfy.com/tutorials/mastering-controlnet-in-comfyui). It is also crucial to adjust the \"strength\" of each ControlNet to achieve the desired stylistic effect.\n\nFinally, crafting the appropriate prompt is essential for refining the style transfer process. If the results from IPAdapter Plus are not satisfactory, adjusting the prompt can be an effective way to guide the outcome more precisely. Consider leveraging the prompt as a strategic method to refine and control the style transfer process.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1087/readme01.webp\" alt=\"How to use ComfyUI IPAdapter Plus for Style Transfer\" width=\"750\"/>\n"
    },
    {
        "id": "1088",
        "readme": "## 1. ComfyUI IPAdapter Workflow for Changing Clothes\n\nUnlock the full potential of the ComfyUI IPAdapter Plus (IPAdapter V2) to revolutionize your e-commerce fashion imagery. Using this ComfyUI IPAdapter workflow, you can easily change the clothes, outfits, or styles of your models. Designed for versatility, the workflow enables the creation of diverse fashion models who can wear anything and be set against any backdrop, enhancing the dynamic display of your products.\n\n**1.1. Generating the Fashion Model**\n    \nUtilize the IPAdapter Plus to create an image of a fashion model wearing the input clothing. Adjusting the \"weight\" setting on the IPAdapter and using a carefully crafted prompt are crucial for ensuring that the generated model accurately wears the provided clothing.\n    \n**1.2. Background Removal and Replacement**\n    \nIn the second step, remove the background from the image generated in the first step. Then, upload a new background of your choice where the model will be placed.\n    \n**1.3. Blending Model with New Background**\n    \nMerge the transparent fashion model with the new background. Use the IPAdapter Plus once more, this time re-incorporating the input clothing image to better integrate the model with the background. It is recommended to adjust the settings on the KSampler, such as the denoise number (e.g., around 0.5), to ensure consistency with the image from step 2.\n    \n**1.4. Enhancing Facial Details**\n    \nFinally, apply the face detailer node to refine and enhance the facial features of the fashion model, improving the overall appearance and realism of the image.\n    \n\nBy implementing this ComfyUI IPAdapter workflow, you can easily create diverse and appealing fashion model images, effortlessly showcasing different clothes, styles, and settings for your e-commerce platform.\n\n\nHere’s a guide on [how to switch IPAdapter V2 and IPAdapter V1 smoothly at RunComfy](https://comfyui-guides.runcomfy.com/ultimate-comfyui-how-tos-a-runcomfy-guide/how-to-install-ipadapter-v2)\n"
    },
    {
        "id": "1089",
        "readme": "## 1. Boosting Your Creative Process with ComfyUI Stable Diffusion 3\n\n🌟🌟🌟**The Stable Diffusion 3 Medium model and its related nodes are now preloaded into the RunComfy‘s ComfyUI Beta Version (Version 24.06.13.0)!!!**🌟🌟🌟 You can either use the Stable Diffusion 3 Medium directly within this ComfyUI workflow or seamlessly integrate it into your existing ComfyUI workflows.\n\nThe ComfyUI Stable Diffusion 3 workflow comes with all the necessary Stable Diffusion 3 Medium models. Simply experiment with different prompts or parameters to experience it!\n\n### 1.1. Stable Diffusion 3 Medium Models Preloaded in ComfyUI\n\n- **`sd3_medium.safetensors`**: Includes the MMDiT and VAE weights but does not include any text encoders.\n- **`sd3_medium_incl_clips_t5xxlfp16.safetensors`**: Contains all necessary weights, including the fp16 version of the T5XXL text encoder.\n- **`sd3_medium_incl_clips_t5xxlfp8.safetensors`**: Contains all necessary weights, including the fp8 version of the T5XXL text encoder, offering a balance between quality and resource requirements.\n- **`sd3_medium_incl_clips.safetensors`**: Includes all necessary weights except for the T5XXL text encoder. This version requires minimal resources, but the model's performance will be different without the T5XXL text encoder.\n- The **`text_encoders`** folder contains three text encoders and their original model card links for user convenience. All components within the this folder (and their equivalents embedded in other packages) are subject to their respective original licenses.\n\n### 1.2 Overall Quality and Photorealism of Stable Diffusion 3 Medium\n\nStable Diffusion 3 Medium sets a new standard for image quality in the AI art community. This model delivers images with exceptional detail, color accuracy, and realistic lighting. Here's what you can expect:\n\n- **Detail & Resolution**: Enhanced ability to render intricate details, making it perfect for close-ups and complex compositions.\n- **Color & Lighting**: Improved algorithms ensure that colors are vibrant and true to life, with dynamic lighting effects that add depth and realism to your images.\n- **Realism in Faces and Hands**: Common pitfalls like distorted hands and faces are significantly reduced, thanks to innovations like the 16-channel Variational Autoencoder (VAE).\n\n### 1.3 Prompt Understanding of Stable Diffusion 3 Medium\n\nOne of the standout features of SD3 Medium is its sophisticated prompt comprehension. This model can interpret long and complex prompts involving spatial reasoning, compositional elements, actions, and styles. Here are some highlights:\n\n- **Text Encoders**: Utilizes three text encoders to balance performance and efficiency. This allows for nuanced understanding and execution of detailed prompts.\n- **Compositional Awareness**: Capable of maintaining spatial relationships and accurately depicting scenes as described, making it ideal for storytelling through visuals.\n\n### 1.4 Typography of Stable Diffusion 3 Medium\n\nTypography has always been a challenge in text-to-image generation. SD3 Medium addresses this with remarkable success:\n\n- **Text Quality**: Achieves unprecedented accuracy in spelling, kerning, letter formation, and spacing.\n- **Diffusion Transformer Architecture**: This advanced architecture enables more precise rendering of text within images, reducing errors and improving visual coherence.\n\n### 1.5 Resource Efficiency of Stable Diffusion 3 Medium\n\nDespite its advanced capabilities, SD3 Medium is designed to be resource-efficient:\n\n- **Low VRAM Footprint**: Can run on standard consumer GPUs without performance degradation, making high-quality AI art accessible to a wider audience.\n- **Optimized for Efficiency**: Balances computational demands with output quality, ensuring smooth operation even on less powerful hardware.\n\n### 1.6 Fine-Tuning of Stable Diffusion 3 Medium\n\nCustomization is a critical aspect for AI artists, and SD3 Medium excels in this area:\n\n- **Absorbing Nuanced Details**: Capable of fine-tuning with small datasets, allowing artists to imprint their unique style or meet specific project requirements.\n- **Versatility**: Whether you are working on specific themes, styles, or intricate details, SD3 Medium provides the flexibility needed for personalized artwork.\n\n## 2. What is Stable Diffusion 3\n\nStable Diffusion 3 is a cutting-edge AI model specifically designed for generating images from prompts. It represents the third iteration in the Stable Diffusion series and aims to deliver improved accuracy, better adherence to the nuances of prompts, and superior visual aesthetics compared to earlier versions and other models like DALL·E 3, Midjourney v6, and Ideogram v1. \n\n## 3. Stable Diffusion 3 Models\n\nStable Diffusion 3 offers three distinct models, each designed to meet different needs and computational capabilities:\n\n### 3.1. Stable Diffusion 3 Medium\n\n🌟🌟🌟 **Integrated directly into this workflow** 🌟🌟🌟\n\n- **Parameters**: 2 billion\n- **Key Features**:\n    - High-quality, photorealistic images\n    - Advanced understanding of complex prompts\n    - Superior typography capabilities\n    - Resource-efficient, suitable for consumer GPUs\n    - Excellent for fine-tuning with small datasets\n\n### 3.2. Stable Diffusion 3 Large\n\nAvailable via [Stability AI Developer Platform API](https://platform.stability.ai/)\n\n- **Parameters**: 8 billion\n- **Key Features**:\n    - Enhanced image quality and detail\n    - Greater capacity for handling complex prompts and styles\n    - Ideal for professional-grade projects requiring high resolution and fidelity\n\n### 3.3. Stable Diffusion 3 Large Turbo\n\nAvailable via [Stability AI Developer Platform API](https://platform.stability.ai/)\n\n- **Parameters**: 8 billion (with optimized inference time)\n- **Key Features**:\n    - The same high performance as SD3 Large\n    - Faster inference, making it suitable for real-time applications and rapid prototyping\n\n## 4. Technical Architecture of Stable Diffusion 3\n\nAt the core of Stable Diffusion 3 lies the Multimodal Diffusion Transformer (MMDiT) architecture. This innovative framework enhances how the model processes and integrates textual and visual information. Unlike its predecessors that utilized a single set of neural network weights for both image and text processing, Stable Diffusion 3 employs separate weight sets for each modality. This separation allows for more specialized handling of text and image data, leading to improved text understanding and spelling in the generated images.\n\n### 4.1. Components of MMDiT Architecture\n\n- **Text Embedders**: Stable Diffusion 3 uses a combination of three text embedding models, including two CLIP models and T5, to convert text into a format that the AI can understand and process.\n- **Image Encoder**: An enhanced autoencoding model is used for converting images into a form suitable for the AI to manipulate and generate new visual content.\n- **Dual Transformer Approach**: The architecture features two distinct transformers for text and images, which operate independently but are interconnected for attention operations. This setup allows both modalities to influence each other directly, enhancing the coherence between the text input and the image output.\n\n## 5. What’s New and Improved in Stable Diffusion 3?\n\n- **Adherence to Prompts**: SD3 excels in closely following the specifics of user prompts, particularly those that involve complex scenes or multiple subjects. This precision in understanding and rendering detailed prompts allows it to outperform other leading models such as DALL·E 3, Midjourney v6, and Ideogram v1, making it highly reliable for projects requiring strict adherence to given instructions.\n- **Text in Images**: With its advanced Multimodal Diffusion Transformer (MMDiT) architecture, SD3 significantly enhances the clarity and readability of text within images. By employing separate sets of weights for processing image and language data, the model achieves superior text comprehension and spelling accuracy. This is a substantial improvement over earlier versions of Stable Diffusion, addressing one of the common challenges in text-to-image AI applications.\n- **Visual Quality**: SD3 not only matches but in many cases surpasses the visual quality of images generated by its competitors. The images produced are not only aesthetically pleasing but also maintain high fidelity to the prompts, thanks to the model's refined ability to interpret and visualize textual descriptions. This makes SD3 a top choice for users seeking exceptional visual aesthetics in their generated imagery.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1089/readme01.webp\" alt=\"ComfyUI Stable Diffusion 3\" width=\"750\"/>\n\nFor detailed insights into the model, please visit [Stable Diffusion 3's research paper](https://stability.ai/news/stable-diffusion-3-research-paper), [Github](https://huggingface.co/stabilityai/stable-diffusion-3-medium)\n"
    },
    {
        "id": "1090",
        "readme": "## 1. ComfyUI IPAdapter and Attention Mask Workflow\n\nIn this ComfyUI workflow, we employ the IPAdapter Plus alongside the Attention Mask feature to enhance image generation. This setup ensures precise control, enabling sophisticated manipulation of both images and videos.\n\n### 1.1. Workflow Input Settings: Selecting Images and Videos\n\nBegin by selecting two distinct images, designated as Image A and Image B. Additionally, choose a video to serve as a mask, which will guide the transformation of Image A into Image B.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1090/readme01.webp\" alt=\"ComfyUI IPAdapter Workflow\" width=\"500\"/>\n\n### 1.2. Workflow Key Process: Utilizing IPAdapter Attention Mask\n\nThis transformation utilizes essential components such as AnimateDiff, multiple instances of IPAdapter Plus (IPAdapter V2), and various ControlNet models:\n\n**AnimateDiff**: AnimateDiff is used in its typical role within the workflow.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1090/readme02.webp\" alt=\"ComfyUI AnimateDiff\" width=\"500\"/>\n  \n**IPAdapter Plus**: Serving as the central element of this process, we employ two instances of IPAdapter Plus. The first IPAdapter Plus processes Image A and uses the video serving as an attention mask. The second IPAdapter Plus works with Image B, utilizing the inverse of the video as its mask. This configuration is pivotal for ensuring a smooth image transition. Furthermore, IPAdapter Plus is very good at preserving the original style of the input images.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1090/readme03.webp\" alt=\"ComfyUI IPAdapter Plus\" width=\"700\"/>\n\n**ControlNet**: We use two different ControlNet models. The first employs the ControlNet Tile model, and the second utilizes the ControlNet Lineart model. Both models take Image B as their input, which influences the final transformation. Adjusting the strength and steps of the ControlNet models according to your specific needs and the characteristics of your input image is recommended to achieve optimal results.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1090/readme04.webp\" alt=\"ComfyUI ControlNet\" width=\"700\"/>\n\n### 1.3. Rendering: Enhanced Video Upscaling with Dual Rendering Rounds\n\nTwo rounds of rendering ensure that the initial output is further refined by applying the ControlNet Tile model a second time, which enhances the video's upscaling process.\n\n## 2. How to Use IPAdapter Attention Mask\n\nThe IPAdapter Attention Mask is specifically designed to enhance precision and focus during the image generation process. Attention Mask allows you to apply a mask that clearly defines specific areas within an image that require adjustments or particular focus during the synthesis phase. Such targeted application is essential for advanced image synthesis, making it particularly valuable when working with sophisticated models like the IPAdapter plus.\n\n### 2.1. How IPAdapter Attention Mask works\n\nThe working principle of the IPAdapter Attention Mask begins with crafting a mask that precisely directs the image generation process. This mask, either in binary or grayscale format, determines how the IPAdapter plus should manipulate each section of the image based on varying intensities. Here's how it works:\n\n- **White areas**: These zones receive the full application of specified adaptations or focus, allowing the IPAdapter plus to fully express the intended adjustments.\n- **Black areas**: These regions undergo minimal to no changes, preserving their original state as the IPAdapter plus exerts little to no influence over them.\n- **Gray areas**: These parts feature a gradient of influence, where the IPAdapter plus gradually transitions between modified and unmodified areas, ensuring a smooth blend within the image.\n\nHere's an example of using IPAdapter Attention Mask.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1090/readme05.webp\" alt=\"IPAdapter Attention Mask\" width=\"650\"/>\n\nThis targeted guidance is crucial for achieving precise outcomes in image synthesis using the IPAdapter plus, enhancing both the accuracy and efficiency of the model.\n\n### 2.2. Advantages of Using IPAdapter Attention Mask\n\nUtilizing the Attention Mask with IPAdapter plus offers several key advantages:\n\n- **Customization**: The Attention Mask provides exceptional control over the generative capabilities of IPAdapter Plus, allowing you to apply specific stylizations or focused enhancements with great precision. This feature is instrumental in tailoring the final image to meet exact specifications.\n- **Quality Enhancement**: The Attention Mask significantly improves the quality of the generated images, ensuring photorealistic results. By fine-tuning how various image elements are integrated, the IPAdapter Plus can produce high-quality visuals that meet professional standards.\n- **Flexibility**: The flexibility offered by the Attention Mask in IPAdapter Plus supports the creation of complex compositions. It enables the integration of multiple styles or characters within different parts of a single image seamlessly, without the need for multiple generation cycles.\n\nBy harnessing the capabilities of the Attention Mask, you can achieve a higher level of detail and customization in your image generation projects with IPAdapter Plus, making it an invaluable tool for advanced digital image processing.\n"
    },
    {
        "id": "1091",
        "readme": "## 1. ComfyUI Workflow: IPAdapter Plus/V2 and ControlNet\n\nIn this workflow, we utilize IPAdapter Plus, ControlNet QRcode, and AnimateDiff to transform a single image into a video. Here’s a simplified breakdown of the process:\n\n1. **Select your input image** to serve as the reference for your video.\n2. **Choose a black and white video** to use as the input for the ControlNet QRCode Monster Model.\n3. **Optionally, use this video as the attention mask input in IPAdapter Plus**. This allows the IPAdapter to concentrate on specific areas of the video, enhancing focus and detail where needed.\n4. **Experiment with different weights and steps in ControlNet QRCode** to optimize the results. Adjusting these parameters can significantly enhance the quality and effectiveness of the transformation.\n\nThis streamlined approach enables you to effortlessly create stunning animations from just one image. By harnessing the powerful style transfer capabilities of IPAdapter Plus, coupled with its precise attention mask feature, and the dynamic visual effects from ControlNet QRCode, you can easily transform your image into impressive videos.\n\nFor more information about the Attention Mask, please visit the introduction to [IPAdapter Attention Mask](https://www.runcomfy.com/comfyui-workflows/comfyui-ipadapter-plus-attention-mask)\n"
    },
    {
        "id": "1092",
        "readme": "\nThis ComfyUI workflow specifically using IPAdapter Plus (also referred to as IPAdapter V2) along with QRCode and AnimateLCM model to efficiently create dynamic morphing videos. By focusing on the capabilities of IPAdapter Plus and integrating additional ComfyUI nodes, this workflow ensures a seamless and precise transformation of static images into captivating animations.\n\n### 1.1. Step-by-Step Guide to Using ComfyUI IPAdapter for Animation/Morphing Videos\n\n1. **Input Images for IPAdapter Plus**: Start by selecting four images, though this number can be adjusted to suit your project's requirements. These images will be processed sequentially to transition into each other throughout the video. For each image, use a separate IPAdapter Plus and generate attention masks with the \"Create Fade Mask Advanced\" Node. These masks are essential for directing the focus of IPAdapter Plus, ensuring a smooth morphing effect in the final video.\n2. **Animate with ControlNet QRCode**: Choose a black and white video to use as the input for the ControlNet QRCode Monster Model. This video will dictate the animation's flow, enhancing the morphing sequence's visual dynamics.\n3. **Speed Optimization with AnimateLCM**: Incorporate the AnimateLCM model within the AnimateDiff setup to accelerate the rendering process. AnimateLCM speeds up video generation by reducing the number of inference steps required and enhances the quality of results through Decoupled Consistency Learning. This allows the use of models that typically do not produce high-quality outcomes, making AnimateLCM an efficient tool for creating detailed animations. Therefore, whether to choose AnimateLCM should be based on your project's specific needs.\n4. **Final Rendering and Video Upscaling**: After setting up the workflow, render your morphing video. Assess the initial results, and if satisfactory, proceed to upscale the video to enhance both its resolution and the clarity of details.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1092/readme01.webp\" alt=\"ComfyUI IPAdapter Workflow for Animation\" width=\"750\"/>\n\n### 1.2. Additional Tips for Enhancing Your Animation/Morphing Video\n\n1. **Customize Attention Masks**: Adjust the parameters within the \"Create Fade Mask Advanced\" Node to fine-tune how the images blend into each other. This customization can help achieve more artistic or natural transitions depending on the desired effect.\n2. **Selecting the Right Reference Video for ControlNet**: The choice of video for the ControlNet QRCode impacts the morphing style. Experiment with different videos to see how they influence the transition dynamics between images.\n3. **Balancing Speed and Quality with AnimateLCM**: Experiment with the settings of AnimateLCM to find the optimal balance between speed and quality, ensuring the morphing effect is both smooth and rapid.\n\n\nThis workflow is inspired by [ipiv](https://civitai.com/models/372584/ipivs-morph-img2vid-animatediff-lcm). For more information, please visit his homepage.\n"
    },
    {
        "id": "1093",
        "readme": "## 1. ComfyUI Upscale Workflow: Superior 8K with SUPIR & Foolhardy Remacri\n\nThis ComfyUI Upscale Workflow leverages the strengths of the SUPIR Upscaler and the 4x Foolhardy Remacri model to achieve remarkable 8K resolution image upscaling. Renowned for its fidelity and detail enhancement capabilities, the SUPIR Upscaler is a top choice for high-resolution image upscaling. On the other hand, the 4x Foolhardy Remacri model, built on the robust ESRGAN architecture, excels in adding realistic textures and details, making it ideal for achieving clear and detailed visual outputs. Together, these workflow ensure that images are not only enlarged but also significantly enhanced in quality, meeting the demands of 8K imaging with exceptional clarity and precision. This synergy allows for superior image processing that meets professional and aesthetic standards.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1093/readme01.webp\" alt=\"ComfyUI Upscale Workflow - SUPIR and Foolhardy Remacri\" width=\"800\"/>\n\n## 2. ComfyUI Upscale Workflow Steps\n\n### 2.1. Step 1: Upscaling to 2K Pixels with SUPIR\n\nThe first step in the ComfyUI Upscale Workflow uses the SUPIR Upscaler to magnify the image to a 2000 pixel resolution, setting a high-quality foundation for further enhancement in the ComfyUI Upscale Workflow.\n\nBelow is an explanation of some key parameters related to SUPIR within the ComfyUI Upscale Workflow:\n\n**2.1.1. \"ImpactInt\" and\"Image Resize\" Node**: These nodes are crucial for resizing the image to a specific target size without distorting its original aspect ratio. It adjusts the image so that its largest dimension is exactly 2K pixels. To comply with the computational requirements of the SUPIR model, it also ensures that both the height and width of the resized image are divisible by 32. If the initial resizing results in dimensions that do not meet this condition, the node can apply padding (adding pixels to the image edges) or cropping (removing pixels from the image edges) as necessary. This adjustment helps to optimize the image for the SUPIR model's processing needs, enhancing the efficiency and compatibility of the upscaling process.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1093/readme02.webp\" alt=\"ComfyUI Upscale Workflow - SUPIR ImpactInt and Image Resize\" width=\"600\"/>\n\n**2.1.2. \"Tile Size\" and \"Tile Stride\" Node**: These parameters are crucial for balancing image quality against system resource usage during the upscaling process. The default setting for tile size is 1024 pixels, which defines the dimensions of the square segments the image is divided into for processing. A larger tile size can improve image quality by providing more context for each processing step but may increase memory demand. The tile stride, set by default to 512 pixels, determines the overlap between adjacent tiles. This overlap helps to prevent visible seams where tiles meet but uses additional computation resources.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1093/readme03.webp\" alt=\"ComfyUI Upscale Workflow - SUPIR Tile Size and Tile Stride\" width=\"800\"/>\n\nTo optimize these settings for your specific system and needs:\n\n- **Increase tile size and stride** for higher quality results if your system has ample video memory and processing power.\n- **Decrease tile size and stride** to reduce memory usage if you are limited by hardware resources, although this may affect the smoothness and detail of the upscaled image.\n\n**2.1.3. \"Noise Setting\" in \"SUPIR Sampler\" Node**: This parameter adjusts the in-painting amplitude, crucial for balancing the image's original appearance with the addition of new details during the upscaling process.\n\n- **Low Settings**: Keeping this setting lower preserves the original look and feel of the image. It minimizes alterations, maintaining the natural appearance without introducing unwanted changes. Typically, a setting around 1.001 is suggested for minimal deviation from the original.\n- **High Settings**: Increasing this setting amplifies the detail enhancement, which can be beneficial for images that require clearer definition or for artistic purposes where enhanced detail is desired. However, setting this too high (e.g., above 1.01) can lead to distortions such as unnatural sharpness or noise, which might detract from the overall image quality.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1093/readme04.webp\" alt=\"ComfyUI Upscale Workflow - SUPIR Noise Setting\" width=\"500\"/>\n\n**2.1.4. \"Sampler Method\" in \"SUPIR Sampler\" Node**: This parameter is crucial as it dictates the method used to process the image during upscaling, affecting both the detail and quality of the final output. Different samplers, such as the DPMPP2M, offer varying balances between image detail, sharpness, and the introduction of artifacts.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1093/readme05.webp\" alt=\"ComfyUI Upscale Workflow - SUPIR Sampler Method\" width=\"500\"/>\n\n- **Setting the Sampler**: The default sampler might be set to a basic model like EDM (Enhanced Diffusion Model), which provides a general enhancement. However, for more detailed results, switching to a more sophisticated sampler like DPMPP2M can significantly improve image clarity and reduce visual noise. DPMPP2M is particularly beneficial for images requiring high precision in detail, such as photographs with intricate textures or complex color gradients.\n- **Experimentation**: To find the optimal setting for your specific upscaling task, experiment with different samplers under various conditions. For example, try using a basic sampler for simple images and switch to DPPM for more complex scenes or when higher fidelity is needed. Observe how each sampler affects the overall image quality and adjust your settings based on the desired outcome.\n- **Adjusting Sampler Parameters**: Most samplers come with configurable parameters that can further refine the upscaling process. These might include adjustments for sharpness, noise level, and texture preservation. Access these settings in the \"SUPIR Sampler\" node and tweak them to suit the specific characteristics of the image you are working on.\n\n### 2.2. Step 2: Upscaling to 8K Pixels with 4x Foolhardy Remacri Model \n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1093/readme06.webp\" alt=\"ComfyUI Upscale Workflow - Foolhardy Remacri Upscale\" width=\"600\"/>\n\n**2.2.1. 4x Foolhardy Remacri High-Resolution Scaling**: This leverages the detailed base image from the SUPIR Upscaler, applying the 4x Foolhardy Remacri Model to quadruple the image size to 4 times that of the input, thus achieving an 8K resolution upscale.\n\n**2.2.2. what is 4x Foolhardy Remacri Model**\nThe 4x Foolhardy Remacri model is a sophisticated upscaling model that quadruples image resolution, making it perfect for producing visuals that are 4x times the original resolution. It uses the Enhanced Super-Resolution Generative Adversarial Network (ESRGAN) architecture, known for adding realistic textures and minimizing blurring and artifacts common in traditional upscaling.\n\nThis model excels at enhancing fine details like skin textures and intricate patterns, making it ideal for high-quality tasks such as upscaling digital art or restoring old photos where detail is paramount. Professionals and enthusiasts favor the 4x Foolhardy Remacri for its precision and clarity.\n\nIn combination with the SUPIR Upscaler, this model offers a workflow that preserves the original image's integrity while improving its quality to meet 8K standards. This approach ensures a technically superior and visually appealing enhancement process.\n\nBy adhering to these detailed steps and fine-tuning the parameters as needed, you can utilize the powerful capabilities of the SUPIR and 4x Foolhardy Remacri model to achieve professional-grade 8K image upscaling from the convenience of their own home.\n\nCredit is given to [Seven947](https://www.youtube.com/watch?v=sCJqmasct_c) for developing this ComfyUI Upscale workflow. For more information, please visit her youtube channel.\n"
    },
    {
        "id": "1094",
        "readme": "## 1. Making Audio-Reactive Videos with ComfyUI and TouchDesigner\n\nCreating audio-reactive videos is all about blending sound with visuals into one seamless artistic vibe. It's super engaging and lets your visuals dance along with the beats. You can start by whipping up some visual content in ComfyUI—it’s a real time-saver for rendering. Then, bring those visuals into TouchDesigner to sync them up with your tunes and really make them pop.\n\n## 2. Mastering the ComfyUI for Enhanced Visual Rendering\n\nUsing ComfyUI for rendering opens up a world of creative possibilities. Start by selecting your initial inputs: a simple black and white video and a reference image that captures the style you aim to achieve. This setup allows you to explore various artistic directions—experimenting with different videos and reference images can lead to exciting discoveries.\n\nIn this workflow, we leverage several powerful components:\n\n- **AnimateDiff** is used for video generation, creating intricate effects based on the differences between video frames.\n- **AnimateLCM** accelerates the rendering process, ensuring efficient production times without sacrificing quality.\n- **IPAdapter Plus** analyzes the style of your input image and adapts the video to match this aesthetic, infusing your final product with the desired artistic flair.\n- **ControlNet** manages the animation flow, providing control over how the animation progresses and interacts with audio inputs.\n\nWhen selecting ControlNets, consider the specific needs of your video:\n\n- If your video includes 3D effects, opt for a depth model from ControlNet to enhance the dimensional aspects.\n- For styles that are more flat and graphic, Lineart or Canny models are excellent choices for achieving sharp, clean lines.\n\nBy experimenting with different combinations of these nodes, you can significantly boost the creativity and uniqueness of your rendered video.\n\n## 3. Synchronizing Visuals with Audio Using TouchDesigner\n\nAfter your video is all generated in ComfyUI, it’s time to bring it into TouchDesigner. This is where you sync it up with your audio. TouchDesigner’s got these awesome operators that analyze the beats and vibes of your music. You can use audio signals, like beats, to adjust your video playback so it matches the sound perfectly. It’s like making your visuals dance to the music, creating a cool experience that feels as good as it looks.\n\n\nThis workflow is inspired by [cerspense](https://civitai.com/models/396716/csp-audioreactive-depthgif-cn-workflow) but has undergone some modifications.\n"
    },
    {
        "id": "1095",
        "readme": "## What is PuLID?\n\nPuLID (Pure and Lightning ID customization) is a novel method for tuning-free identity (ID) customization in text-to-image generation models. It aims to embed a specific ID (e.g. a person's face) into a pre-trained text-to-image model without disrupting the model's original capabilities. This allows generating images of the specific person while still being able to modify attributes, styles, backgrounds etc. using text prompts.\n\n<video src=\"https://cdn.runcomfy.net/workflow_assets/1095/readme02.mp4\" alt=\"PuLID Examples\" width=\"750\"/>\n\n\n### PuLID incorporates two key components:\n\n1. A \"Lightning T2I\" branch that rapidly generates high-quality images conditioned on the ID in just a few denoising steps, alongside the standard diffusion branch. This enables calculating an accurate ID loss to improve the fidelity of the generated face.\n2. Contrastive alignment losses between the Lightning T2I paths with and without ID conditioning. This instructs the model on how to embed the ID information without contaminating the model's original prompt-following and image generation capabilities.\n\n## How PuLID Works\n\nPuLID's architecture consists of a conventional diffusion training branch and the novel Lightning T2I branch:\n\n1. In the diffusion branch, PuLID follows the standard diffusion training process of iterative denoising. The ID condition is cropped from the target training image.\n2. The Lightning T2I branch leverages recent fast sampling methods to generate a high-quality image conditioned on the ID prompt in just 4 denoising steps, starting from pure noise.\n3. Within the Lightning T2I branch, two paths are constructed - one conditioned only on the text prompt, the other conditioned on both the ID and text prompt. The UNET features of these paths are aligned using contrastive losses:\n    - A semantic alignment loss ensures the model's response to the text prompt is similar with and without ID conditioning. This preserves the model's original prompt-following ability.\n    - A layout alignment loss maintains consistency of the generated image layout before and after ID insertion.\n4. The Lightning T2I branch enables calculating an accurate ID loss between the generated face embedding and the real ID embedding, since it produces a clean, denoised output face. This improves the fidelity of the generated ID.\n\n## How to Use ComfyUI PuLID\n\nUsing the ComfyUI PuLID workflow to apply ID customization to a model involves several key parameters in the \"Apply PuLID\" node:\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1095/readme01.webp\" alt=\"ComfyUI PuLID\" width=\"750\"/>\n\n### \"Apply PuLID\" Required Inputs:\n\n- model: The base text-to-image diffusion model to customize with the specific ID. This is typically a pre-trained model like Stable Diffusion.\n- pulid: The loaded PuLID model weights, which define how the ID information is inserted into the base model. Different PuLID weight files can be trained to prioritize either ID fidelity or preserving the model's original generation style.\n- eva_clip: The loaded Eva-CLIP model for encoding facial features from the ID reference image(s). Eva-CLIP produces a meaningful facial feature embedding.\n- face_analysis: The loaded InsightFace model for recognizing and cropping the face in the ID reference image(s). This ensures only relevant facial features are encoded.\n- image: The reference image or images depicting the specific ID to insert into the model. Multiple images of the same identity can be provided to improve the ID embedding.\n- method: Selects the ID insertion method, with options \"fidelity\", \"style\" and \"neutral\". \"fidelity\" prioritizes maximum likeness to the ID reference even if generation quality degrades. \"style\" focuses on preserving the model's original generation capabilities with a lower-fidelity ID. \"neutral\" balances the two.\n- weight: Controls the strength of the ID insertion, from 0 (no effect) to 5 (extremely strong). Default is 1. Higher weight improves ID fidelity but risks overriding the model's original generation.\n- start_at: The denoising step (as a percentage from 0 to 1) to start applying the PuLID ID customization. Default is 0, starting the ID insertion from the first denoising step. Can be increased to start ID insertion later in the denoising process.\n- end_at: The denoising step (as a percentage from 0 to 1) to stop applying the PuLID ID customization. Default is 1, applying ID insertion till the end of denoising. Can be reduced to stop ID insertion before the final denoising steps.\n\n### \"Apply PuLID\" Optional Inputs:\n\n- attn_mask: An optional grayscale mask image to spatially control where the ID customization is applied. White areas of the mask receive the full ID insertion effect, black areas are unaffected, gray areas receive partial effect. Useful for localizing the ID to just the face region.\n\n### \"Apply PuLID\" Outputs:\n\n- MODEL: The input model with the PuLID ID customization applied. This customized model can be used in other ComfyUI nodes for image generation. The generated images will depict the ID while still being controllable via prompt.\n\nAdjusting these parameters allows fine-tuning the PuLID ID insertion to achieve the desired balance of ID fidelity and generation quality. Generally, a weight of 1 with method \"neutral\" provides a reliable starting point, which can then be adjusted based on the results. The start_at and end_at parameters provide further control over when the ID takes effect in the denoising, with the option to localize the effect via an attn_mask.\n\n\nFor more information, please visit [github](https://github.com/cubiq/PuLID_ComfyUI)\n"
    },
    {
        "id": "1096",
        "readme": "## 1. What is IC-Light?\n\nIC-Light is an AI-based image editing tool that integrates with Stable Diffusion models to perform localized edits on generated images. It works by encoding the image into a latent space representation, applying edits to specific regions, and then decoding the modified latent representation back into an image. This approach allows for precise control over the editing process while preserving the overall style and coherence of the original image.\n\nNow there’re two models released: text-conditioned relighting model and background-conditioned model. Both types take foreground images as inputs.\n\n## 2. How IC-Light Works\n\nUnder the hood, IC-Light leverages the power of Stable Diffusion models to encode and decode images. The process can be broken down into the following steps:\n\n2.1. Encoding: The input image is passed through the Stable Diffusion VAE (Variational Autoencoder) to obtain a compressed latent space representation.\n2.2. Editing: The desired edits are applied to specific regions of the latent representation. This is typically done by concatenating the original latent with a mask indicating the areas to be modified, along with the corresponding edit prompts.\n2.3. Decoding: The modified latent representation is passed through the Stable Diffusion decoder to reconstruct the edited image.\nBy operating in the latent space, IC-Light can make localized edits while maintaining the overall coherence and style of the image.\n\n## 3. How to Use ComfyUI IC-Light\n\nThe main node you'll be working with is the \"IC-Light Apply\" node, which handles the entire process of encoding, editing, and decoding your image.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1096/readme01.webp\" alt=\"ComfyUI IC-Light\" width=\"650\"/>\n\n### 3.1. \"IC-Light Apply\" Input Parameters:\n\nThe \"IC-Light Apply\" node requires three main inputs:\n\n- model: This is the base Stable Diffusion model that will be used for encoding and decoding your image.\n- ic_model: This is the pre-trained IC-Light model that contains the necessary weights for the editing process.\n- c_concat: This is a special input that combines your original image, the mask indicating which areas to edit, and the edit prompts that define how those areas should be modified.\n\nTo create the c_concat input:\n\n1. Use the VAEEncodeArgMax node to encode your original image. This node ensures that the most probable latent representation of your image is obtained.\n2. Use the ICLightApplyMaskGrey node to create a masked version of your image. This node takes your original image and a mask as input, and it outputs a version of the image where the non-masked areas are greyed out.\n3. Create latent representations of your edit prompts. These prompts will guide the modifications made to the selected regions of your image.\n4. Combine the latent representations of your original image, mask, and edit prompts into a single input for the \"IC-Light Apply\" node.\n\n### 3.2. \"IC-Light Apply\" Output Parameters:\n\nAfter processing your inputs, the \"IC-Light Apply\" node will output a single parameter:\n\n- model: This is the patched Stable Diffusion model with the IC-Light modifications applied.\n\nTo generate your final edited image, simply connect the output model to the appropriate nodes in your ComfyUI workflow, such as the KSampler and VAEDecode nodes.\n\n### 3.3. Tips for Best Results:\n\n1. Use high-quality masks: To ensure that your edits are precise and effective, make sure that your masks accurately outline the regions you want to modify.\n2. Experiment with different edit prompts: The edit prompts are what guide the modifications made to the selected regions of your image. Feel free to try different prompts to achieve the desired effect, and don't hesitate to refine your prompts based on the results you get.\n3. Balance global and local edits: While IC-Light is great for making localized edits, it's important to consider the overall composition and coherence of your image. Try to find a balance between focused edits and global adjustments to maintain the integrity of your generated artwork.\n\nFor more information, please visit [github](https://github.com/huchenlei/ComfyUI-IC-Light-Native)\n"
    },
    {
        "id": "1097",
        "readme": "## 1. What is MistoLine?\n\nMistoLine is an advanced deep learning model designed to generate detailed and aesthetically pleasing line art from input images. Utilizing the SDXL-ControlNet framework, MistoLine is trained on a vast and diverse dataset of artworks, allowing it to extract and reproduce the essential lines and edges that define an image's structure and form. It demonstrates high accuracy and stability, adapting to various types of line art inputs, including hand-drawn sketches and model-generated outlines. Capable of producing high-quality images with a minimum resolution of 1024px on the shorter side, MistoLine excels in generalizing across diverse line art conditions, eliminating the need for multiple ControlNet models for different preprocessors. This makes it a perfect tool for \"Sketch to Image\" transformations.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1097/readme01.webp\" alt=\"MistoLine examples\" width=\"750\"/>\n\nThe key features of MistoLine include:\n\n1. **High-quality lineart generation**: MistoLine produces clean, crisp, and well-defined lines that capture the essence of the input image.\n2. **Versatility**: The model can handle a wide range of art styles, from realistic to stylized, making it suitable for various artistic projects, including \"Sketch to Image\" conversions.\n3. **Customization options**: MistoLine offers adjustable parameters to fine-tune the lineart output according to your specific needs and preferences.\n4. **Efficiency**: With MistoLine, you can generate lineart quickly and effortlessly, saving valuable time in your artistic process.\n\n## 2. How MistoLine Works\n\nMistoLine is powered by a convolutional neural network (CNN) that has been trained to translate input images into high-quality lineart. Let's break down the process.\n\n### 2.1. The Model's Structure\n\nMistoLine uses an encoder-decoder structure with skip connections. The encoder takes your input image and breaks it down into smaller, more manageable pieces, capturing important features like edges and shapes. The decoder then puts these pieces back together, creating a detailed lineart output. The skip connections help preserve fine details throughout the process, ensuring an accurate \"Sketch to Image\" translation.\n\n### 2.2. Training the Model\n\nTo train MistoLine, the developers used a huge dataset of original artworks and their corresponding lineart. The model learned by comparing its generated lineart with the ground truth lineart, and it kept adjusting itself until it could produce results that were very close to the real thing.\n\nThe training process used a combination of techniques to ensure the generated lineart was both accurate and visually appealing. This involved a lot of complex math and computational power, but the end result is a model that can create stunning lineart with ease, facilitating efficient \"Sketch to Image\" transformations.\n\n### 2.3. Inference\n\nOnce trained, MistoLine can be used for inference, where it takes an input image and generates the corresponding lineart. The inference process is relatively fast, allowing for quick generation of lineart from new images, perfect for \"Sketch to Image\" applications.\n\nFor more information, please visit [github](https://github.com/TheMistoAI/MistoLine).\n\nThis workflow uses the MistoLine-SDXL-ControlNet developed by TheMisto.ai.\n"
    },
    {
        "id": "1098",
        "readme": "## AI Renderings of 3D Animations: Blender + ComfyUI\n\nThis innovative workflow, developed by the talented [Mickmumpitz](https://www.youtube.com/@mickmumpitz/videos), combines Blender and ComfyUI to produce stunning 3D animations rendered with AI. We highly recommend checking out his YouTube channel for more inspiration.\n\nIn the following, we used different materials to reproduce his approach. Here are the steps:\n\n🌟 [Download all our Blender materials for experiments from this link.](https://cdn.runcomfy.net/workflow_assets/1098/RunComfy_example_1098.zip)\n\n## Part 1: Using Blender to Create 3D Animations and Render Passes\n\n### 1. Set Up a 3D Scene in Blender\n\n- Start by importing or modeling the necessary 3D assets, such as characters, environments, and props. Arrange these assets within the scene, position the cameras, and set up any required animations. Remove existing materials, textures, and lights, as ComfyUI will generate these during the rendering process.\n\n### 2. Render Depth Pass\n\n- The depth pass provides essential distance information to enhance depth perception, which will be utilized in ComfyUI.\n- In Blender, go to `View Layer Properties`, activate the `Z` pass, and render the image. In the `Compositing` tab, connect a viewer node to the depth output. Normalize the depth values using a `Map Range` node to create a black-and-white gradient representing the scene's depth.\n\n### 3. Render Outline Pass\n\n- The outline pass produces line art edges that define the shapes and silhouettes of objects for use in ComfyUI.\n- In Blender, use the Freestyle tool to create outlines based on the 3D geometry. Set the color to white and adjust the line thickness. Render the image and process it in the `Compositing` tab.\n\n### 4. Render Mask Pass\n\n- The mask pass assigns unique colors to different objects. Make sure to note the Hex Codes for these colors, as they will be used for specific AI prompts in ComfyUI.\n- In Blender, assign simple emission shaders with distinct colors to each object. Render the image and save it.\n\nFor detailed instructions on these steps, refer to [Mickmumpitz](https://www.youtube.com/@mickmumpitz/videos)'s YouTube tutorial if you are unfamiliar with Blender.\n\n**After completing Part 1, you will have created three types of image sequences: mask images, depth images, and outline images. These will be used in the next step with ComfyUI.**\n\n## Part 2: Using ComfyUI to Render AI Animations\n\nMickmumpitz's ComfyUI workflow consists of several key components:\n\n### 1. Load Image Sequences\n\n- **Load Mask Sequences**: Load the mask sequence and then use the \"regional conditioning by color mask\" node to separate the objects in the mask image. Enter the Hex Code obtained from Step 1 into the \"mask_color\" field. This node will then automatically segment objects based on the assigned colors. 🌟 Note: We use specific Hex Codes for our materials in this workflow. If you use a different mask image, make sure to enter the corresponding Hex Code in the \"mask_color\" field. This ensures the \"regional conditioning by color mask\" node correctly segments the objects.  \n- **Load Depth Sequences**: Load the depth sequences to provide information about the distance of objects in the scene.\n- **Load Outline Sequences**: Load the outline sequences to define the shapes and edges of objects in the scene.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1098/readme01.webp\" alt=\"Using ComfyUI to Render AI Animations\" width=\"750\"/>\n\n### 🌟 How to Load Image Sequences by Path\n**Organize Your Sequences**\n- Place your mask sequences in a folder, for example, `{mask}`.\n- Place your depth sequences in a separate folder, for example, `{depth}`.\n- Place your outline sequences in another folder, for example, `{outline}`.\n\n**Upload Folders**\n- Use our file browser to upload these folders to the `ComfyUI/input` directory.\n\n**Directory Path**\n- The full directory path should be: `/home/user/ComfyUI/input/{folder_name}`.\n- For example: `/home/user/ComfyUI/input/{mask}`.\n\n### 2. Use Text Prompts to Define Visual Effects\nFor **Mask Sequences**, use text prompts to specify the desired visual effects for each object in the scene.\n- **Master Prompt**: This prompt sets the overall style and lighting for the entire scene. It dictates the general mood, atmosphere, and visual tone that the final rendering should achieve.\n- **Regional Prompts**: These prompts provide detailed descriptions for specific objects or areas within the scene. Each prompt should correspond to a distinct object or region, ensuring that every element is accurately represented as intended.\n- **Negative Prompt**: This prompt lists the elements that should be excluded from the rendering. It helps to prevent certain features or objects from appearing in the final output, ensuring the scene remains focused and free from unwanted details.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1098/readme02.webp\" alt=\"Using ComfyUI to Render AI Animations\" width=\"750\"/>\n\n### 3. ControlNet Module\n\n- Input the depth sequences into the ControlNet Depth model.\n- Input the outline sequences into the ControlNet Canny model.\n\n### 4. AnimateDiff Module\n\n- Use this module to render smooth animations from the processed sequences.\n\n### 5. Optional IPAdapter Usage\n\n- Use the IPAdapter for additional conditioning guidance to improve the consistency and quality of the generated images.\n\nBy leveraging Blender's precise 3D data alongside the powerful image synthesis capabilities of Stable Diffusion via ComfyUI, you can generate incredibly photorealistic or stylized 3D animations with full creative control.\n"
    },
    {
        "id": "1099",
        "readme": "## 1. What is ToonCrafter?\n\nToonCrafter is an advanced AI technique that interpolates between two cartoon images using pre-trained image-to-video diffusion priors. This allows ToonCrafter to generate interpolated videos from two distinct cartoon images, creating a seamless transition between them. It supports video generation of up to 16 frames with a resolution of 512x320 pixels.\n\n## 2. How ToonCrafter works?\n\nToonCrafter is an AI tool designed to create smooth animations from static cartoon images using advanced AI techniques. It employs Latent Diffusion Models (LDMs) to encode images into a compressed latent space, where noise is added and then progressively removed through a denoising process. This method generates intermediate frames between the original images, resulting in fluid animations​.\n\nA notable feature of ToonCrafter is its Toon Rectification Learning. This process adapts the AI model, originally trained on live-action videos, to understand and generate cartoon animations. By fine-tuning the model with a large dataset of high-quality cartoon videos, ToonCrafter learns the unique motion and stylistic elements of cartoons, such as exaggerated movements and simpler textures​.\n\nToonCrafter also incorporates a Detail Injection and Propagation mechanism. This uses a dual-reference-based 3D decoder to maintain the visual fidelity of the generated frames. The decoder analyzes and injects pixel-level details from the input frames into the new frames, ensuring consistency with the original artwork and preventing visual artifacts​.\n\nAdditionally, ToonCrafter offers sketch-based controllable generation, allowing animators to provide sketches that guide the creation of intermediate frames. This feature gives artists more control over the animation process, enabling them to specify particular poses or movements and ensuring that the final animation aligns with their vision​​.\n\n## 3. How to Use ComfyUI ToonCrafter\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1099/readme01.webp\" alt=\"ComfyUI ToonCrafter Node\" width=\"500\"/>\n\n### ComfyUI ToonCrafter Node: Input Parameters\n\nThe ToonCrafter node requires several input parameters that determine the behavior and output of the interpolation process. Here's a detailed explanation of each parameter:\n\n- **image**: The first input image (type: IMAGE).\n- **image2**: The second input image (type: IMAGE).\n- **ckpt_name**: The name of the checkpoint to use (type: STRING, options: list of available checkpoints).\n- **prompt**: A textual description to guide the interpolation (type: STRING, supports multiline and dynamic prompts).\n- **seed**: A seed value for random number generation to ensure reproducibility (type: INT, default: 123).\n- **eta**: The parameter controls the scale of the noise added during the diffusion process. In diffusion models, noise is gradually reduced to generate the final image or frame. Adjusting the eta value determines how much noise is introduced at each iteration of this process. (type: FLOAT, default: 1.0, range: 0.0 to 15.0, step: 0.1).\n- **cfg_scale**: The classifier-free guidance scale (type: FLOAT, default: 7.5, range: 1.0 to 15.0, step: 0.5).\n- **steps**: Number of diffusion steps (type: INT, default: 50, range: 1 to 60, step: 1).\n- **frame_count**: The number of frames to generate (type: INT, default: 10, range: 5 to 30, step: 1).\n- **fps**: Frames per second for the output video (type: INT, default: 8, range: 1 to 60, step: 1).\n\n### ComfyUI ToonCrafter Node: Output Parameters\n\nThe output of the ToonCrafter node is a sequence of interpolated frames, which can be used to create a video. Here's what you can expect:\n\n- **IMAGE**: The generated frames of the interpolated video. These frames are returned as a tensor and can be further processed or saved as a video file.\n"
    },
    {
        "id": "1100",
        "readme": "## 1. What is Omost?\n\nOmost, short for \"Your image is almost there!\", is an innovative project that converts Large Language Models' (LLM) coding capabilities to image generation, or more precisely, image composing capabilities. The name \"Omost\" has a dual meaning: it implies that every time you use Omost, your image is almost complete, and it also signifies \"omni\" (multi-modal) and \"most\" (getting the most out of it).\n\nOmost provides pretrained LLM models that generate code to compose image visual content using Omost's virtual Canvas agent. This Canvas can then be rendered by specific implementations of image generators to create the final images. Omost is designed to simplify and enhance the image generation process, making it accessible and efficient for AI artists.\n\n## 2. How Omost Works\n\n### 2.1. Canvas and Descriptions\n\nOmost uses a virtual Canvas where elements of the image are described and positioned. The Canvas is divided into a grid of 9x9=81 positions, allowing precise placement of elements. These positions are further refined into bounding boxes, providing 729 different possible locations for each element. This structured approach ensures that elements are placed accurately and consistently.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1100/readme01.webp\" alt=\"How Omost Works\" width=\"500\"/>\n\n### 2.2. Depth and Color\n\nElements on the Canvas are assigned a `distance_to_viewer` parameter, which helps sort them into background-to-foreground layers. This parameter acts as a relative depth indicator, ensuring that closer elements appear in front of those that are further away. Additionally, the `HTML_web_color_name` parameter provides a coarse color representation for initial rendering, which can be refined using diffusion models. This initial color helps in visualizing the composition before fine-tuning.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1100/readme02.webp\" alt=\"How Omost Works\" width=\"500\"/>\n\n### 2.3. Prompt Engineering\n\nOmost uses sub-prompts, which are brief, self-contained descriptions of elements, to generate detailed and coherent image compositions. Each sub-prompt is less than 75 tokens and describes an element independently. These sub-prompts are merged into complete prompts for the LLM to process, ensuring that the generated images are accurate and semantically rich. This method ensures that the text encoding is efficient and avoids semantic truncation errors.\n\n### 2.4. Regional Prompter\n\nOmost implements advanced attention manipulation techniques to handle regional prompts, ensuring that each part of the image is generated accurately based on the given descriptions. Techniques such as attention score manipulation ensure that the activations within masked areas are encouraged, while those outside are discouraged. This precise control over attention results in high-quality, region-specific image generation.\n\n## 3. Detailed Explanation of ComfyUI Omost Nodes\n\n### 3.1. Omost LLM Loader Node\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1100/readme03.webp\" alt=\"How Omost Works\" width=\"400\"/>\n\n#### Input parameters of Omost LLM Loader Node\n\n- `llm_name`: The name of the pretrained LLM model to load. Available options include:\n    - `lllyasviel/omost-phi-3-mini-128k-8bits`\n    - `lllyasviel/omost-llama-3-8b-4bits`\n    - `lllyasviel/omost-dolphin-2.9-llama3-8b-4bits`\n\nThis parameter specifies which model to load, each offering different capabilities and optimizations.\n\n#### Output parameters of Omost LLM Loader Node\n\n- `OMOST_LLM`: The loaded LLM model.\n\nThis output provides the loaded LLM, ready to generate image descriptions and compositions.\n\n### 3.2. Omost LLM Chat Node\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1100/readme04.webp\" alt=\"How Omost Works\" width=\"400\"/>\n\n#### Input parameters of Omost LLM Chat Node\n\n- `llm`: The LLM model loaded by the `OmostLLMLoader`.\n- `text`: The text prompt to generate an image. This is the main input where you describe the scene or elements you want to generate.\n- `max_new_tokens`: Maximum number of new tokens to generate. This controls the length of the generated text, with a higher number allowing more detailed descriptions.\n- `top_p`: Controls the diversity of the generated output. A value closer to 1.0 includes more diverse possibilities, while a lower value focuses on the most likely outcomes.\n- `temperature`: Controls the randomness of the generated output. Higher values result in more random outputs, while lower values make the output more deterministic.\n- `conversation` (Optional): Previous conversation context. This allows the model to continue from previous interactions, maintaining context and coherence.\n\n#### Output parameters of Omost LLM Chat Node\n\n- `OMOST_CONVERSATION`: The conversation history, including the new response. This helps in tracking the dialogue and maintaining context across multiple interactions.\n- `OMOST_CANVAS_CONDITIONING`: The generated Canvas conditioning parameters for rendering. These parameters define how the elements are placed and described on the Canvas.\n\n### 3.3. Omost Render Canvas Conditioning Node\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1100/readme05.webp\" alt=\"How Omost Works\" width=\"400\"/>\n\n#### Input parameters of Omost Render Canvas Conditioning Node\n\n- `canvas_conds`: The Canvas conditioning parameters. These parameters include detailed descriptions and positions of elements on the Canvas.\n\n#### Output parameters of Omost Render Canvas Conditioning Node\n\n- `IMAGE`: The rendered image based on the Canvas conditioning. This output is the visual representation of the described scene, generated from the conditioning parameters.\n\n### 3.4. Omost Layout Cond Node\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1100/readme06.webp\" alt=\"How Omost Works\" width=\"400\"/>\n\n#### Input parameters of Omost Layout Cond Node\n\n- `canvas_conds`: The Canvas conditioning parameters.\n- `clip`: The CLIP model for text encoding. This model encodes the text descriptions into vectors that can be used by the image generator.\n- `global_strength`: The strength of the global conditioning. This controls how strongly the overall description affects the image.\n- `region_strength`: The strength of the regional conditioning. This controls how strongly the specific regional descriptions affect their respective areas.\n- `overlap_method`: The method to handle overlapping areas (e.g., `overlay`, `average`). This defines how to blend overlapping regions in the image.\n- `positive` (Optional): Additional positive conditioning. This can include extra prompts or conditions to enhance specific aspects of the image.\n\n#### Output parameters of Omost Layout Cond Node\n\n- `CONDITIONING`: The conditioning parameters for image generation. These parameters guide the image generation process, ensuring that the output matches the described scene.\n- `MASK`: The mask used for the conditioning. This helps in debugging and applying additional conditions to specific regions.\n\n### 3.5. Omost Load Canvas Conditioning Node\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1100/readme07.webp\" alt=\"How Omost Works\" width=\"400\"/>\n\n#### Input parameters of Omost Load Canvas Conditioning Node\n\n- `json_str`: The JSON string representing the Canvas conditioning parameters. This allows loading predefined conditions from a JSON file.\n\n#### Output parameters of Omost Load Canvas Conditioning Node\n\n- `OMOST_CANVAS_CONDITIONING`: The loaded Canvas conditioning parameters. These parameters initialize the Canvas with specific conditions, ready for image generation.\n"
    },
    {
        "id": "1101",
        "readme": "## ComfyUI IC-Light workflow for Video Relighting\n\nUnlock the full potential of your video projects with the **ComfyUI IC-Light** workflow, designed specifically for enhancing the lighting in your \"Human\" character videos using a lightmap. With **ComfyUI IC-Light**, you can easily transform your videos by simply providing your prompts and lightmap elements, such as shapes and neon lights. The tool will create a new video with significantly improved lighting.\n\n## HOW TO USE ComfyUI IC-Light workflow\n\n1) Upload Source Video: Start by uploading the video you want to enhance.\n2) Upload Light Map Video or Single Light Map Image: Choose a light map video or a single image to serve as your new lighting template.\n3) Enter Load Cap and other Settings, same settings should be in Light Map Video.\n4) Enter Prompts which describes your new light settings like Sunlight or neon lights.\n5) Select You Model. Realistic model is preferred.\n6) Change Light Map Composing and other Settings if needed.\n7) Hit render.\n\nOutputs will be saved in ComfyUI > Outputs\n\n## Inputs_1 - Settings\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme01.webp\" alt=\"\" width=\"700\"/>\n\nHere we have 5 Settings:\n\n- Sampler Steps: It determines the total number of steps KSampler take to render an image. It should not be changed. [Default Value 26]\n- Detail Enhancer: It Increase the minute Details in the Final Render. [Use value Between 0.1 and 1]\n- Seed: It controls the Generations Seed for every KSamplers.\n- Sampler CFG: This Controls the CFG values of the KSamplers.\n- Refiner Upscale: This works like the Highres Fix value. [Use between 1.1 – 1.6 for best results]\n\n## Prompts\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme02.webp\" alt=\"\" width=\"700\"/>\n\n- Positive Prompt: Enter prompts which best describes your Image with the new lighting.\n- Negative Prompts: It is configured to give best results. Feel free to edit it.\n- Clip Text Encode nodes: It helps in Encoding Text to maximize quality. Leave it at “full”\n\n## Models and Loras\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme03.webp\" alt=\"\" width=\"700\"/>\n\n- Checkpoint: Choose any **realistic SD 1.5** model for accurate results. Feel to choose any SD 1.5 model for stylistic results.\n- Loras: [Optional] Choose any loras from the give list if you desire. Do not use them at full strength. Use around 0.5-0.7 for best effect\n\n## Input Source Video\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme04.webp\" alt=\"\" width=\"700\"/>\n\n- Upload Source Video: Here you click and upload your Human Character video you want to change light of.\n    - It should be under 100 MB, Comfy will fail to upload large size.\n    - It should be no longer than 15-20 seconds. It may fail to render longer videos\n    - It should be in 720p or lower\n    - Use Skip Frames Nodes if you need to skip some starting frames. [Light Map video’s will also skip this much frames]\n- Fit Image Size Limiter: Here you cap the rendering resolution, whether be landscape or portrait, max resolution will always be under or equal to Set value.\n    - Use Value between 800 – 1200 for best results. [This will impact Vram]\n    \n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme05.webp\" alt=\"\" width=\"500\"/>    \n\n**Important**: Use Frames Load Cap of 10 to Test Out First\n- Use About 200 - 300 frames at 1000 – 1200 fit size, if you have 24 GB.\n- Use 0 if you want to render all frames. [Not Recommended for longer videos]\n\n## Mask and Depth Settings\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme06.webp\" alt=\"\" width=\"700\"/>    \n\n- Mask: It uses the Robust Video Matting, The default settings are fine.\n- Depth ControlNet: It uses the latest DepthAnything v2 models.\n    - Strength and End Percent are Set at 75% to give optimal results\n    - Use Co [Adaptor Depth](https://huggingface.co/TencentARC/T2I-Adapter/blob/main/models/coadapter-depth-sd15v1.pth) for best results.\n\n## Light Map\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme07.webp\" alt=\"\" width=\"700\"/>    \n\n- Upload Light Map: Click and upload a light map video you want.\n  - It will auto scale to the Source video’s dimensions\n  - Make Sure it is longer or equal to source video’s dimensions else it will give error.\n- Light Map ControlNet: This light map is also used as Light controlnet using [this model](https://civitai.com/models/80536?modelVersionId=85428)\n- CN Strength and End Percent: Use Low values here, Higher values may cause over exposure or Sharp Light transition.\n\n## Single Light Map\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme08.webp\" alt=\"\" width=\"700\"/>    \n\n- To Use a Single Image as light map, unmute these nodes and connect the reroute node to “Pick one Input” Node.\n\n## AnimateDIff\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme09.webp\" alt=\"\" width=\"700\"/>    \n\n- Load Animatediff Model: You can use any model for different effects.\n- Animatediff Other nodes: You need to have some knowledge of animatediff to change other settings [ You can find them [here](https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved/tree/main) ]\n- Settings SMZ: This is node to increase more quality of the model Pipeline, all settings are predefined to work well.\n\n## Composing of Light Map and IC Conditioning\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme10.webp\" alt=\"\" width=\"700\"/>    \n\n- The Above top Adjustment Nodes (In Grey color) are there to control the Conditioning of the IC-Light Conditioning, to make is less contrast and control brightness.\n- Generate New background: When Disabled it will the original image inputs and try to map the details similar to the source video’s background according to “Background Prompts” if present in the positive prompt box\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme11.webp\" alt=\"\" width=\"400\"/>    \n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme12.webp\" alt=\"\" width=\"400\"/>    \n\n[1girl, sunlight, sunset, white shirt, black short jeans, **interior, room**]\n\n- When Generate New background is Enabled: It will generate a new background considering the depth\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme13.webp\" alt=\"\" width=\"400\"/>  \n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme14.webp\" alt=\"\" width=\"400\"/>    \n\n[1girl, sunlight, sunset, **nature in the background, sky**]   \n\nAlso Depth ControlNet’s Strength and End Percent was decreased to 45 % to have an Open Area in the background.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme15.webp\" alt=\"\" width=\"700\"/>  \n\n- Light Map on Top: When True, Light map will be on top of the Source video and more dominant, When False Source will be on top, more dominant and Brighter\n- Subject Affecting Area: 2 Blending modes works the best\n    - Multiply: It will darker the shadow areas according to light map on top or bottom\n    - Screen: It will brighten the shadow area according to light map on top or bottom\n    - Blend Factor is for the intensity.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme16.webp\" alt=\"\" width=\"700\"/>  \n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme17.webp\" alt=\"\" width=\"700\"/>  \n\n- Overall Adjustments: This will control the brightness, contrast, gamma, tint of the Final Processed Light map from above.\n- Image Remap: Use this node to control the overall global brightness and Darkness the whole image.\n    - Higher min value will brighten the scene\n    - Lower Max values will make the Scene Darker and can convert the Brighter areas into morphing objects like the QrCode Monster CN\n    - Use mostly the Min value to 0.1 or 0.2 to light up a scene a little bit.\n    - Min Value 0 will have a pitch-black shadow for black pixels of the light map.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme18.webp\" alt=\"\" width=\"700\"/>  \n\n## KSamplers (Raw and Refine)\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme19.webp\" alt=\"\" width=\"700\"/>  \n\nIC Raw Ksampler: Unlike any other sampler it is starting at step 8 instead of zero, due the IC-Light Condition (The frames are denoised from 8th Step)\n- For example, an End step of 20\n- Start Step at\n    - 0 will have no Light map effect.\n    - 5 will have 50 percent effect\n    - 10 will have 100 percent effect.\n    - So, about 3-8 is a good value to test from.\n\nWhen Generate New Background is TRUE, you can go Lower than 5 for Better Results\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme20.webp\" alt=\"\" width=\"700\"/>  \n\n- Ksampler Refine: It works like a Img2Img Refiner After IC raw sampler.\n\nFor an End step of 25\n\n- Start Step at  \n  - 10 and below will work like raw sampler and will give you morphing objects\n  - 15 will work like a proper refiner\n  - 20 will not work properly\n  - Above 20 and above will produce messed up results\n  - So, Default 16 is good.\n\n## Face Fix\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme21.webp\" alt=\"\" width=\"700\"/>  \n\n- Upscale For Face Fix: If your Faces are not satisfactory after face fix, you can upscale it to about 1.2 to 1.6 to have better faces.\n- Positive Prompt: Here you can write the prompts for the face. It’s set to “smiling” by default. You can change it.\n- Face Denoise: Use Around, 0.35 – 0.45. On higher face may render incorrectly and also sliding faces issue may arise.\n\n## Saving\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1101/readme22.webp\" alt=\"\" width=\"700\"/>  \n\n- Video Combine: This will export all the frames in a video format. If this node fails why combining that means there are too much frames, and it is running out of ram. Reduce the frames load cap if it happens\n  - It will save into ComfyUI > Outputs by default.\n- Change Output Path: Unmute this node, if you want to save the output to a custom save location\n\n\n## About this workflow’s Author\n\nJerry Davos\n- YouTube Channel: https://www.youtube.com/@jerrydavos\n- Patreon: https://www.patreon.com/jerrydavos\n\nContacts\n- Email: davos.jerry@gmail.com\n- Discord: https://discord.gg/z9rgJyfPWJ\n"
    },
    {
        "id": "1102",
        "readme": "The ComfyUI Vid2Vid workflow, created by [**YVANN**](https://civitai.com/models/501382/yvann-vid2vid-automated-ip2p-masking-sdxl-workflow), introduces two distinct workflows to achieve high-quality, professional animations.\n  - The first ComfyUI workflow: ComfyUI Vid2Vid Part 1 | Composition and Masking\n  - The second workflow: [ComfyUI Vid2Vid Part 2 | SDXL Style Transfer](https://www.runcomfy.com/comfyui-workflows/comfyui-vid2vid-part2-sdxl-style-transfer)\n\n## ComfyUI Vid2Vid Part 1 | Composition and Masking\n\nThis workflow enhances creativity by focusing on the composition and masking of your original video.\n\n## Step 1: Models Loader | ComfyUI Vid2Vid Workflow Part1\n\nSelect the appropriate models for your animation. This includes choosing the checkpoint model, VAE (Variational Autoencoder) model, and LoRA (Low-Rank Adaptation) model. These models are crucial for defining the capabilities and style of your animation.\n\n## Step 2: Video Loader | ComfyUI Vid2Vid Workflow Part1\n\nThe **Input Video** node is responsible for importing the video file that will be used for the animation. The node reads the video and converts it into individual frames, which are then processed in subsequent steps. This allows for detailed frame-by-frame editing and enhancement.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1102/readme01.webp\" alt=\"\" width=\"600\"/>\n\n## Step 3: Remove Background (Auto Masking) | ComfyUI Vid2Vid Workflow Part1\n\n**Remove Background (Auto Masking)** isolates the subject from the background using an automated masking technique. This involves models that detect and separate the foreground subject from the background, creating a binary mask. This step is crucial for ensuring that the subject can be manipulated independently of the background.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1102/readme02.webp\" alt=\"\" width=\"600\"/>\n\n## Step 4: Masking Specific Area (Manual Mask or Auto Mask) | ComfyUI Vid2Vid Workflow Part1\n\nThis step allows for the refinement of the mask created in the previous step. You can either manually mask specific areas using other software or rely on ComfyUI's 'Segment Anything' automated mask feature.\n\n- **Manual Mask:** This needs to be handled by other software outside of ComfyUI for precise control.\n- **Auto Mask:** Using the auto mask feature, you can describe what you want to mask using simple words in the prompt widget. The model will create a mask automatically, though it may not be as accurate as a manual mask.\n\nThe default version uses a manual mask. If you want to try the auto one, please bypass the manual mask group and enable the auto mask group. Additionally, connect the MASK of 'GroundingDinoSAM' (auto mask) to 'GrowMaskWithBlur' instead of connecting 'ImageToMask' (manual mask) to 'GrowMaskWithBlur'.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1102/readme03.webp\" alt=\"\" width=\"600\"/>\n\n## Step 5: Transform Mask | ComfyUI Vid2Vid Workflow Part1\n\n**Transform Mask** converts the mask into an image and allows additional adjustments such as adding blur to the original mask. This helps in softening the edges and making the mask blend more naturally with the rest of the image.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1102/readme04.webp\" alt=\"\" width=\"600\"/>\n\n## Step 6: Input Prompt | ComfyUI Vid2Vid Workflow Part1\n\nInput textual prompts to guide the animation process. The prompt can describe the desired style, appearance, or actions of the subject. It is crucial for defining the creative direction of the animation, ensuring that the final output matches the envisioned artistic style.\n\n## Step 7: AnimateDiff | ComfyUI Vid2Vid Workflow Part1\n\nThe **AnimateDiff** node creates smooth animations by identifying differences between consecutive frames and applying these changes incrementally. This helps in preserving motion coherence and reducing abrupt changes in the animation, leading to a more fluid and natural look.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1102/readme05.webp\" alt=\"\" width=\"600\"/>\n\n## Step 8: IPAdapter | ComfyUI Vid2Vid Workflow Part1\n\nThe **IPAdapter** node adapts the input images to align with the desired output styles or features. This includes tasks like colorization and style transfer, ensuring that each frame of the animation maintains a consistent look and feel.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1102/readme06.webp\" alt=\"\" width=\"750\"/>\n\n## Step 9: ControlNet | ComfyUI Vid2Vid Workflow Part1\n\nUsing **ControlNet - v1.1 - Instruct Pix2Pix Version model** enhances diffusion models by enabling them to process additional input conditions (e.g., edge maps, segmentation maps). It facilitates text-to-image generation by controlling these pretrained models with task-specific conditions in an end-to-end manner, allowing robust learning even with smaller datasets.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1102/readme07.webp\" alt=\"\" width=\"750\"/>\n\n## Step 10: Render | ComfyUI Vid2Vid Workflow Part1\n\nIn the **Render** step, the processed frames are compiled into a final video output. This step ensures that all individual frames are combined seamlessly into a coherent animation, ready for export and further use.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1102/readme08.webp\" alt=\"\" width=\"600\"/>\n\n## Step 11: Compose Background | ComfyUI Vid2Vid Workflow Part1\n\nThis involves compositing the animated subject with the background. You can add a static or dynamic background to the animation, ensuring that the subject integrates smoothly with the new background to create a visually appealing final product.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1102/readme09.webp\" alt=\"\" width=\"750\"/>\n\nBy utilizing the ComfyUI Vid2Vid workflow Part1, you can create intricate animations with precise control over every aspect of the process, from composition and masking to final rendering.\n"
    },
    {
        "id": "1103",
        "readme": "The ComfyUI Vid2Vid workflow, created by [**YVANN**](https://civitai.com/models/501382/yvann-vid2vid-automated-ip2p-masking-sdxl-workflow), introduces two distinct workflows to achieve high-quality, professional animations.\n\n- The first workflow: [ComfyUI Vid2Vid Part 1 | Composition and Masking](https://www.runcomfy.com/comfyui-workflows/comfyui-vid2vid-part1-composition-and-masking)\n- The second workflow: ComfyUI Vid2Vid Part 2 | SDXL Style Transfer\n\n## ComfyUI Vid2Vid Part2 | SDXL Style Transfer\n\nThis workflow transform the original video style to your desired style.\n\n## Step 1: Models Loader | ComfyUI Vid2Vid Workflow Part2\n\nThe **Efficient Loader** node is the initial step where you select the appropriate models for your animation. This includes choosing the checkpoint model, VAE (Variational Autoencoder) model, and LoRA (Low-Rank Adaptation) model. These models are crucial for defining the capabilities and style of your animation.\n\n## Step 2: Loading the Output from the First Workflow | ComfyUI Vid2Vid Workflow Part2\n\nIMPORTANT Tips:\n1) Use frame_load_cap=10 for a quick test.\n2) If the test result is satisfactory, increase frame_load_cap to a larger size.\n\n⚠️ If you want to render a long video, such as one with more than 100 frames, **it is recommended to separate the video into multiple renders, each containing NO MORE THAN 100 FRAMES.** Rendering more than this at once can cause the ComfyUI system to be interrupted. \nYou can adjust the skip_first_frames and frame_load_cap parameters to decide which frames to render.\n\nFor more detailes, please visit [📽️**⚠️ How to Render Long Videos (Over 200 Frames)**](https://comfyui-guides.runcomfy.com/ultimate-comfyui-how-tos-a-runcomfy-guide/limitation-of-rendering-long-videos)\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1103/readme01.webp\" alt=\"\" width=\"600\"/>\n\n## Step 3: ControlNet | ComfyUI Vid2Vid Workflow Part2\n \nControlNet is used to add various conditional controls to the image generation process. It consists of different models to apply specific transformations:\n\n1. **ControlNet Tile**: This model helps in maintaining consistency across tiled images, ensuring that patterns and textures remain continuous without visible seams.\n2. **ControlNet Depth**: Adds depth information to the images, creating a more realistic sense of 3D space. This model uses depth maps to guide the diffusion model, enhancing the perception of depth and volume.\n3. **ControlNet Canny**: Utilizes edge detection to provide structural guidance. By detecting and preserving the edges in the input images, it helps in maintaining the integrity of shapes and outlines during style transfer.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1103/readme02.webp\" alt=\"\" width=\"750\"/>\n\n## Step 4: AnimateDiff | ComfyUI Vid2Vid Workflow Part2\n\nThe **AnimateDiff** node introduces temporal coherence to the animation by ensuring that stylistic changes are smoothly transitioned between frames. This is crucial for avoiding flickering and ensuring a seamless visual experience.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1103/readme03.webp\" alt=\"\" width=\"600\"/>\n\n## Step 5: IPAdapter | ComfyUI Vid2Vid Workflow Part2\n\nThe **IPAdapter** node applies a strong style transfer to the original video, effectively transferring the desired artistic style onto the video frames. This robust application of style ensures that the final output closely matches the intended artistic vision while maintaining the integrity of the original content.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1103/readme04.webp\" alt=\"\" width=\"600\"/>\n\n## Step 6: First Render | ComfyUI Vid2Vid Workflow Part2\n\nThe first render is produced based on the initial application of the models and conditions. This render serves as a draft.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1103/readme05.webp\" alt=\"\" width=\"600\"/>\n\n## Step 7: Color Match and Second Render | ComfyUI Vid2Vid Workflow Part2\n\nAfter the first rendering, color matching techniques are applied to ensure consistency in color tones and palettes across frames. After color matching, a second render is produced, incorporating any necessary adjustments from the first render.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1103/readme06.webp\" alt=\"\" width=\"600\"/>\n\n## Step 8: Upscale if Needed | ComfyUI Vid2Vid Workflow Part2\n\nIf the resolution of the animation needs to be enhanced, upscaling techniques are applied. This step ensures that the final output meets the desired quality and resolution standards.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1103/readme07.webp\" alt=\"\" width=\"600\"/>\n\nBy utilizing the ComfyUI Vid2Vid workflow Part2, you can effectively transform the original video style into your desired artistic expression, maintaining consistency and quality throughout the animation process.\n"
    },
    {
        "id": "1104",
        "readme": "## What is MimicMotion\n\nMimicMotion is a controllable video generation framework developed by researchers at Tencent and Shanghai Jiao Tong University. It can generate high-quality videos of arbitrary length following any provided motion guidance. Compared to previous methods, MimicMotion excels in producing videos with rich details, good temporal smoothness, and the ability to generate long sequences.\n\n## How MimicMotion Works\n\nMimicMotion takes a reference image and pose guidance as inputs. It then generates a video that matches the reference image's appearance while following the provided motion sequence.\n\nA few key innovations enable MimicMotion's strong performance:\n\n1. Confidence-aware pose guidance: By incorporating pose confidence information, MimicMotion achieves better temporal smoothness and is more robust to noisy training data. This helps it generalize well.\n2. Regional loss amplification: Focusing the loss more heavily on high-confidence pose regions, especially the hands, significantly reduces image distortion in the generated videos.\n3. Progressive latent fusion: To generate smooth, long videos efficiently, MimicMotion generates video segments with overlapping frames and progressively fuses their latent representations. This allows generating videos of arbitrary length with controlled computational cost.\n\nThe model is first pre-trained on large video datasets, then fine-tuned for the motion mimicking task. This efficient training pipeline does not require massive amounts of specialized data.\n\n## How to Use ComfyUI MimicMotion (ComfyUI-MimicMotionWrapper)\n\nAfter testing different MimicMotion nodes available in ComfyUI, we recommend using [kijai's ComfyUI-MimicMotionWrapper](https://github.com/kijai/ComfyUI-MimicMotionWrapper/tree/main) for the best results.\n\n### Step 1: Preparing Your Input for MimicMotion\nTo start animating with ComfyUI MimicMotion, you'll need two key ingredients:\n- A reference image: This is the initial frame that serves as the starting point for your animation. Choose an image that clearly depicts the subject you want to animate.\n- Pose images: These are the images that define the motion sequence. Each pose image should show the desired position or pose of your subject at a specific point in the animation. You can create these pose images manually or use pose estimation tools to extract poses from a video.\n\n**🌟Ensure that your reference image and pose images have the same resolution and aspect ratio for optimal results.🌟**\n\n### Step 2: Loading the MimicMotion Model\nComfyUI MimicMotion requires the MimicMotion model to function properly. In RunComfy, the model is already preloaded for your convenience. To configure the \"DownLoadMimicMotionModel\" node, follow these steps:\n- Set the \"model\" parameter to \"MimicMotion-fp16.safetensors\" (or the appropriate model file name, if different).\n- Select the desired precision (fp32, fp16, or bf16) based on your GPU capabilities. This choice can impact performance and compatibility.\n- Leave the \"lcm\" parameter set to False, unless you specifically want to use the LCM (Latent Conditional Motion) variant of the model.\n\nOnce you have configured the node settings, connect the output of the \"DownloadAndLoadMimicMotionModel\" node to the input of the next node in your workflow. This will ensure that the loaded MimicMotion model is properly utilized in the subsequent steps of your ComfyUI pipeline.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1104/readme01.webp\" alt=\"DownLoadMimicMotionModel\" width=\"500\"/>\n\n### Step 3: Configuring the MimicMotion Sampler\nThe \"MimicMotionSampler\" node is responsible for generating the animated frames based on your input. Here's how to set it up:\n- Add the \"MimicMotionSampler\" node and connect it to the output of the \"DownloadAndLoadMimicMotionModel\" node.\n- Set the \"ref_image\" parameter to your reference image and the \"pose_images\" parameter to your sequence of pose images.\n- Adjust the sampling settings according to your preferences:\n   - \"steps\" determines the number of diffusion steps (higher values lead to smoother results but longer processing times).\n   - \"cfg_min\" and \"cfg_max\" control the strength of the conditional guidance (higher values adhere more closely to the pose images).\n   - \"seed\" sets the random seed for reproducibility.\n   - \"fps\" specifies the frames per second of the generated animation.\n   - Fine-tune additional parameters like \"noise_aug_strength\", \"context_size\", and \"context_overlap\" to experiment with different styles and temporal coherence.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1104/readme02.webp\" alt=\"MimicMotionSampler\" width=\"500\"/>\n\n### Step 4: Decoding the Latent Samples\nThe \"MimicMotionSampler\" node outputs latent space representations of the animated frames. To convert these latents into actual images, you need to use the \"MimicMotionDecode\" node:\n- Add the \"MimicMotionDecode\" node and connect it to the output of the \"MimicMotionSampler\" node.\n- Set the \"decode_chunk_size\" parameter to control the number of frames decoded simultaneously (**higher values may consume more GPU memory**).\nThe output of the \"MimicMotionDecode\" node will be the final animated frames in image format.\n\n### Step 5: Enhancing Poses with MimicMotionGetPoses\nIf you want to visualize the extracted poses alongside your reference image, you can use the \"MimicMotionGetPoses\" node:\n- Connect the \"ref_image\" and \"pose_images\" to the \"MimicMotionGetPoses\" node.\n- Set the \"include_body\", \"include_hand\", and \"include_face\" parameters to control which pose keypoints are displayed.\nThe output will include the reference image with the extracted pose and the individual pose images.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1104/readme03.webp\" alt=\"MimicMotionGetPoses\" width=\"500\"/>\n\n### Tips and Best Practices\nHere are some tips to help you get the most out of ComfyUI MimicMotion:\n- Experiment with different reference images and pose sequences to create a variety of animations.\n- Adjust the sampling settings to balance quality and processing time based on your needs.\n- Use high-quality, consistent pose images for the best results. Avoid drastic changes in perspective or lighting between poses.\n- Monitor your GPU memory usage, especially when working with high-resolution images or long animations.\n- Take advantage of the \"DiffusersScheduler\" node to customize the noise scheduling for unique effects.\n\nComfyUI MimicMotion is a powerful and versatile tool that enables you to effortlessly create stunning animations. By understanding the workflow and exploring the various parameters, you'll be able to animate anyone with ease. As you delve into the world of animation, remember to experiment, iterate, and have fun throughout the process. With ComfyUI MimicMotion, the possibilities are endless, so enjoy bringing your creative visions to life!\n\n\n"
    },
    {
        "id": "1105",
        "readme": "This powerful workflow has been meticulously designed to simplify the complex process of relighting products, empowering you to achieve stunning results with ease. By providing a user-friendly interface and minimizing the required inputs, this workflow allows you to focus on your creative vision while the backend takes care of the intricate tasks of relighting and color matching the subject, background, and lighting.\n\n## User Inputs | ComfyUI Product Relighting Workflow\n\nTo begin your relighting journey, the workflow is conveniently divided into three main input groups:\n\n1.&nbsp;&nbsp;**Input Images**: In this group, you have the opportunity to upload the essential images that will serve as the foundation for the relighting process.\n    - Subject Image (required): This is the star of the show – the main product image that you wish to relight. Ensure that your subject image is of high quality and clearly showcases the product you want to enhance.\n    - Background (optional): If you have a specific background in mind for your final result, you can provide an image here. This allows you to create a cohesive and visually appealing composition.\n    - Light Mask (optional): For more advanced relighting control, you can upload a light mask image that precisely defines the areas where the lighting should be applied. This is particularly useful when you want to emphasize certain aspects of your product or create specific lighting effects.\n    - Background Switch: If you have provided a background image, make sure to set this switch to \"True\" to ensure that the workflow incorporates your chosen background seamlessly.\n    - Light Mask Switch: Similarly, if you have uploaded a light mask image, set this switch to \"True\" to enable the workflow to utilize your custom lighting setup.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1105/readme01.webp\" alt=\"ComfyUI Product Relighting Workflow\" width=\"750\"/>\n\n2.&nbsp;&nbsp;**Input Prompts**: In this group, you have the power to guide the relighting process by providing concise textual descriptions.\n    - Product Description: Briefly describe the product you are relighting. For example, \"a red leather handbag\" or \"a sleek silver wristwatch.\" This description helps the workflow understand the nature of your product and optimize the relighting accordingly.\n    - Light Description: Use your creative vision to describe the desired lighting effect you want to achieve. Be as specific as possible, such as \"soft, warm golden hour light\" or \"dramatic, cool blue spotlights.\" This description will serve as a guiding light for the workflow to create the perfect ambiance for your product.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1105/readme02.webp\" alt=\"ComfyUI Product Relighting Workflow\" width=\"750\"/>\n3.&nbsp;&nbsp;**Input Variables**: In this group, you have the ability to control the extent of background changes during the relighting process.\n    - Denoise Value: This powerful variable allows you to determine how much the background should be altered. A higher value will result in more significant changes, while a lower value will preserve more of the original background.\n  \n<img src=\"https://cdn.runcomfy.net/workflow_assets/1105/readme03.webp\" alt=\"ComfyUI Product Relighting Workflow\" width=\"500\"/>\n\n**The workflow automatically adjusts the dimensions and aspect ratios of the input images to ensure compatibility.**\n\n## Results | ComfyUI Product Relighting Workflow\n\nAfter the workflow has worked its magic, you will be presented with two stunning images in the Results Group:\n\n1. **Color Matched Result**: Prepare to be amazed by the first image, where your subject, background, and lighting are expertly color matched to create a cohesive and visually striking result. This image showcases the perfect harmony between all elements, enhancing the overall aesthetics of your product.\n2. **Relit Result**: The second image takes the relighting effect to the next level, allowing the lighting to have a more pronounced influence on the final result. This image is perfect for showcasing the dramatic impact of your chosen lighting setup, making your product truly stand out.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1105/readme04.webp\" alt=\"ComfyUI Product Relighting Workflow\" width=\"750\"/>\n\nTo further refine your results, you have the power to fine-tune the **remapping** parameters and adjust the brightness of the final images. If you find that the IC-Light component has made the image too dark, simply decrease the min value to achieve deeper, richer blacks, or increase the max value to produce brighter, more vibrant whites. This level of control ensures that you can achieve the perfect balance and create images that truly reflect your artistic vision.\n\n## Workflow Components\n\nThe workflow consists of three main parts, each responsible for a specific aspect of the relighting process:\n\n1. **Yellow Part (Image Regeneration)**: This section is responsible for regenerating the image by seamlessly blending the subject, background (if provided), and lighting. It ensures that all elements of the image are harmoniously integrated before the relighting process begins, laying the foundation for a polished and professional result.\n2. **Pink Part (Relighting)**: The relighting section apples either global light illumination or your custom light mask (if available) to create the desired lighting effect. Watch as your product comes to life, bathed in the perfect light that enhances its unique features and captivates your audience.\n3. **Blue Part (Detail Recovery and Color Matching)**: The final section of the workflow is dedicated to recovering any lost details and correcting mismatching colors that may have occurred during the relighting process. Leveraging advanced techniques such as Frequency Separation and Color Matching.\n\nWith this ComfyUI Product Relighting workflow at your fingertips, you now have the power to transform your product images into stunning, professionally relit masterpieces.\n"
    },
    {
        "id": "1106",
        "readme": "## What is LivePortrait?\n\nLivePortrait is an efficient portrait animation framework developed by Kuaishou Technology that aims to synthesize lifelike videos from a single source image. It uses the source image as an appearance reference and derives motion (facial expressions and head pose) from a driving video, audio, text, or generation.\n\nUnlike mainstream diffusion-based methods, LivePortrait explores and extends an implicit-keypoint-based framework to effectively balance computational efficiency and controllability. It focuses on better generalization, controllability, and efficiency for practical usage. LivePortrait has high generation quality even compared to diffusion methods, while being extremely fast - about 12.8ms per frame on an RTX 4090 GPU with PyTorch.\n\nFor more details, visit [**LivePortrait**](https://github.com/KwaiVGI/LivePortrait)\n\n## How LivePortrait Works\n\nAt a high level, LivePortrait takes a source image and a driving video as inputs. It extracts the appearance from the source image and motion from the driving video. These are then combined using warping and generation modules to synthesize an animated portrait video that retains the identity of the source image but follows the motion and expressions of the driving video.\n\nThe key components in LivePortrait are:\n\n1. Appearance Feature Extractor: Encodes the identity and appearance information from the source image.\n2. Motion Extractor: Extracts motion features (facial keypoints) from the driving video frames.\n3. Warping Module: Uses the extracted motion to warp the source image features, aligning them to the driving pose and expression.\n4. Image Generator: Takes the warped features and synthesizes the final photorealistic animated frame.\n5. Stitching & Retargeting Module: Optionally stitches the generated portrait back onto the original image and allows controlling specific facial regions like eyes and mouth.\n\nThese modules are efficiently designed and work together to enable high-quality, controllable LivePortrait animation.\n\n## How to Use ComfyUI LivePortrait\n\nThanks to [kijai's ComfyUI-LivePortraitKJ](https://github.com/kijai/ComfyUI-LivePortraitKJ) node and workflow, creating realistic portrait animations in ComfyUI is now easier. The following is a breakdown of the key components and parameters of his ComfyUI LivePortrait workflow.\n\n### Key Steps of ComfyUI LivePortrait Img2Vid Workflow\n\n#### 1. Load the Live Portrait Models\n\n- Add the \"DownloadAndLoadLivePortraitModels\" node\n- Set precision to auto or fp16 for best performance\n\n#### 2. Choose Face Detector for LivePortrait\n\n- You have a choice between the \"LivePortraitLoadCropper\" (InsightFace) and \"LivePortraitLoadMediaPipeCropper\" nodes\n- InsightFace is more accurate but has a non-commercial license, while MediaPipe is faster on CPU but less accurate\n- Both output a \"cropper\" that will be used to detect and crop faces\n\n#### 3. Load and Preprocess Source Image for LivePortrait\n\n- Load your source portrait image using the \"Load Image\" node\n- Resize it to 512x512 using the \"ImageResize\" node\n- Connect the resized image to a \"LivePortraitCropper\" node\n- Also connect your selected face detector's \"cropper\" output to this node\n- Key parameters in \"LivePortraitCropper\" Node\n  \n\"dsize\": This sets the output resolution of the cropped face image\n    - Default is 512, meaning the face will be cropped to 512x512 pixels\n    - Higher values will crop the face in higher resolution, but may be slower to process\n    - Lower values will be faster but may lose some detail\n      \n  \"scale\": This controls how zoomed in the face crop will be\n    - Default is 2.3, higher values will zoom in closer on the face, lower values will include more of the head/background\n    - You want to adjust this so the crop includes the entire face and some background, but not too much extra space\n    - A good face crop is important for the motion transfer to work well\n    - Typical values range from 1.8 to 2.5 depending on the framing of the source image\n      \n  \"face_index\": If there are multiple faces detected in the image, this selects which one to crop\n    - Default is 0, which selects the first detected face\n    - Increase this if you want to select a different face in the image\n    - Detected faces are ordered based on the \"face_index_order\" setting (default is largest to smallest)\n      \n  \"vx_ratio\" and \"vy_ratio\" (Optional): These allow you to offset the crop vertically (vy) or horizontally (vx)\n    - Values range from -1 to 1\n    - For example, setting vy to 0.1 will shift the crop upwards by 10% of the frame size\n    - This can help if the automatic crop is slightly misaligned\n      \n  \"face_index_order\": This sets how detected faces are ordered when selecting with face_index\n    - Default is \"large-small\" which orders from largest to smallest face\n    - Can also order from left to right, top to bottom, etc.\n    - This is only relevant if you have multiple faces in your image\n\n#### 4. Load and Preprocess Driving Video for LivePortrait\n\n- Load your driving video using the \"VHS_LoadVideo\" node\n- Adjust num_frames using the \"frame_load_cap\" primitive\n- Resize the video frames to 480x480 using a \"GetImageSizeAndCount\" node\n- You can optionally crop the driving video frames using another \"LivePortraitCropper\" node\n\n#### 5. Apply Motion Transfer for LivePortrait\n\n- Add the \"LivePortraitProcess\" node\n- Connect the loaded pipeline, source image crop_info, cropped source image, and driving frames to \"LivePortraitProcess\" node\n- **Key parameters in \"LivePortraitProcess\" Node**\n    \n\"lip_zero\": When enabled, this will zero out the lip parameters if they fall below a certain threshold    \n    - This can help reduce unnatural lip movements and improve lip sync\n    - Recommended to enable this unless you specifically want to preserve all lip motion\n    \n  \"lip_zero_threshold\": This sets the threshold below which lip parameters will be zeroed out when \"lip_zero\" is enabled\n    - Default is 0.03, higher values will zero out more lip motion, lower values will preserve more\n    - Adjust this if you want to change how much lip motion is suppressed\n    \n  \"stitching\": When enabled, this will blend the animated face back into the original image using a stitching process\n    - This can help create a more seamless transition between the animated face and the background\n    - Recommended to enable this for the most natural-looking results\n    \n  \"delta_multiplier\": This scales the motion parameters by a multiplier    \n    - Default is 1.0, higher values will exaggerate motion, lower values will reduce it\n    - Can be used to adjust the overall intensity of the facial motion\n    - Typical values range from 0.8 to 1.5 depending on the desired effect\n    \n  \"mismatch_method\": This sets how the workflow handles mismatches between the number of source and driving frames    \n    - Options are \"constant\", \"cycle\", \"mirror\", and \"cut\"\n    - \"constant\" will hold on the last frame, \"cycle\" will loop, \"mirror\" will play forwards then backwards, \"cut\" will stop\n    - Default is \"constant\", change this if you want a different behavior when the driving video is longer or shorter than the source\n    \n  \"relative_motion_mode\": This controls how motion is transferred from the driving video to the source image \n    - Options are \"relative\", \"source_video_smoothed\", \"relative_rotation_only\", \"single_frame\", and \"off\"\n    - \"relative\" is the default and uses relative motion transfer\n    - \"off\" will disable motion transfer entirely\n    - Experiment with different modes to see which gives the best results for your specific use case\n    \n  \"driving_smooth_observation_variance\": This controls the smoothness of the driving motion when using the \"source_video_smoothed\" motion mode    \n    - Higher values will smooth out the motion more, lower values will preserve more of the original motion\n    - Default is 0.000003, adjust this if you want to change the smoothness of the transferred motion\n\n#### 6. Composite Result (Optional) for LivePortrait\n\n- To composite the animated face back into the source image, use the \"LivePortraitComposite\" node\n- Connect the original source image, cropped animated image, LivePortrait output data, and an optional mask\n- This will output full frames with the animated face blended in\n\n#### 7. Configure Retargeting (Optional) for LivePortrait\n\n- For finer control over eyes and lips, use the \"LivePortraitRetargeting\" node\n- Enable eye and/or lip retargeting and adjust their multipliers\n- Connect the retargeting info to \"LivePortraitProcess\"\n\nPlease note that the Insightface model is required in this workflow. Insightface model (https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip) license is non-commercial in nature.\n\nIf you are interested in LivePortrait Vid2Vid, please use [LivePortrait | Animate Portraits | Vid2Vid](https://runcomfy.com/comfyui-workflows/comfyui-liveportrait-workflow-animate-portraits-vid2vid)\n"
    },
    {
        "id": "1107",
        "readme": "## What is Face-to-Many?\nFace-to-many is a powerful workflow in ComfyUI that allows you to transform a single facial image into multiple artistic styles effortlessly. By utilizing the InstantID technology within the ControNet model, you can generate personalized images in various styles such as 3D, emoji, pixel art, video game, clay, or toy aesthetics, all while preserving the key facial features and identity of the original image.\n\n## How Does Face-to-Many Work?\nThe Face-to-many workflow in ComfyUI leverages several key components to achieve its impressive results:\n### 1. InstantID\nAt the core of the Face-to-many workflow is the InstantID model, which specializes in identity-preserving personalized image synthesis. It maps and preserves key facial points and attributes across different artistic transformations, ensuring high fidelity to the original face.\n### 2. ControlNet\nThe ControlNet model is used to guide the image generation process by providing additional control and consistency. It helps maintain the structure and key features of the original facial image throughout the transformations into different artistic styles.\n### 3. Lora models\nDifferent Lora models are used to define the specific artistic styles that can be applied to the facial image. By selecting the appropriate Lora model, you can transform the face into 3D, emoji, pixel art, video game, clay, or toy styles.\n\n## Step-by-Step Tutorial: Using Face-to-Many in ComfyUI\nTo generate personalized images in various styles using the Face-to-Many workflow, follow these steps:\n\n### 1. Upload the facial image:\nUse the \"LoadImage\" node to upload the single facial image you want to transform. Make sure the image is of sufficient quality and resolution for optimal results.\n\n### 2. Select the desired styles for Face-to-Many\nIn the \"LoRA Stacker\" node, choose the Lora models corresponding to the styles you want to apply. The mapping of styles to Lora models is as follows:\n- 3D: sdxl/3DRedmond-3DRenderStyle-3DRenderAF.safetensors\n- Emoji: sdxl/fofr/emoji.safetensors\n- Video game: sdxl/PS1Redmond-PS1Game-Playstation1Graphics.safetensors\n- Pixels: sdxl/PixelArtRedmond-Lite64.safetensors\n- Clay: sdxl/ClayAnimationRedm.safetensors\n- Toy: sdxl/ToyRedmond-FnkRedmAF.safetensors\n  \n### 3. Adjust settings and prompts for better Face-to-Many results\nProvide style-specific prompts in the \"EfficientLoader\" node to guide the image generation process.\n- 3D: 3D Style\n- Emoji: Emiji Style\n- Video game: PS1 Style\n- Pixels: Pixels Arty\n- Clay: Clay Animation\n- Toy: Toy Style\n\n### 4. Generate the Face-to-Many images:\nOnce all the settings are configured, run the Face-to-Many workflow to generate the personalized images in the selected styles.\nFeel free to experiment with different combinations of styles, prompts, and settings to achieve desired results.\n\nBy leveraging the power of the Face-to-Many workflow in ComfyUI, you can easily transform a single facial image into a variety of captivating artistic styles. Whether you want to create a 3D render, an emoji version, or a video game character, Face-to-Many provides a seamless and efficient way to generate personalized images while preserving the identity of the original face.\n\n\nFor more information and to view the original work, please visit the GitHub page of the author fofr at [fofr](https://github.com/fofr/cog-face-to-many). Many of the loras are crafted by artificialguybr. You can support artificialguybr's work through [Patreon](https://www.patreon.com/user?u=81570187) or [Ko-fi](https://ko-fi.com/artificialguybr) or follow artificialguybr on Twitter: [artificialguybr](https://twitter.com/artificialguybr).\n\n"
    },
    {
        "id": "1108",
        "readme": "Thanks to [kijai's ComfyUI-LivePortraitKJ](https://github.com/kijai/ComfyUI-LivePortraitKJ) node and workflow, creating realistic LivePortrait animations in ComfyUI is now easier. The following is a breakdown of the key components and parameters of his workflow.\n\nPlease read the [LivePortrait Img2Vid](https://www.runcomfy.com/comfyui-workflows/comfyui-liveportrait-workflow-animate-portraits) description first to understand the workflow steps. After familiarizing yourself with the LivePortrait Img2Vid process, you will notice some small changes between the LivePortrait Vid2Vid and Img2Vid workflows.\n\n## The Difference Between ComfyUI LivePortrait Vid2Vid and Img2Vid\n\n### 1. Load videos using \"VHS_LoadVideo\" instead of images\n\n- In this LivePortrait Img2Vid workflow, you load a static image as the source using the \"LoadImage\" node. However, in the Vid2Vid workflow, you need to load a video as the source instead. Adjust the \"frame_load_cap\" to control how many frames are loaded.\n- Resize source video to a higher resolution like 1024x1024 for better quality. After loading the source video with \"VHS_LoadVideo\", use the \"ImageResizeKJ\" node to upscale the frames to a resolution like 1024x1024. This will help maintain sharpness and detail in the final output. When working with videos, it's recommended to use a higher resolution for the source compared to the Img2Vid workflow. While 512x512 is often sufficient for static images, videos benefit from higher resolutions to preserve detail and quality.\n- The driving video frames can still be resized to a lower resolution like 480x480 to save processing time, as they only provide motion information.\n\n### 2. Use \"source_video_smoothed\" relative motion mode for smoother LivePortrait Vid2Vid results\n\n- The \"LivePortraitProcess\" node has a \"relative_motion_mode\" parameter that controls how motion is transferred from the driving video to the source. For Vid2Vid, it's recommended to use the \"source_video_smoothed\" mode.\n- In this mode, the LivePortrait motion is smoothed over time based on the input video, which helps create more temporally coherent and stable results. This is especially important for videos, where sudden jumps or jitter in motion can be more noticeable than in single images.\n- Other motion modes like \"relative\" or \"single_frame\" may work better for Img2Vid, but \"source_video_smoothed\" is typically the best choice for Vid2Vid.\n\n### 3. Connect source video FPS and audio to \"VHS_VideoCombine\" to maintain audio sync for LivePortrait Vid2Vid\n\n- When creating the final output video with the \"VHS_VideoCombine\" node, it's important to maintain audio synchronization with the video frames. This involves two key connections:\n- First, connect the source video's audio to the \"audio\" input of \"VHS_VideoCombine\" using a \"Reroute\" node. This will ensure the original audio is used in the output video.\n- Second, connect the source video's frame rate (FPS) to the \"frame_rate\" input of \"VHS_VideoCombine\". You can get the FPS using the \"VHS_VideoInfo\" node, which extracts metadata from the source video. This will ensure the output video matches the timing of the source.\n- By carefully handling the audio and frame rate, you can create a LivePortrait Vid2Vid output that maintains proper synchronization and timing, which is crucial for a realistic and watchable result.\n"
    },
    {
        "id": "1109",
        "readme": "## What is PhotoMakerV2\nPhotoMakerV2, an upgrade from PhotoMaker, offers an efficient method for personalized text-to-image generation. It synthesizes realistic photos of individuals using a few input identity images and a text prompt.\n\n### Some key features of PhotoMakerV2 include:\n- High efficiency: Quickly generates personalized photos.\n- Excellent identity preservation: Maintains the likeness of input identities.\n- Flexible text control: Allows specifying context, style, attributes, etc., in the prompt.\n- Improved identity fidelity: Enhanced compared to PhotoMaker V1.\nPhotoMakerV2 generates photorealistic images of a person in various contexts, stylizes appearances, changes attributes like age and gender, merges identities, and modernizes people from old photos or artwork. It unlocks numerous creative possibilities.\n\n## How PhotoMakerV2 Works\nPhotoMakerV2 encodes one or more input identity images into a \"stacked ID embedding,\" serving as a unified representation encapsulating identity information.\n\nThis embedding, combined with a text prompt, feeds into a text-to-image diffusion model. The model then produces an image depicting the embedded identity in the context described by the prompt.\n\nSome key aspects of how it works under the hood:\n- Uses an identity encoder to extract identity information from input face images\n- Improves identity preservation by leveraging an external face recognition model (InsightFace)\n- Encodes multiple identity images into a stacked embedding to capture identity comprehensively\n- Feeds the stacked ID embedding into the diffusion model's cross-attention layers\n- Guides generation with the text prompt while adaptively merging the identity information\n- Trained with an identity-oriented dataset to improve identification capabilities\n\n\n## How to Use ComfyUI PhotoMakerV2\nTo use PhotoMakerV2 in ComfyUI, primarily interact with the PhotoMakerEncodePlus node. A typical workflow involves:\n1. Load PhotoMakerV2 model using \"PhotoMaker Loader Plus\" node.\n2. Load one or more identity images using \"Prepare Images For CLIP Vision\" node.\n3. Load InsightFace model required by PhotoMakerV2 using \"PhotoMaker InsightFace Loader\" node.\n4. Connect outputs of these nodes to corresponding inputs of \"PhotoMaker Encode Plus\" node.\n5. In the \"PhotoMaker Encode Plus\" node, specify the prompt describing the desired image. Use the special trigger word in the prompt where the identity should appear.\n6. Connect output conditioning from \"PhotoMaker Encode Plus\" to a \"KSampler\" node to generate the image.\n\n\nFor more information, please visit [PhotoMaker Hugging Face](https://huggingface.co/papers/2312.04461) and [ComfyUI-PhotoMaker-Plus](https://github.com/shiimizu/ComfyUI-PhotoMaker-Plus/tree/main). All credit goes to their contributions.\n"
    },
    {
        "id": "1110",
        "readme": "## Overview of the Product Relighting Video Workflow\n\nThis workflow allows you to input a video and one or more light masks to generate a relighting video. It supports three scenarios:\n\n1. Moving subject with a moving light mask\n2. Still subject with a moving light mask\n3. Moving subject with a still light mask\n\nWhile this relighting video workflow is experimental and may not produce perfect results, it represents a significant step towards non-AnimateDiff pipelines that prioritize product fidelity over perfect frame merging in the video relighting process.\n\n## Inputs and Variables for Relighting Video\n\nTo run the workflow and achieve the desired video relighting effects, you must provide the following inputs and variables:\n\n- Source Video: The input video to be relit\n- Number of frames: The total number of frames to process\n- Number of initial frames to skip: The number of frames to skip from the beginning of the video\n- Still or Moving Subject: A boolean switch to indicate whether the subject is still or moving\n- Size of the longer side (resize): The size to which the frames will be resized based on the longer side\n- Two Light Masks: Either the initial and final light mask positions (for a moving light mask) or the same light mask (for a still light mask)\n- Light prompt: A text prompt describing the desired lighting\n- CFG: A value between 1.05 and 3 to control the strength of the lighting changes (higher values lead to more noticeable changes but a higher chance of artifacts)\n- Denoise: A value between 0.3 and 0.6 to control the denoising strength (higher values result in less flickering but may make the light mask look more like a filter)\n\n## How the Video Relighting Workflow Works\n\n1. The workflow uses IC-Light to relight each frame independently.\n2. For each frame, a latent is generated based on a blend of the subject frame and the corresponding light mask frame.\n3. The latent is generated at a low denoise value to force the light mask onto the final output.\n4. Since IC-Light may lose details at low denoising, the details and original colors are transferred back using Frequency Separation at the end of the workflow to maintain video quality after relighting.\n"
    },
    {
        "id": "1111",
        "readme": "## 1. What is FLUX\n\nFLUX is a new image generation model developed by [Black Forest Labs](https://blackforestlabs.ai/), the masterminds behind Stable Diffusion. FLUX marks a significant advancement in the realm of AI-generated art. This state-of-the-art model comes in three distinct variants:\n\n- **FLUX.1 [pro]**: The pinnacle of the FLUX.1 lineup, offering top-tier performance in image generation.\n- **FLUX.1 [dev]**: An open-weight, guidance-distilled model designed for non-commercial use. It delivers quality and prompt adherence similar to FLUX.1 [pro], but with greater efficiency.\n- **FLUX.1 [schnell]**: The fastest model variant, optimized for local development and personal use, and available under an Apache 2.0 license.\n\nFLUX.1 models excel in prompt adherence, visual quality, image detail, and output diversity. They handle text with exceptional precision, follow complex scene composition instructions faithfully, and generate hands more accurately than previous models.\n\n## 2. What Makes FLUX.1 Special?\n\nFLUX.1 redefines the possibilities of AI-generated art. Here are the standout features:\n\n1. **Text Accuracy**: FLUX handles tricky words and repeated letters effortlessly, unlike older models. This makes it ideal for designs where text precision is paramount.\n2. **Complex Composition**: FLUX excels at interpreting and executing detailed instructions for object placement and scene composition, generating accurate scenes from elaborate prompts.\n3. **Realistic Hands**: Historically, generating realistic hands has been a challenge for AI art models. FLUX makes significant progress in this area, producing hands with the correct number of fingers in accurate positions. While not flawless, it is a notable improvement over previous models.\n4. **Efficiency**: The dev and schnell variants of FLUX provide similar quality to the pro version while being more efficient in size and speed, facilitating faster iteration and experimentation.\n5. **Diversity**: FLUX can generate a broad range of artistic styles, from photorealistic to painterly to illustrative, opening new avenues for artistic expression.\n\n## 3. How to Use ComfyUI FLUX\n\nComfyUI FLUX is released by [comfyanonymous](https://comfyanonymous.github.io/ComfyUI_examples/flux/). Thanks for their contributions to the community.\n\nFLUX models can be seamlessly integrated within the ComfyUI framework for intuitive AI-generated art creation. All necessary model files for FLUX.1 [dev] and FLUX.1 [schnell] are preloaded at RunComfy platform. Simply follow these key steps to run the workflow:\n\nStep 1. **Configure UNETLoader Node**: Choose `flux1-dev.sft` or `flux1-schnell`.\n\nStep 2. **Configure DualCLIPLoader Node**: For lower memory usage, use `sd3m/t5xxl_fp8_e4m3fn.safetensors`; For higher memory, use `sd3m/t5xxl_fp16.safetensors`.\n   \n🌟IMPORTANT🌟: Choose the appropriate model and clip based on the machine size.\n\nWhen launching a RunComfy **medium-sized** machine, refer to the following image for settings: checkpoint (flux_schnell,fp8) and clip (t5_xxl_fp8), or it may cause an out-of-memory error.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1111/readme02.webp\" alt=\"FLUX.1 [schnell]\" width=\"500\"/>\n\nWhen launching a RunComfy **large-sized or above** machine, you can choose a large machine and clip, refer to this image for settings: checkpoint (flux_dev,default) and clip (t5_xxl_fp16) \n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1111/readme01.webp\" alt=\"FLUX.1 [dev]\" width=\"500\"/>\n\nStep 3. **Configure VAELoader Node**: The required model for FLUX is preloaded.\n\nStep 4. **Set Up CLIPTextEncode Node**: Enter your desired text prompt in the node's properties to guide the image generation process.\n\nStep 5. **Adjust EmptyLatentImage Node (Optional)**: This node creates an empty latent image that serves as the starting point for the generation process.\n\nStep 6. **Configure SamplerCustomAdvanced Node**: This node processes noise, guider, sampler, sigmas, and latent_image inputs to produce the denoised output.\n\nStep 7. **Adjust KSamplerSelect Node (Optional)**: Select the specific sampler algorithm (e.g., \"euler\") by adjusting this node's properties.\n\nStep 8. **Generate Image**: Proceed to generate your desired image.\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n\n\n\n\n"
    },
    {
        "id": "1112",
        "readme": "This ComfyUI Img2Vid workflow is created by [Titto13](https://civitai.com/models/590964/wbmix-img2vid-animatediff-lcm) based on the exceptional work of [ipiv](https://civitai.com/models/372584/ipivs-morph-img2vid-animatediff-lcm-hyper-sd). This workflow focusing on dynamic image generation and adjustment, consists of **AnimateDiff LCM**, **IPAdapter**, **QRCode ControlNet**, and **Custom Mask** modules. Each of these modules plays a crucial role in the Img2Vid process, enhancing the quality of morphing animation.\n\n## Core Components of the Img2Vid workflow for morphing animation\n\n**1. AnimateDiff LCM Module**:\n\nIntegrates the AnimateLCM model into the AnimateDiff setup to accelerate the rendering process. AnimateLCM speeds up video generation by reducing the number of inference steps required and improves result quality through decoupled consistency learning. This allows the use of models that typically do not produce high-quality results, making AnimateLCM an effective tool for creating detailed animations.\n\n**2. IPAdapter Module**:\n\nUtilizes the attention mask function of IPAdapter to achieve morphing between reference images. Users can generate dedicated attention masks for each image, ensuring smooth transitions in the final video.\n\n**3. QRCode ControlNet Module**:\n\nUses a black-and-white video as the input for the ControlNet QRCode model, guiding the animation flow and enhancing the visual dynamics of the morphing sequence.\n\n**4. Mask Module**:\n\nProvides three preset masks and allows users to load custom masks. All these masks can be switched with a simple one-click operation to achieve various effects.\n\n## How to use the Img2Vid workflow to create morphing animations\n\n**1. Image Loading and Mask Application**\n- Image Loading: Load images into the \"Load White\" and \"Load Black\" nodes. The workflow includes various image masks that users can select based on their needs.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1112/readme01.webp\" alt=\"\" width=\"750\"/>\n- Mask Processing: Masks can be selected by clicking \"Action!\" and custom masks can be uploaded and applied, enhancing flexibility.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1112/readme02.webp\" alt=\"\" width=\"750\"/>\n\n**2. Image Adjustment**\n- Image Rotation、Cropping and Flipping: Adjust images using the \"Rotate Mask\" and \"Flip Image\" functions to achieve the desired effects.The \"Fast Crop\" function allows users to choose between center cropping or adding black borders to make images fit. The \"Detail Crop\" function enables cropping of specific details from images.(This feature gives you more control over your creations, so enable it if you need to!)\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1112/readme03.webp\" alt=\"\" width=\"500\"/>\n\n**3. Parameter Adjustment**\n- AnimateDiff - Motion Scale: Adjusting this parameter changes the animation's fluidity. Increasing the value adds more movement but may reduce detail quality. The recommended range is 1.000-1.300, with experimentation encouraged.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1112/readme04.webp\" alt=\"\" width=\"750\"/>\n- QRCode ControlNet - Strength and End Percent: These parameters control the animation's intensity and transition effect. Generally, adjust \"Strength\" between 0.4 and 1.0 and \"End Percent\" between 0.350 and 0.800.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1112/readme05.webp\" alt=\"\" width=\"750\"/>\n- Mask - Force Rate: Set to \"0\" for initial speed or \"12\" for accelerated and doubled cycles. Adjust this value based on animation length and effect needs.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1112/readme06.webp\" alt=\"\" width=\"750\"/>\n- IPAdapter - Preset: It is recommended to use the \"VIT-G\" preset for more stable results. For results closer to the original images, switch to the “PLUS” preset and set “weight_type” to “ease in-out.”\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1112/readme07.webp\" alt=\"\" width=\"750\"/>\n\n\nFor more information and to view the original work, please visit the Civitai page of the author [Titto13](https://civitai.com/models/590964/wbmix-img2vid-animatediff-lcm).\n"
    },
    {
        "id": "1113",
        "readme": "Segment Anything V2, also known as SAM2, is a groundbreaking AI model developed by Meta AI that revolutionizes object segmentation in both images and videos.\n\n## What is Segment Anything V2 (SAM2)?\nSegment Anything V2 is a state-of-the-art AI model that enables the seamless segmentation of objects across images and videos. It is the first unified model capable of handling both image and video segmentation tasks with exceptional accuracy and efficiency. Segment Anything V2 (SAM2) builds upon the success of its predecessor, the Segment Anything Model (SAM), by extending its promptable capabilities to the video domain.\n\nWith Segment Anything V2 (SAM2), users can select an object in an image or video frame using various input methods, such as a click, bounding box, or mask. The model then intelligently segments the selected object, allowing for precise extraction and manipulation of specific elements within the visual content.\n\n## Highlights of Segment Anything V2 (SAM2)\n1. State-of-the-Art Performance: SAM2 outperforms existing models in the field of object segmentation for both images and videos. It sets a new benchmark for accuracy and precision, surpassing the performance of its predecessor, SAM, in image segmentation tasks.\n2. Unified Model for Images and Videos: SAM2 is the first model to provide a unified solution for segmenting objects across both images and videos. This integration simplifies the workflow for AI artists, as they can use a single model for various segmentation tasks.\n3. Enhanced Video Segmentation Capabilities: SAM2 excels in video object segmentation, particularly in tracking object parts. It outperforms existing video segmentation models, offering improved accuracy and consistency in segmenting objects across frames.\n4. Highlights of Segment A. Reduced Interaction Time: Compared to existing interactive video segmentation methods, SAM2 requires less interaction time from users. This efficiency allows AI artists to focus more on their creative vision and spend less time on manual segmentation tasks.\n5. Simple Design and Fast Inference: Despite its advanced capabilities, SAM2 maintains a simple architectural design and offers fast inference speeds. This ensures that AI artists can integrate SAM2 into their workflows seamlessly without compromising on performance or efficiency.\n   \n## How Segment Anything V2 (SAM2) Works\nSAM2 extends SAM's promptable capability to videos by introducing a per-session memory module that captures target object information, enabling object tracking across frames, even with temporary disappearances. The streaming architecture processes video frames one at a time, behaving like SAM for images when the memory module is empty. This allows for real-time video processing and natural generalization of SAM's capabilities. SAM2 also supports interactive mask prediction corrections based on user prompts. The model utilizes a transformer architecture with streaming memory and is trained on the SA-V dataset, the largest video segmentation dataset collected using a model-in-the-loop data engine that improves both the model and data through user interaction.\n\n## How to use Segment Anything V2 (SAM2) in ComfyUI\n\nThis ComfyUI workflow supports selecting an object in a video frame using a click/point.\n\n### 1. Load Video(Upload)\n**Video Loading**: Select and upload the video you wish to process.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1113/readme01.webp\" alt=\"ComfyUI Segment Anything V2 (SAM2)\" width=\"450\"/>\n \n### 2. Points Editor\n**key point**: Place three key points on the canvas—`positive0`, `positive1`, and `negative0`:\n\n`positive0` and `positive1` mark the regions or objects you want to segment.\n\n`negative0` helps exclude unwanted areas or distractions.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1113/readme02.webp\" alt=\"ComfyUI Segment Anything V2 (SAM2)\" width=\"450\"/>\n\n**points_store**: Allows you to add or remove points as needed to refine the segmentation process.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1113/readme03.webp\" alt=\"ComfyUI Segment Anything V2 (SAM2)\" width=\"700\"/>\n     \n### 3. Model Selection of SAM2\n**Model Options**: Choose from available SAM2 models: `tiny`, `small`, `large`, or `base_plus`. Larger models provide better results but require more loading time.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1113/readme04.webp\" alt=\"ComfyUI Segment Anything V2 (SAM2)\" width=\"450\"/>\n\nFor more information, please visit [Kijai ComfyUI-segment-anything-2](https://github.com/kijai/ComfyUI-segment-anything-2).\n"
    },
    {
        "id": "1114",
        "readme": "This creative software soap workflow is created by Pranjal Choudhary. It enables rapid material application to models, achieving high-quality product rendering. This workflow integrates essential components such as IPAdapter and ControlNet, each playing a crucial role in delivering superior image output. By leveraging these modules, users can efficiently apply various textures and styles, enhancing the visual quality and realism of the rendered products.\n\n## Core Components and Their Functions\n\n**1. Image In**:  This component is responsible for uploading the original image and mask images. These images serve as the visual content for further processing.\n \n**2. IPAdapter**:  IPAdapter utilizes its attention mechanism to learn the material style and content of reference images. This component is crucial for seamlessly integrating style and contextual details with the original image, ensuring a high-quality image fusion process.\n\n**3. ControlNet**:  ControlNet allows precise adjustment of image depth and line framework. It provides control over how the model interprets and processes visual content, improving the accuracy and detail of the output.\n\n**4. Upscale**:  The Upscale component displays the final results of the fusion process. It allows users to preview and assess the integrated output, showcasing the enhancement in image and style.\n\n## Workflow Operation\n\n**1. Image Upload and Processing**\n\n- **Image Upload (Image In)**: Start by uploading the original image and mask images through the \"Image Upload\" component. These images are the basis for visual content and subsequent processing.\n- **Reference Image and Logo Upload (IPAdapter)**: Upload reference images and logo images in IPAdapter. These reference images guide the style and content integration process.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1114/readme01.webp\" alt=\"\" width=\"750\"/>\n\n**2. IPAdapter Processing**\n\n- **IPAdapter**: Utilize two IPAdapters, allowing you to adjust each adapter's weight, as well as the start and end steps, to precisely control the final result. This flexibility enables fine-tuning of the output to meet specific rendering requirements.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1114/readme02.webp\" alt=\"\" width=\"750\"/>\n\n**3. Control and Prompt**\n\n- **ControlNet**: Use ControlNet to adjust the image depth and line framework, influencing how the model processes visual content for more precise results.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1114/readme03.webp\" alt=\"\" width=\"750\"/>\n\n- **CLIP Text Encoding (CLIP Text Encode (Prompt))**: Enter prompts in the CLIP Text Encoder to optimize image generation effects, making the fused image closer to your envisioned concept.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1114/readme04.webp\" alt=\"\" width=\"400\"/>\n\n**4. Final Output**\n\n- **Upscale**: View and assess the final image output in the Upscale component. This step provides a preview of the integrated results, allowing for necessary adjustments to ensure enhanced output quality and detail.\n\n\n## More Information\n\nFor a detailed tutorial on the creative software soap workflow, please visit the [Pranjal Choudhary's tutorial video](https://www.youtube.com/live/8FS8kKZCNAM).\n\nYou can also explore more of Pranjal Choudhary's work on his [YouTube channel](https://www.youtube.com/@jalpranjal).\n"
    },
    {
        "id": "1115",
        "readme": "FLUX is a new image generation model developed by [Black Forest Labs](https://blackforestlabs.ai/), The FLUX-ControlNet-Depth and FLUX-ControlNet-Canny models were created by the XLabs AI team. This ComfyUI FLUX ControlNet workflow was also created by the XLabs AI team. For more details, please visit [**x-flux-comfyui**](https://github.com/XLabs-AI/x-flux-comfyui). All credit goes to their contribution.\n\n## About FLUX\n\nThe FLUX models are preloaded on RunComfy, named `flux/flux-schnell` and `flux/flux-dev`.\n\n- **When launch a RunComfy Medium-Sized Machine**: Select the checkpoint `flux-schnell, fp8` and clip `t5_xxl_fp8` to avoid out-of-memory issues.\n- **When launch a RunComfy Large-Sized or Above Machine**: Opt for a large checkpoint `flux-dev, default` and a high clip `t5_xxl_fp16`.\n\nFor more details, visit: [ComfyUI FLUX | A New Art Image Generation](https://www.runcomfy.com/comfyui-workflows/comfyui-flux-a-new-art-image-generation)\n\n🌟The following FLUX-ControlNet Workflow is specifically designed for the [`FLUX.1 [dev]`](https://github.com/black-forest-labs/flux) model.🌟\n\n## About FLUX-ControlNet Workflow (FLUX-ControlNet-Depth-V3 and FLUX-ControlNet-Canny-V3)\n\nWe present two exceptional FLUX-ControlNet Workflows: FLUX-ControlNet-Depth and FLUX-ControlNet-Canny, each offering unique capabilities to enhance your creative process.\n\n### 1. How to Use ComfyUI FLUX-ControlNet-Depth-V3 Workflow\n\nThe FLUX-ControlNet Depth model is first loaded using the \"LoadFluxControlNet\" node. Select the \"flux-depth-controlnet.safetensors\" model for optimal depth control.\n\n- flux-depth-controlnet\n- flux-depth-controlnet-v2\n- flux-depth-controlnet-v3: ControlNet is trained on 1024x1024 resolution and works for 1024x1024 resolution, with better and realistic version\n\nConnect the output of this node to the \"ApplyFluxControlNet\" node. Also, connect your depth map image to the image input of this node. The depth map should be a grayscale image where closer objects are brighter and distant objects are darker, allowing FLUX-ControlNet to interpret depth information accurately.\n\nYou can generate the depth map from an input image using a depth estimation model. Here, the \"MiDaS-DepthMapPreprocessor\" node is used to convert the loaded image into a depth map suitable for FLUX-ControlNet. Key params:\n\n- Threshold = 6.28 (affects sensitivity to edges)\n- Depth scale = 0.1 (amount depth map values are scaled by)\n- Output Size = 768 (resolution of depth map)\n\nIn the \"ApplyFluxControlNet\" node, the Strength parameter determines how much the generated image is influenced by the FLUX-ControlNet depth conditioning. Higher strength will make the output adhere more closely to the depth structure.\n\n### 2. How to Use ComfyUI FLUX-ControlNet-Canny-V3 Workflow\n\nThe process is very similar to the FLUX-ControlNet-Depth workflow. First, the FLUX-ControlNet Canny model is loaded using \"LoadFluxControlNet\". Then, it is connected to the \"ApplyFluxControlNet\" node.\n\n- flux-canny-controlnet\n- flux-canny-controlnet-v2\n- flux-canny-controlnet-v3: ControlNet is trained on 1024x1024 resolution and works for 1024x1024 resolution, with better and realistic version\n\nThe input image is converted to a Canny edge map using the \"CannyEdgePreprocessor\" node, optimizing it for FLUX-ControlNet. Key params:\n\n- Low Threshold = 100 (edge intensity threshold)\n- High Threshold = 200 (hysteresis threshold for edges)\n- Size = 832 (edge map resolution)\n\nThe resulting Canny edge map is connected to the \"ApplyFluxControlNet\" node. Again, use the Strength parameter to control how much the edge map influences the FLUX-ControlNet generation.\n\n### 3. **Both for ComfyUI FLUX-ControlNet-Depth-V3 and ComfyUI FLUX-ControlNet-Canny-V3**\n\nIn both FLUX-ControlNet workflows, the CLIP encoded text prompt is connected to drive the image contents, while the FLUX-ControlNet conditioning controls the structure and geometry based on the depth or edge map.\n\nBy combining different FLUX-ControlNets, input modalities like depth and edges, and tuning their strength, you can achieve fine-grained control over both the semantic content and structure of the images generated by FLUX-ControlNet.\n\nLicense: controlnet.safetensors falls under the [**FLUX.1 [dev]**](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) Non-Commercial License\n\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1116",
        "readme": "FLUX is a new image generation model developed by [Black Forest Labs](https://blackforestlabs.ai/), The [flux-RealismLora](https://huggingface.co/XLabs-AI/flux-RealismLora) created by [XLabs AI](https://github.com/XLabs-AI) team, enhances its realism capabilities. This ComfyUI FLUX-RealismLoRA workflow was created by [Leo Kadieff](https://www.linkedin.com/posts/leokadieff_ai-generativeai-filmmaking-activity-7227322182920536066-K980?utm_source=share&utm_medium=member_desktop). All credit goes to their contribution.\n\n## What is the ComfyUI FLUX-RealismLoRA workflow?\n\nThe ComfyUI FLUX-RealismLoRA workflow combines the FLUX-1 text-to-image model with a FLUX-RealismLoRA model. The FLUX-RealismLoRA model has been trained on a dataset of realistic photos and prompts, enhancing FLUX-1's capability for photorealism.\n\nBy using this FLUX-RealismLoRA workflow, you can generate incredibly photorealistic images from text prompts alone, without the need to include numerous realism-related tokens in every prompt, such as \"a RAW ultra-realistic photo, UHD, 8k\".\n\n## About FLUX\n\nThe FLUX models are preloaded on RunComfy, named `flux/flux-schnell` and `flux/flux-dev`.\n\n- **When launch a RunComfy Medium-Sized Machine**: Select the checkpoint `flux-schnell, fp8` and clip `t5_xxl_fp8` to avoid out-of-memory issues.\n- **When launch a RunComfy Large-Sized or Above Machine**: Opt for a large checkpoint `flux-dev, default` and a high clip `t5_xxl_fp16`.\n\nFor more details, visit: [ComfyUI FLUX | A New Art Image Generation](https://www.runcomfy.com/comfyui-workflows/comfyui-flux-a-new-art-image-generation)\n\n## About FLUX-RealismLoRA\n\nThe FLUX-RealismLoRA is available on RunComfy as `flux/realism_lora.safetensors`.\n\nTrained on a curated dataset of high-resolution photorealistic images with text captions, the FLUX-RealismLoRA weights guide the FLUX-1 model to generate images resembling real photographs by emphasizing:\n- Detailed sharpness\n- Realistic textures, shadows, and lighting\n- Believable proportions and perspectives\n- Natural color palettes and tones\n\nThis compact FLUX-RealismLoRA file (22MB) can be loaded into FLUX-1, enhancing photorealistic generation without altering the base model weights. It functions as a \"realism filter,\" shaping the model's outputs.\n\nLicense：lora.safetensors falls under the FLUX.1 [dev] Non-Commercial License.\n\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1117",
        "readme": "## ComfyUI Linear Mask Dilation\n\nCreate stunning video animations by transforming your subject (dancer) and have them travel through different scenes via a mask dilation effect. This workflow is designed to be used with single subject videos.\n\n## How to use ComfyUI Linear Mask Dilation Workflow:\n\n1. Upload a subject video in the Input section\n2. Select the desired width and height of the final video, along with how many frames of the input video should be skipped with “every_nth”. You can also limit the total number of frames to render with “frame_load_cap”.\n3. Fill out the positive and negative prompt. Set batch frame times to match when you’d like the scene transitions to occur.\n4. Upload images for each of the IP Adapter subject mask colors:\n    1. White = subject (dancer)\n    2. Black = First background\n    3. Red = Red dilation mask background\n    4. Green = Green dilation mask background\n    5. Blue = Blue dilation mask background\n5. Load a good LCM checkpoint (I use ParadigmLCM by Machine Delusions) in the “Models” section.\n    1. Add any loras using the Lora stacker below the model loader\n6. Hit Queue Prompt\n\n## Input\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1117/readme01.webp\" alt=\"\" width=\"750\"/>\n\n- You can adjust the width, height using the top left two inputs\n- every_nth sets how many frames of the input should be skipped (2 = every other frame)\n- The number fields in bottom left display info about the uploaded input video: total frames, width, height, and FPS from top to bottom.\n- If you already have a mask video of the subject generated (must be white subject on black background), you can un-mute the “Override Subject Mask” section and upload the mask video. Optionally you can mute the “Segment Subject” section to save some processing time.\n- Sometimes the segmented subject will not be perfect, you can check the mask quality using the preview box in the bottom right seen above. If that is the case you can play around with the prompt in the “Florence2Run” node to target different body parts such as “head”, “chest”, “legs”, etc. and see if you get a better result.\n\n## Prompt\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1117/readme02.webp\" alt=\"\" width=\"450\"/>\n\n- Set the positive prompt using batch formatting:\n    - e.g. “0”: “4k, masterpiece, 1girl standing on the beach, absurdres”, “25”: “HDR, sunset scene, 1girl with black hair and a white jacket, absurdres”, …\n- Negative prompt is normal format, you can add embeddings if desired.\n\n## Mask Dilations\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1117/readme03.webp\" alt=\"\" width=\"750\"/>\n\n- Each colored group corresponds to the color of dilation mask that will be generated by it.\n- You can set the shape of the mask, along with speed of dilation and frame delay with the following node:\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1117/readme04.webp\" alt=\"\" width=\"450\"/>\n  - shape: “circle” is the most accurate but takes longer to generate. Set this when you are ready to perform the final rendering. “square” is fast to compute but less accurate, best for testing out the workflow and deciding on IP adapter images.\n  - dilate_per_frame: How quickly the mask should dilate, larger numbers = faster dilation speed\n  - delay: How many frames to wait before the mask begins to dilate.\n- If you already have a composite mask video generated you can un-mute the “Override Composite Mask” group and upload it. It’s recommended to bypass the dilation mask groups if overriding to save on processing time.\n\n## Models\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1117/readme05.webp\" alt=\"\" width=\"450\"/>\n\n- Use a good LCM model for the checkpoint. I recommend ParadigmLCM by Machine Delusions.\n- You can optionally specify the AnimateLCM_sd15_t2v_lora.safetensors with a low weight of 0.18 to further enhance the final result.\n- Add any additional Loras to the model using the blue Lora stacker below the model loader.\n\n## AnimateDiff\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1117/readme06.webp\" alt=\"\" width=\"450\"/>\n\n- You can set a different Motion Lora instead of the one I used (LiquidAF-0-1.safetensors)\n- Adjust the Multival Dynamic float value higher or lower depending on if you want the result to have more or less motion.\n\n## IP Adapters\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1117/readme07.webp\" alt=\"\" width=\"750\"/>\n\n- Here you can specify the reference subjects that will be used to render the backgrounds for each of the dilation masks, as well as your video subject.\n- The color of each group represents the mask it targets:\n    - White = subject (dancer)\n    - Black = First background\n    - Red = Red dilation mask background\n    - Green = Green dilation mask background\n    - Blue = Blue dilation mask background\n- If you want the final render to more closely follow the input IP adapter images, you can change the IPAdapter preset from VIT-G to PLUS in the IPA Unified Loader group.\n\n## ControlNet\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1117/readme08.webp\" alt=\"\" width=\"750\"/>\n\n- This workflow makes use of 5 different controlnets, including AD, Lineart, QR Code, Depth, and OpenPose.\n- All of the inputs to the controlnets are generated automatically\n- You can choose to override the input video for the Depth and Openpose controlnets if desired by un-muting the “Override Depth” and “Override Openpose” groups as seen below:\n    \n<img src=\"https://cdn.runcomfy.net/workflow_assets/1117/readme09.webp\" alt=\"\" width=\"450\"/>\n    \n\n- It is recommended you mute the “Generate Depth” and “Generate Openpose” groups if overriding to save processing time.\n\n## Sampler\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1117/readme10.webp\" alt=\"\" width=\"750\"/>\n\n- By default the HiRes Fix sampler group will be muted to save processing time when testing\n- I recommend bypassing the Sampler group as well when trying to experiment with dilation mask settings to save time.\n- On final renders you can un-mute the HiRes Fix group which will upscale and add details to the final result.\n\n## Output\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1117/readme11.webp\" alt=\"\" width=\"750\"/>\n\n- There are two output groups: the left is for standard sampler output, and the right is for the HiRes Fix sampler output.\n- You can change where files will be saved by changing the “custom_directory” string in the “FileNamePrefixDateDirFirst” nodes. By default this node will save output videos in a timestamped directory in the ComfyUI “output” directory\n    - e.g. `…/ComfyUI/output/240812/<custom_directory>/<my_video>.mp4`\n\n## About Author\n\nAkatz AI:\n\n- Website:  [https://akatz.ai](https://akatz.ai/)\n- https://www.youtube.com/@akatz_ai\n- https://www.instagram.com/akatz.ai/\n- https://www.tiktok.com/@akatz_ai\n- https://x.com/akatz_ai\n- https://github.com/akatz-ai\n\nContacts:\n\n- Email: akatzfey@sendysoftware.com\n"
    },
    {
        "id": "1119",
        "readme": "FLUX is a new image generation model developed by [Black Forest Labs](https://blackforestlabs.ai/). This ComfyUI FLUX Img2Img workflow is shared by Matt3o from his YouTube channel. please visit [Matt3o's Deep dive into the Flux](https://www.youtube.com/watch?v=tned5bYOC08) for more details. All credit goes to their contribution.\n\n## About FLUX\n\nThe FLUX models are preloaded on RunComfy, named `flux/flux-schnell` and `flux/flux-dev`.\n\n- **When launch a RunComfy Medium-Sized Machine**: Select the checkpoint `flux-schnell, fp8` and clip `t5_xxl_fp8` to avoid out-of-memory issues.\n- **When launch a RunComfy Large-Sized or Above Machine**: Opt for a large checkpoint `flux-dev, default` and a high clip `t5_xxl_fp16`.\n\nFor more details, visit: [ComfyUI FLUX | A New Art Image Generation](https://www.runcomfy.com/comfyui-workflows/comfyui-flux-a-new-art-image-generation)\n\n\n## What is the ComfyUI FLUX Img2Img?\nThe ComfyUI FLUX Img2Img workflow allows you to transform existing images using textual prompts. By combining the visual elements of a reference image with the creative instructions provided in the prompt, the FLUX Img2Img workflow creates stunning results. The FLUX Img2Img model excels at preserving key aspects of the original image while enhancing it with photorealistic details or artistic flair, based on the user's input. Whether you need subtle modifications or extensive artistic reimagining, the FLUX Img2Img workflow is the perfect tool for the job.\n\n## How to use the ComfyUI Flux Img2Img\nTo harness the power of the ComfyUI Flux Img2Img workflow, follow these steps:\n\n### Step 1: Configure DualCLIPLoader Node\n\nFor lower memory usage, load the sd3m/t5xxl_fp8_e4m3fn.safetensors using the FLUX Img2Img workflow.\nFor higher memory setups, load the sd3m/t5xxl_fp16.safetensors for optimal FLUX Img2Img performance.\n\n### Step 2: Configure Load Diffusion Model Node\n\nFor RunComfy medium-sized machines, set the checkpoint to flux_schnell,fp8 and clip to t5_xxl_fp8 to leverage the FLUX Img2Img model.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1119/readme01.webp\" alt=\"FLUX model\" width=\"750\"/>\n\nFor RunComfy large-sized machines or above, use flux_dev,default with t5_xxl_fp16 for enhanced FLUX Img2Img results.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1119/readme02.webp\" alt=\"FLUX model\" width=\"750\"/>\n\n### Step 3: Configure VAELoader Node\n\nEnsure the required VAE model for FLUX Img2Img is preloaded for seamless integration.\n\n### Step 4: Load Your Reference Image\n\nLoad your reference image to begin the FLUX Img2Img transformation process.\n\n### Step 5: Set Up CLIPTextEncode Node\n\nEnter your desired text prompt in the node's properties to guide the FLUX Img2Img generation process.\n\n### Step 6: Configure SamplerCustomAdvanced Node\n\nThis node processes inputs like noise, guider, sampler, sigmas, and latent_image to produce the denoised output for the FLUX Img2Img workflow.\n\n### Step 7: Adjust KSamplerSelect Node (Optional)\n\nSelect the specific sampler algorithm (e.g., \"euler\") by adjusting this node's properties to fine-tune your FLUX Img2Img results.\n\n### Step 8: Generate Image\n\nIn this way, you can generate your desired image using the FLUX Img2Img workflow, merging the original image's key features with the creative aspects outlined by your prompts. FLUX Img2Img retains essential elements from the original image, such as background colors and specific areas, while incorporating the prompt-driven enhancements.\n\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1120",
        "readme": "FLUX is a new image generation model developed by [Black Forest Labs](https://blackforestlabs.ai/).\n\n## About FLUX\n\nThe FLUX models are preloaded on RunComfy, named `flux/flux-schnell` and `flux/flux-dev`.\n\n- **When launch a RunComfy Medium-Sized Machine**: Select the checkpoint `flux-schnell, fp8` and clip `t5_xxl_fp8` to avoid out-of-memory issues.\n- **When launch a RunComfy Large-Sized or Above Machine**: Opt for a large checkpoint `flux-dev, default` and a high clip `t5_xxl_fp16`.\n\nFor more details, visit: [ComfyUI FLUX | A New Art Image Generation](https://www.runcomfy.com/comfyui-workflows/comfyui-flux-a-new-art-image-generation)\n\n\n## What is the ComfyUI Flux Inpainting?\nThe ComfyUI FLUX Inpainting workflow leverages the inpainting capabilities of the Flux family of models developed by Black Forest Labs. FLUX Inpainting is a valuable tool for image editing, allowing you to fill in missing or damaged areas of an image with impressive results. It is particularly useful for restoring old photographs, removing unwanted objects, or refining AI-generated images. By utilizing the advanced image processing techniques of the Flux model, FLUX Inpainting seamlessly integrates newly generated content with the existing image, resulting in natural and cohesive outputs.\n\n## How to use the ComfyUI Flux Inpainting\nTo use the ComfyUI Flux Inpainting workflow effectively, follow these steps:\n\n### Step 1: Configure DualCLIPLoader Node\n\nFor lower memory usage, load the sd3m/t5xxl_fp8_e4m3fn.safetensors.\nFor higher memory setups, load the sd3m/t5xxl_fp16.safetensors.\n\n### Step 2: Configure Load Diffusion Model Node\n\nFor RunComfy medium-sized machines, set the checkpoint to flux_schnell,fp8 and clip to t5_xxl_fp8.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1120/readme01.webp\" alt=\"FLUX model\" width=\"750\"/>\n\nFor RunComfy large-sized machines or above, use flux_dev,default with t5_xxl_fp16 for better performance.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1120/readme02.webp\" alt=\"FLUX model\" width=\"750\"/>\n\n### Step 3: Configure VAELoader Node\n\nEnsure the required VAE model for FLUX is preloaded.\n\n### Step 4: Load and Mask the Image for Inpainting\n\nLoad the image you want to transform with FLUX Inpainting. Then Right-click on the image and select the \"Open in MaskEditor\" option to access the powerful masking tools. Use the MaskEditor to precisely select the areas you want to apply FLUX Inpainting to.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1120/readme03.webp\" alt=\"FLUX inpainting\" width=\"750\"/>\n\n### Step 5: Set Up CLIPTextEncode Node\n\nEnter your desired text prompt in the node's properties to guide the FLUX Inpainting process and achieve your desired outcome.\n\n### Step 6: Generate the Image\n\nWith all the pieces in place, proceed to generate your image using the FLUX Inpainting technology.\n\nThe ComfyUI FLUX Inpainting workflow is a powerful tool for enhancing images, making it an essential addition to any image editing toolkit. By leveraging the capabilities of FLUX Inpainting, you can achieve professional-quality results with ease.\n\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1121",
        "readme": "FLUX is a new image generation model developed by [Black Forest Labs](https://blackforestlabs.ai/). This FLUX NF4 model is created by lllyasviel, please visit [this link](https://huggingface.co/lllyasviel/flux1-dev-bnb-nf4) for more information.\n\n## About FLUX\n\nThe FLUX models are preloaded on RunComfy, named `flux/flux-schnell` and `flux/flux-dev`.\n\n- **When launch a RunComfy Medium-Sized Machine**: Select the checkpoint `flux-schnell, fp8` and clip `t5_xxl_fp8` to avoid out-of-memory issues.\n- **When launch a RunComfy Large-Sized or Above Machine**: Opt for a large checkpoint `flux-dev, default` and a high clip `t5_xxl_fp16`.\n\nFor more details, visit: [ComfyUI FLUX | A New Art Image Generation](https://www.runcomfy.com/comfyui-workflows/comfyui-flux-a-new-art-image-generation)\n\n## Introduction to FLUX NF4\n\nFLUX NF4 is a specialized model checkpoint designed for performance optimization in Stable Diffusion workflows. Developed by the author *lllyasviel*, this model utilizes NF4 (Normal Float 4-bit) quantization to significantly improve inference speed and reduce memory usage compared to traditional FP8 (Float 8-bit) models. FLUX NF4 is part of a series of models aimed at enhancing efficiency, particularly on newer GPU architectures like the NVIDIA RTX 3000 and 4000 series. The model includes advanced features such as \"Distilled CFG Guidance,\" which refines the process of generating images with more accurate prompts. On a high-end GPU like the RTX 4090, the regular FLUX model takes around 50 seconds to generate an image, while FLUX NF4 only takes about 13-14 seconds. This makes FLUX NF4 accessible to a wider range of users who may not have top-of-the-line hardware.\n\n## How to use ComfyUI FLUX NF4\n\n### 1. Loading the Model: CheckpointLoaderNF4\n\nThis node loads the FLUX model (`flux/flux1-dev-bnb-nf4-v2.safetensors`). The model is responsible for guiding the entire image generation process by providing the underlying framework that controls the behavior and characteristics of the generated images.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1121/readme01.webp\" alt=\"FLUX NF4\" width=\"450\"/>\n\n### 2. Generating Random Noise: RandomNoise\n\nThis node generates a random noise pattern, which serves as the initial input for image generation. The noise acts as the starting point that will be transformed into a final result.\n\n### 3.*Model Sampling Flux: ModelSamplingFlux\n\nThe `ModelSamplingFlux` node adjusts the model's sampling behavior based on resolution and other parameters. It optimizes the model's output, ensuring that the image quality is maintained as transformations are applied. If you prefer not to adjust the sampling behavior, this node can be bypassed.\n\n### 4. Setting Image Dimensions: PrimitiveNode (Width and Height)\n\nThese nodes define the dimensions of the image (width and height), typically set to 1024x1024. The specified dimensions influence the resolution and detail level of the generated image.\n\n### 5. Conditioning with CLIP Text: CLIPTextEncode (Positive and Negative)\n\nThe CLIPTextEncode nodes encode text prompts into conditioning data that guides the image generation process. Positive prompts enhance desired features, while negative prompts suppress undesired ones, giving you control over the content and style of the output.\n\n### 6. Applying Flux Guidance: FluxGuidance\n\nThe `FluxGuidance` node applies a guidance scale (e.g., 3.5) to the conditioning data. This scale adjusts the influence of the text prompts on the final result, allowing for fine-tuning of the generated output.\n\n### 7. Scheduling with BasicScheduler: BasicScheduler\n\nThis node manages the scheduling of the image generation process, controlling the transition from noise to the final image. The scheduling parameters affect how quickly and smoothly the image evolves during generation.\n\n### 8. Custom Sampling: SamplerCustomAdvanced\n\nThis advanced sampler node refines the image by applying additional transformations to the latent image. It integrates the noise, guider, sampler, sigmas, and latent image data to produce a high-quality output.\n\n### 9. Decoding the VAE: VAEDecode\n\nThe `VAEDecode` node decodes the latent image into an actual visual image using a VAE (Variational Autoencoder). This step is crucial for translating the abstract latent space into a visible and interpretable image.\n\n### 10. Upscaling the Image: UpscaleModelLoader and UltimateSDUpscale\n\nThe `UpscaleModelLoader` loads an upscaling model (e.g., `4x-UltraSharp.pth`), and the `UltimateSDUpscale` node applies this model to enhance the image resolution. This step ensures that the final image is sharp and detailed, even at higher resolutions.\n\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1122",
        "readme": "FLUX is a new image generation model developed by [Black Forest Labs](https://blackforestlabs.ai/). The FLUX IPAdapter checkpoint for the FLUX.1-dev model is provided by the XLabs AI team, who also created the ComfyUI FLUX IPAdapter workflow. For more details, please visit [FLUX IPAdapter](https://huggingface.co/XLabs-AI/flux-ip-adapter). All credit goes to their contribution.\n\n## About FLUX\n\nThe FLUX models are preloaded on RunComfy, named `flux/flux-schnell` and `flux/flux-dev`.\n\n- **When launch a RunComfy Medium-Sized Machine**: Select the checkpoint `flux-schnell, fp8` and clip `t5_xxl_fp8` to avoid out-of-memory issues.\n- **When launch a RunComfy Large-Sized or Above Machine**: Opt for a large checkpoint `flux-dev, default` and a high clip `t5_xxl_fp16`.\n\nFor more details, visit: [ComfyUI FLUX | A New Art Image Generation](https://www.runcomfy.com/comfyui-workflows/comfyui-flux-a-new-art-image-generation)\n\n🌟The following FLUX IPAdapter Workflow is specifically designed for the [`FLUX.1 [dev]`](https://github.com/black-forest-labs/flux) model.🌟\n\n## About FLUX IPAdapter\n\nFLUX IPAdapter allows users to generate images by adapting pre-trained models to specific image styles or concepts. The FLUX IPAdapter model is trained on both 512x512 and 1024x1024 resolutions, making it versatile for various image generation tasks. \n\n## How to Use ComfyUI FLUX IPAdapter Workflow\n\nTo use the FLUX IPAdapter in ComfyUI, follow these steps:\n\n### 1. Load the FLUX IPAdapter Model\n\n- Use the \"Flux Load IPAdapter\" node in the ComfyUI workflow.\n- Select the appropriate FLUX IPAdapter model file (e.g., \"flux-ip-adapter.safetensors\").\n- Select the appropriate clip vision (e.g., \"clip_vision_l.safetensors\").\n- Choose the desired device for running the model (e.g., CPU or GPU).\n\n### 2. Apply the FLUX IPAdapter to the Base Model\n\n- Connect the output of the \"Flux Load IPAdapter\" node to the \"Apply Flux IPAdapter\" node.\n- Load the base model using the \"UNETLoader\" node and connect its output to the \"Apply Flux IPAdapter\" node.\n- Set the desired mix strength (e.g., 0.92) in the \"Apply Flux IPAdapter\" node to control the influence of the IPAdapter on the base model.\n\n### 3. Provide Input Image and Text Conditioning\n\n- Load an input image using the \"LoadImage\" node and connect its output to the \"ImageScale\" node.\n- Scale the input image to the desired resolution using the \"ImageScale\" node.\n- Connect the output of the \"ImageScale\" node to the \"Apply Flux IPAdapter\" node.\n- Use the \"CLIPTextEncodeFlux\" node to provide text conditioning for the image generation process. Enter the desired text prompt and connect its output to the \"XlabsSampler\" node.\n\n### 4. Configure Sampling Settings and Generate\n\n- In the \"XlabsSampler\" node, set the following parameters:\n    - Sampling Steps: The number of steps for the sampling process (e.g., 50).\n    - Sampler: The sampling method to use (e.g., \"fixed\").\n    - CFG Scale: The Classifier-Free Guidance scale to control the influence of the text conditioning (e.g., 3.5).\n    - Seed: The random seed for reproducibility (set to \"-1\" for random seed).\n\nLicense: FLUX IPAdapter model weights fall under the [**FLUX.1 [dev]**](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) Non-Commercial License\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1123",
        "readme": "FLUX LoRA has gained immense popularity in the AI community, particularly among those seeking to fine-tune AI models with their own datasets. This approach allows you to effortlessly adapt pre-existing FLUX models to your unique datasets, making it highly customizable and efficient for a wide range of creative endeavors. If you're already familiar with ComfyUI, using the ComfyUI FLUX LoRA Training workflow to train your FLUX LoRA model will be a breeze. The workflow and related nodes were created by Kijai, so big thanks to him for his contribution! Check out [Kijai's GitHub](https://github.com/kijai/ComfyUI-FluxTrainer/tree/main) for more info.\n\n## ComfyUI FLUX LoRA Training Tutorial\n\nThe ComfyUI FLUX LoRA Training workflow is a powerful process designed for training FLUX LoRA models. Training with ComfyUI offers several advantages, particularly for users already familiar with its interface. With FLUX LoRA Training, you can use the same models employed for inference, ensuring there are no compatibility issues when working within the same Python environment. Additionally, you can build workflows to compare different settings, enhancing your training process. This tutorial will guide you through the steps to set up and use FLUX LoRA Training in ComfyUI.\n\nWe will cover:\n1. Preparing Your Dataset for FLUX LoRA Training\n2. The FLUX LoRA Training Process\n3. Executing FLUX LoRA Training\n4. How and Where to Use the FLUX and FLUX LoRA Models\n\n## 1. Preparing Your Dataset for FLUX LoRA Training\n\nWhen preparing your training data for FLUX LoRA Training, it's essential to have high-quality images for your target subject.\n\nIn this example, we're training a FLUX LoRA model to generate images of a specific influencer. For this, you'll need a set of high-quality images of the influencer in various poses and settings. A convenient way to gather these images is by using the [ComfyUI Consistent Character workflow](https://www.runcomfy.com/comfyui-workflows/create-consistent-characters-within-comfyui), which makes it easy to generate a collection of images that show the same character in different poses while keeping their appearance consistent. For our training dataset, we've selected five high-quality images of the influencer in various poses and settings, ensuring the dataset is robust enough for FLUX LoRA Training to learn the intricate details needed to produce consistent and accurate outputs.\n\n### Process for Obtaining Training Data\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme01.webp\" alt=\"FLUX LoRA Training Data\" width=\"750\"/>\n\n### Training Data Example\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme02.webp\" alt=\"FLUX LoRA Training Data\" width=\"750\"/>\n\nYou can also collect your own dataset based on your specific needs——FLUX LoRA Training is flexible and works with various types of data.\n\n## 2. The FLUX LoRA Training Process\n\nThe FLUX LoRA Training workflow consists of several key nodes that work together to train and validate your model. Here's a detailed overview of the main nodes, separated into three parts: Dataset, Settings and Init, and Training.\n\n### 2.1. Set Datasets for FLUX LoRA Training\n\nThe Dataset section consists of two essential nodes that help you configure and customize your training data: **TrainDatasetGeneralConfig** and **TrainDatasetAdd**.\n\n### 2.1.1. TrainDatasetGeneralConfig\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme03.webp\" alt=\"FLUX LoRA Training: TrainDatasetGeneralConfig\" width=\"400\"/>\n\nThe **TrainDatasetGeneralConfig** node is where you define the overall settings for your training dataset in FLUX LoRA Training. This node gives you control over various aspects of data augmentation and preprocessing. For instance, you can choose to enable or disable **color augmentation**, which can help improve the model's ability to generalize across different color variations. Similarly, you can toggle **flip augmentation** to randomly flip images horizontally, providing more diverse training samples. Additionally, you have the option to **shuffle the captions** associated with each image, introducing randomness and reducing overfitting. The **caption dropout rate** allows you to randomly drop captions during training, which can help the model become more robust to missing or incomplete captions.\n\n### 2.1.2. TrainDatasetAdd\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme04.webp\" alt=\"FLUX LoRA Training: TrainDatasetAdd\" width=\"400\"/>\n\nThe **TrainDatasetAdd** node is where you specify the details of each individual dataset to include in your FLUX LoRA Training. \n\n#### Input directory: Train Dataset Path\n\nTo make the most of this node, it's important to organize your training data properly. When using RunComfy’s file browser, place the training data in the `/home/user/ComfyUI/input/{file-name}` directory, where `{file-name}` is a meaningful name you assign to your dataset.\n\nOnce you've placed your training data in the appropriate directory, you need to provide the path to that directory in the `image_dir` parameter of the **TrainDatasetAdd** node. This tells the node where to find your training images.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme05.webp\" alt=\"FLUX LoRA TrainDatasetAdd: Dataset Path\" width=\"750\"/>\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme06.webp\" alt=\"FLUX LoRA TrainDatasetAdd: Dataset Path\" width=\"400\"/>\n\n#### Class Token\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme07.webp\" alt=\"FLUX LoRA TrainDatasetAdd: Class Token\" width=\"400\"/>\n\nIf your dataset benefits from using specific class tokens or trigger words, you can enter them in the `class_tokens` parameter. Class tokens are special words or phrases that are prepended to each caption and help guide the model's generation process. For instance, if you're training on a dataset of various animal species, you could use class tokens like \"dog\", \"cat\", or \"bird\" to indicate the desired animal in the generated images. When you later use these class tokens in your prompts, you can control which specific aspects you want the model to generate.\n\n#### Set the resolution (width and height), batch size\n\nIn addition to the `image_dir` and `class_tokens` parameters, the **TrainDatasetAdd** node provides several other options to fine-tune your dataset. You can set the resolution (width and height) of the images, specify the batch size for training, and determine the number of times the dataset should be repeated per epoch.\n\n#### Multiple datasets\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme08.webp\" alt=\"FLUX LoRA TrainDatasetAdd: Multiple datasets\" width=\"750\"/>\n\nOne of the powerful features of FLUX LoRA Training is the ability to combine multiple datasets seamlessly. In the FLUX LoRA Training  workflow, there are three **TrainDatasetAdd** nodes connected in sequence. Each node represents a distinct dataset with its own unique settings. By linking these nodes together, you can create a rich and diverse training set that incorporates images and captions from various sources.\n\nTo illustrate this, let's consider a scenario where you have three separate datasets: one for cats, one for dogs, and another for bears. You can set up three TrainDatasetAdd nodes, each dedicated to one of these datasets. In the first node, you would specify the path to the \"cats\" dataset in the `image_dir` parameter, set the `class token` to \"cat,\" and adjust other parameters like resolution and batch size to suit your needs. Similarly, you would configure the second and third nodes for the \"dogs\" and \"bears\" datasets, respectively.\n\nThis approach allows the FLUX LoRA Training process to leverage a diverse range of images, improving the model’s ability to generalize across different categories.\n\n#### Example\n\nIn our example, we use only one dataset to train the model, so we enable one **TrainDatasetAdd** node and bypass the other two. Here's how you can set it up:\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme09.webp\" alt=\"FLUX LoRA TrainDatasetAdd: Multiple datasets\" width=\"750\"/>\n\n### 2.2. Settings and Initialization\n\nThe Settings and Initialization section is where you configure the key components and parameters for FLUX LoRA Training. This section includes several essential nodes that work together to set up your training environment.\n\n### 2.2.1. FluxTrainModelSelect\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme10.webp\" alt=\"FLUX LoRA Training: FluxTrainModelSelect\" width=\"600\"/>\n\nFirst, you have the **FluxTrainModelSelect** node, which is responsible for selecting the FLUX models that will be used during FLUX LoRA Training. This node allows you to specify the paths to four critical models: the transformer, VAE (Variational Autoencoder), CLIP_L (Contrastive Language-Image Pre-training), and T5 (Text-to-Text Transfer Transformer). These models form the backbone of the FLUX training process, and all have been set up on the RunComfy platform.\n\n### 2.2.2. OptimizerConfig\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme11.webp\" alt=\"FLUX LoRA Training: OptimizerConfig\" width=\"450\"/>\n\nThe **OptimizerConfig** node is crucial for setting up the optimizer within FLUX LoRA Training, which determines how the model's parameters are updated during training. You can choose the optimizer type (e.g., AdamW, CAME), set the maximum gradient norm for gradient clipping to prevent exploding gradients, and select the learning rate scheduler (e.g., constant, cosine annealing). Additionally, you can fine-tune optimizer-specific parameters like warmup steps and scheduler power, and provide extra arguments for further customization.\n\nIf you prefer the Adafactor optimizer, known for its memory efficiency and ability to handle large models, you can use the **OptimizerConfigAdafactor** node instead.\n\n### 2.2.3. **InitFluxLoRATraining**\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme12.webp\" alt=\"FLUX LoRA Training: InitFluxLoRATraining\" width=\"450\"/>\n\nThe **InitFluxLoRATraining** node is the central hub where all the essential components converge to kickstart the FLUX LoRA Training process.\n\n#### Output directory: FLUX LoRA Path\n\nOne of the key things you'll need to specify in the InitFluxLoRATraining node is the output directory, where your trained model will be saved. On the RunComfy platform, you can choose `/home/user/ComfyUI/output/{file_name}` as the location for your output. Once the training is complete, you will be able to view it in the file browser.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme13.webp\" alt=\"FLUX LoRA InitFluxLoRATraining: FLUX LoRA Path\" width=\"750\"/>\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme14.webp\" alt=\"FLUX LoRA InitFluxLoRATraining: FLUX LoRA Path\" width=\"450\"/>\n\n#### Network dimensions and learning rates\n\nNext, you'll want to set the network dimensions and learning rates. The network dimensions determine the size and complexity of your LoRA network, while the learning rates control how quickly your model learns and adapts.\n\n#### Max train steps\n\nAnother important parameter to consider is the `max_train_steps`. It determines how long you want the training process to run, or in other words, how many steps you want your model to take before it's fully baked. You can adjust this value based on your specific needs and the size of your dataset. It's all about finding that sweet spot where your model has learned enough to produce mouth-watering outputs!\n\n### 2.3.4. FluxTrainValidationSettings\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme15.webp\" alt=\"FLUX LoRA Training: FluxTrainValidationSettings\" width=\"450\"/>\n\nFinally, the **FluxTrainValidationSettings** node allows you to configure the validation settings for evaluating your model's performance during the FLUX LoRA Training process. You can set the number of validation steps, image size, guidance scale, and seed for reproducibility. Additionally, you can choose the timestep sampling method and adjust the sigmoid scale and shift parameters to control the timestep scheduling and improve the quality of the generated images.\n\n## 3. Train\n\nThe Train section of FLUX LoRA Training is where the magic happens. It's divided into four parts: Train_01, Train_02, Train_03, and Train_04. Each of these parts represents a different stage in the FLUX LoRA Training process, allowing you to gradually refine and improve your model.\n\n### 3.1. Train_01\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme16.webp\" alt=\"FLUX LoRA Training\" width=\"450\"/>\n\nLet's start with **Train_01**. This is where the initial training loop takes place. The star of this section is the **FluxTrainLoop** node, which is responsible for executing the training loop for a specified number of steps. In this example, we've set it to 250 steps, but you can adjust this based on your needs. Once the training loop is complete, the trained model is passed to the **FluxTrainSave** node, which saves the model at regular intervals. This ensures that you have checkpoints of your model at different stages of training, which can be useful for tracking progress and recovering from any unexpected interruptions.\n\nBut training isn't just about saving the model. We also need to validate its performance to see how well it's doing. That's where the **FluxTrainValidate** node comes in. It takes the trained model and puts it to the test using a validation dataset. This dataset is separate from the training data and helps assess how well the model generalizes to unseen examples. The **FluxTrainValidate** node generates sample images based on the validation data, giving you a visual representation of the model's output at this stage.\n\nTo keep an eye on the training progress, we have the **VisualizeLoss** node. This handy node visualizes the training loss over time, allowing you to see how well the model is learning and whether it's converging towards a good solution. It's like having a personal trainer who keeps track of your progress and helps you stay on track.\n\n### 3.2. Train_02, Train_03, Train_04\n\nIn **Train_02**, continuing from **Train_01** in the FLUX LoRA Training, the output is further trained for an additional specified number of steps (e.g., 250 steps). **Train_03** and **Train_04** follow a similar pattern, extending training with updated connections for a smooth progression. Each stage outputs a FLUX LoRA model, allowing you to test and compare performance.\n\n#### Example\n\nIn our example, we've chosen to use only **Train_01** and **Train_02**, each running for 250 steps. We've bypassed **Train_03** and **Train_04** for now. But feel free to experiment and adjust the number of training sections and steps based on your specific needs and resources.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme17.webp\" alt=\"FLUX LoRA Training\" width=\"750\"/>\n\n## 4. How and Where to Use the FLUX and FLUX LoRA Models\n\nOnce you have the FLUX LoRA model, you can incorporate it into the [FLUX LoRA workflow](https://www.runcomfy.com/comfyui-workflows/comfyui-flux-realismlora-workflow-photorealistic-ai-images). Replace the existing LoRA model with your trained model, then test the results to evaluate its performance. \n\n#### Example\nIn our example, we use the FLUX LoRA workflow to generate more influencer images by applying the FLUX LoRA model and observing its performance.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1123/readme18.webp\" alt=\"FLUX LoRA Training\" width=\"750\"/>\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1124",
        "readme": "ComfyUI Vid2Vid Dance Transfer is a powerful ComfyUI Vid2Vid workflow that allows users to stylize and transform real videos into various objects or elements. With this workflow, you can create stunning video animations by transferring the motion and style of a source video onto a target image or object. This tutorial will guide you through the key components and settings required to achieve the best results.\n\n**Credit**\n\nThanks to MDMZ and DP for their incredible contributions in creating this workflow! MDMZ shared his ComfyUI Vid2Vid Dance Transfer Workflow through his YouTube tutorial [Transform Videos with AI: Dancing Noodles Step-by-Step Tutorial](https://www.youtube.com/watch?v=d3vpKqTiTvc), which builds upon DP's original [DP's - Vid2Vid AniamteDiff LCM dance transfer](https://civitai.com/models/559596?modelVersionId=643099) workflow.\n\n\n## 1. What does ComfyUI Vid2Vid Dance Transfer Workflow do?\n\nThe ComfyUI Vid2Vid Dance Transfer workflow enables you to take a real video and transform it into a stylized animation. It transfers the motion and dance moves from the source video onto a target image or object of your choice. \n\n## 2. Key Components of the ComfyUI Vid2Vid Workflow\n\n### 2.1 Uploading Video and Creating Mask\n\n- The ComfyUI Vid2Vid workflow starts with the VHS_LoadVideo component, where you upload the source video that contains the dance moves you want to transfer.\n- The LayerMask: RemBgUltra component is used to remove the background from the video frames and create a black and white mask of the subject. This mask is crucial for properly identifying and transferring the motion.\n- The ImageToMask and MaskToImage components convert between image and mask formats as needed in the ComfyUI Vid2Vid workflow.\n- 🌟 Important Tips: RunComfy platform has preloaded the \"RemBgultra model\" required for the \"Layer Style\" node. If you have installed the \"Layer Style\" nodes yourself, please delete the \"Layer Style\" file from your private file browser to avoid conflicts. Once you delete your \"Layer Style\" file, the workflow will use the RunComfy preload and run smoothly.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1124/readme01.webp\" alt=\"ComfyUI Vid2Vid Workflow\" width=\"700\"/>\n\n### 2.2 Setting Up Target Image with IPAdapter\n\n- The IPAdapterAdvanced components are used to load and configure the target image or object that will receive the ComfyUI Vid2Vid dance transfer. You can upload the target image using the LoadImage node, and upload the target background image using the LoadBgImage node\n- The IPAdapterUnifiedLoader is used to load the IPAdapter, which determines the strength of the target image's influence on the output.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1124/readme02.webp\" alt=\"ComfyUI Vid2Vid Workflow\" width=\"700\"/>\n\n### 2.3 Applying AnimateDiff for Motion Transfer\n\n- The core of the ComfyUI Vid2Vid dance transfer is powered by the AnimateDiff Latent Condition Model (LCM). This is loaded using the ADE_LoadAnimateDiffModel component in ComfyUI Vid2Vid.\n- Additional components like ADE_AnimateDiffSamplingSettings, ADE_LoopedUniformContextOptions, and ADE_ApplyAnimateDiffModelSimple are used to configure the sampling settings, context options, and apply the AnimateDiff model respectively in ComfyUI Vid2Vid.\n- The ADE_UseEvolvedSampling component is used to select the appropriate sampling method for the AnimateDiff model in ComfyUI Vid2Vid.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1124/readme03.webp\" alt=\"ComfyUI Vid2Vid Workflow\" width=\"700\"/>\n\n### 2.4 Guiding Output with ControlNet\n\n- ControlNet is used to guide and define the subject's shape and outline in the output animation.\n- The ControlNetLoaderAdvanced components are used to load the ControlNet models. This workflow uses two ControlNet models: QR Code Monster and Lineart.\n- The ControlNet Stacker components are used to stack and combine the ControlNet outputs with adjustable strengths in ComfyUI Vid2Vid Transfer.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1124/readme04.webp\" alt=\"ComfyUI Vid2Vid Workflow\" width=\"700\"/>\n\n### 2.5 Generating Output Frames with KSampler\n\n- The KSampler (Efficient) components in ComfyUI Vid2Vid are responsible for generating the output frames based on all the provided configurations and inputs.\n- The main KSampler component processes the video frames to create the animation preview. It takes the AnimateDiff model, IPAdapter outputs, ControlNet stack, and other settings as inputs.\n- The second KSampler component is used for upscaling and denoising the ComfyUI Vid2Vid output frames.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1124/readme05.webp\" alt=\"ComfyUI Vid2Vid Workflow\" width=\"700\"/>\n\n### 2.6 Upscaling if needed\n\n- The ImageScaleBy component is used to scale up the resolution of the output frames. The ImageUpscaleWithModel component is used to further upscale the frames using a chosen upscale model. The workflow defaults to disabling the upscale components. If you need it, enable these components to have a better result.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1124/readme06.webp\" alt=\"ComfyUI Vid2Vid Workflow\" width=\"700\"/>\n"
    },
    {
        "id": "1125",
        "readme": "## 1. About CogVideoX-5B\n\nCogVideoX-5B is a cutting-edge text-to-video diffusion model developed by Zhipu AI at Tsinghua University. As part of the CogVideoX series, this model creates videos directly from text prompts using advanced AI techniques such as a 3D Variational Autoencoder (VAE) and an Expert Transformer. CogVideoX-5B generates high-quality, temporally consistent results that capture complex motion and detailed semantics.\n\nWith CogVideoX-5B, you achieve exceptional clarity and fluidity. The model ensures seamless flow, capturing intricate details and dynamic elements with extraordinary accuracy. Leveraging CogVideoX-5B reduces inconsistencies and artifacts, leading to a polished and engaging presentation. The high-fidelity outputs of CogVideoX-5B facilitate the creation of richly detailed and coherent scenes from text prompts, making it an essential tool for top-tier quality and visual impact.\n\n## 2. The Technique of CogVideoX-5B\n\n### 2.1 3D Causal Variational Autoencoder (VAE) of CogVideoX-5B\n\nThe 3D Causal VAE is a key component of CogVideoX-5B, enabling efficient video generation by compressing video data both spatially and temporally. Unlike traditional models that use 2D VAEs to process each frame individually—often resulting in flickering between frames—CogVideoX-5B uses 3D convolutions to capture both spatial and temporal information at once. This approach ensures smooth and coherent transitions across frames.\n\nThe architecture of the 3D Causal VAE includes an encoder, a decoder, and a latent space regularizer. The encoder compresses video data into a latent representation, which the decoder then uses to reconstruct the video. A Kullback-Leibler (KL) regularizer constrains the latent space, ensuring the encoded video remains within a Gaussian distribution. This helps maintain high video quality during reconstruction.\n\n**Key Features of the 3D Causal VAE**\n- Spatial and Temporal Compression: The VAE compresses video data by a factor of 4x in the temporal dimension and 8x8 in the spatial dimensions, achieving a total compression ratio of 4x8x8. This reduces computational demands, allowing the model to process longer videos with fewer resources.\n- Causal Convolution: To preserve the order of frames in a video, the model uses temporally causal convolutions. This ensures that future frames don't influence the prediction of current or past frames, maintaining the sequence's integrity during generation.\n- Context Parallelism: To manage the high computational load of processing long videos, the model uses context parallelism in the temporal dimension, distributing the workload across multiple devices. This optimizes the training process and reduces memory usage.\n\n### 2.2 Expert Transformer Architecture of CogVideoX-5B\n\nCogVideoX-5B's expert transformer architecture is designed to handle the complex interaction between text and video data effectively. It uses an adaptive LayerNorm technique to process the distinct feature spaces of text and video.\n\n**Key Features of the Expert Transformer**\n- Patchification: After the 3D Causal VAE encodes the video data, it's divided into smaller patches along the spatial dimensions. This process, called patchification, converts the video into a sequence of smaller segments, making it easier for the transformer to process and align with the corresponding text data.\n- 3D Rotary Positional Embedding (RoPE): To capture spatial and temporal relationships within the video, CogVideoX-5B extends the traditional 2D RoPE to 3D. This embedding technique applies positional encoding to the x, y, and t dimensions of the video, helping the transformer effectively model long video sequences and maintain consistency across frames.\n- Expert Adaptive LayerNorm (AdaLN): The transformer uses an expert adaptive LayerNorm to process the text and video embeddings separately. This allows the model to align the different feature spaces of text and video, enabling smooth fusion of these two modalities.\n\n### 2.3 Progressive Training Techniques of CogVideoX-5B\n\nCogVideoX-5B uses several progressive training techniques to improve its performance and stability during video generation.\n\n**Key Progressive Training Strategies**\n- Mixed-Duration Training: The model is trained on videos of various lengths within the same batch. This technique enhances the model's ability to generalize, enabling it to generate videos of different durations while maintaining consistent quality.\n- Resolution Progressive Training: The model is first trained on lower-resolution videos and then gradually fine-tuned on higher-resolution videos. This approach allows the model to learn the basic structure and content of videos before refining its understanding at higher resolutions.\n- Explicit Uniform Sampling: To stabilize the training process, CogVideoX-5B uses explicit uniform sampling, setting different timestep sampling intervals for each data parallel rank. This method accelerates convergence and ensures the model learns effectively across the entire video sequence.\n\n\n## 3. How to Use the ComfyUI CogVideoX-5B Workflow\n\n### Step 1: Load the CogVideoX-5B Model\nBegin by loading the CogVideoX-5B model into the ComfyUI workflow. The CogVideoX-5B models have been preload on RunComfy's platform.\n\n### Step 2: Input Your Text Prompt\nEnter your desired text prompt in the designated node to guide the CogVideoX-5B video generation process. CogVideoX-5B excels at interpreting and transforming text prompts into dynamic video content.\n\n\n## 4. License Agreement\n\nThe code of CogVideoX models is released under the [Apache 2.0 License](https://github.com/THUDM/CogVideo/blob/main/LICENSE).\n\nThe CogVideoX-2B model (including its corresponding Transformers module and VAE module) is released under the [Apache 2.0 License](https://github.com/THUDM/CogVideo/blob/main/LICENSE).\n\nThe CogVideoX-5B model (Transformers module) is released under the [CogVideoX LICENSE](https://huggingface.co/THUDM/CogVideoX-5b/blob/main/LICENSE).\n"
    },
    {
        "id": "1126",
        "readme": "Create stunning video animations by transforming your subject (e.g., a dancer) with a dynamic aura that rhythmically expands and contracts in sync with the beat. Use this workflow with single subjects or multiple subjects as seen in the examples.\n\n## How to use Audioreactive Mask Dilation Workflow:\n\n1. Upload a subject video in the Input section\n2. Select the desired width and height for the final video, along with how many frames from the input video should be skipped with 'every_nth'. You can also limit the total number of frames to render with 'frame_load_cap'.\n3. Fill out the positive and negative prompt. Set batch frame times to match when you’d like the scene transitions to occur.\n4. Upload images for each of the default IP Adapter subject mask colors:\n    1. Red = subject (dancer)\n    2. Black = Background\n    3. White = White audioreactive dilation mask\n5. Load a good LCM checkpoint (I use ParadigmLCM by Machine Delusions) in the 'Models' section.\n    1. Add any loras using the Lora stacker below the model loader\n6. Hit Queue Prompt\n\n## Input\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1126/readme01.webp\" alt=\"\" width=\"750\"/>\n\n- Upload your desired subject video to the Load Video (Upload) node.\n- Adjust the output width and height using the top left two inputs.\n- **every_nth** sets whether to use every other frame, every third frame and so on (2 = every other frame). Left at 1 by default.\n- **skip_frames** is used to skip frames at the video’s beginning. (100 = skip the first 100 frames from input video). Left at 0 by default.\n- **frame_load_cap** is used to specify how many total frames from the input video should be loaded. Best to keep low when testing settings (30 - 60 for example) and then increase or set to 0 (no frame cap) when rendering the final video.\n- The number fields in bottom right display info about the uploaded input video: total frames, width, height, and FPS from top to bottom.\n- If you already have a mask video of the subject generated , un-mute the 'Upload Subject Mask' section and upload the mask video. Optionally mute the 'Segment Dancer' section to save some processing time.\n- Sometimes the segmented subject will not be perfect, then check the mask quality using the preview box in the bottom right seen above. If that is the case you can play around with the prompt in the 'Florence2Run' node to target different body parts such as 'head', 'chest', 'legs', etc. and see if you get a better result.\n\n## Prompt\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1126/readme02.webp\" alt=\"\" width=\"300\"/>\n\n- Set the positive prompt using batch formatting:\n    - e.g. '0': '4k, masterpiece, 1girl standing on the beach, absurdres', '25': 'HDR, sunset scene, 1girl with black hair and a white jacket, absurdres', …\n- Negative prompt is normal format, add embeddings if desired.\n\n## Audio Processing\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1126/readme03.webp\" alt=\"\" width=\"750\"/>\n\n- This section takes in audio from the input video, extracts the stems (bass, drums, vocals, etc.) and then converts it to a normalized amplitude synced with the input video frames.\n- amp_control = total range the amplitude can travel.\n- amp_offset = the minimum value the amplitude can take.\n    - Example: amp_control = 0.8 and amp_offset = 0.2 means the signal will travel between 0.2 and 1.0.\n- Sometimes the Drums stem contains the actual Bass notes from the song; preview each to determine which is best for your masks.\n- Use the graphs to gain a clear understanding of how the signal for that stem changes throughout the video's duration.\n\n## Dilate Masks\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1126/readme04.webp\" alt=\"\" width=\"750\"/>\n\n- Each colored group corresponds to the color of dilation mask that will be generated by it.\n- Set the min and max radius for the dilation mask, along with its shape, using the following node:\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1126/readme05.webp\" alt=\"\" width=\"400\"/>\n\n- shape: 'circle' is the most accurate but takes longer to generate. Set this when you are ready to perform the final rendering. 'square' is fast to compute but less accurate, best for testing out the workflow and deciding on IP adapter images.\n- max_radius: The mask radius in pixels when amplitude value is max (1.0).\n- min_radius: The mask radius in pixels when amplitude value is min (0.0).\n- If you already have a composite mask video generated you can un-mute the 'Override Composite Mask' group and upload it. It’s recommended to bypass the dilation mask groups if overriding to save on processing time.\n\n## Models\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1126/readme06.webp\" alt=\"\" width=\"300\"/>\n\n- Use a good LCM model for the checkpoint. I recommend ParadigmLCM by Machine Delusions.\n- Merge multiple models together using the Model Merge Stack to get various interesting effects. Make sure the weights add up to 1.0 for the enabled models.\n- Optionally specify the AnimateLCM_sd15_t2v_lora.safetensors with a low weight of 0.18 to further enhance the final result.\n- Add any additional Loras to the model using the Lora stacker below the model loader.\n\n## AnimateDiff\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1126/readme07.webp\" alt=\"\" width=\"300\"/>\n\n- Set a different Motion Lora instead of the one I used (LiquidAF-0-1.safetensors)\n- Increase/decrease the Scale and Effect floats to increase/decrease the amount of motion in the output.\n\n## IP Adapters\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1126/readme08.webp\" alt=\"\" width=\"750\"/>\n\n- Here you can specify the reference images that will be used to render the backgrounds for each of the dilation masks, as well as your video subject(s).\n- The color of each group represents the mask it targets:\n\n#### Red, Green, Blue:\n\n- Subject mask reference images.\n\n#### Black:\n\n- Background mask image, upload a reference image for the background.\n\n#### White, Yellow, Magenta, Cyan:\n\n- Dilation mask reference images, upload a reference image for each color dilation mask in use.\n\n## ControlNet\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1126/readme09.webp\" alt=\"\" width=\"750\"/>\n\n- This workflow makes use of 5 different controlnets, including AD, Lineart, QR Code, Depth, and OpenPose.\n- All of the inputs to the controlnets are generated automatically\n- You can choose to override the input video for the Lineart, Depth, and Openpose controlnets if desired by un-muting the 'Override ' groups as seen below:\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1126/readme10.webp\" alt=\"\" width=\"750\"/>\n\n- It is recommended you also mute the 'Generate' groups if overriding to save processing time.\n\nTip:\n\n- Bypass the Ksampler and commence a render with your full input video. Once all the preprocessor videos are generated save them and upload them to the respective overrides. From now on when testing the workflow you will not have to wait for each preprocessor video to be generated individually.\n\n## Sampler\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1126/readme11.webp\" alt=\"\" width=\"600\"/>\n\n- By default the HiRes Fix sampler group will be muted to save processing time when testing\n- I recommend bypassing the Sampler group as well when trying to experiment with dilation mask settings to save time.\n- On final renders you can un-mute the HiRes Fix group which will upscale and add details to the final result.\n\n## Output\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1126/readme12.webp\" alt=\"\" width=\"450\"/>\n\n- There are two output groups: the left is for standard sampler output, and the right is for the HiRes Fix sampler output.\n\n## About Author\n\nAkatz AI:\n\n- Website: [https://akatz.ai](https://akatz.ai/)\n- http://patreon.com/Akatz\n- https://civitai.com/user/akatz\n- https://www.youtube.com/@akatz_ai\n- https://www.instagram.com/akatz.ai/\n- https://www.tiktok.com/@akatz_ai\n- https://x.com/akatz_ai\n- https://github.com/akatz-ai\n\nContacts:\n\n- Email: **akatz.hello@gmail.com**\n"
    },
    {
        "id": "1128",
        "readme": "This Unsampling guide, written by Inner-Reflections, greatly contributes to exploring Unsampling method for achieving dramatically consistent video style transfer.\n\n## 1. Introduction: Latent Noise Control with Unsampling\n\nLatent Noise is the basis for all of what we do with Stable Diffusion. It is amazing to take a step back and think of what we are able to accomplish with this. However generally speaking we are forced to use a random number to generate the noise. What if we could control it?\n\nI am not the first to use Unsampling. It has been around for a very long time and has been used in several different ways. Until now however I generally have not been satisfied with the results. I have spent several months finding the best settings and I hope you enjoy this guide.\n\nBy using the sampling process with AnimateDiff/Hotshot we can find noise that represents our original video and therefore makes any sort of style transfer easier. It is especially helpful to keep Hotshot consistent given its 8 frame context window.\n\nThis unsampling process essentially converts our input video into latent noise that maintains the motion and composition of the original. We can then use this representational noise as the starting point for the diffusion process rather than random noise. This allows the AI to apply the target style while keeping things temporally consistent.\n\nThis guide assumes you have installed AnimateDiff and/or Hotshot. If you haven't already, the guides are available here:\n\nAnimateDiff: https://civitai.com/articles/2379\n\nHotshot XL guide: https://civitai.com/articles/2601/\n\nLink to resource - If you want to post videos on Civitai using this workflow. https://civitai.com/models/544534\n\n## 2. System Requirements for this Workflow\n\nA Windows computer with an NVIDIA graphics card that has at least 12GB of VRAM is recommended. On the RunComfy platform, use a Medium (16GB of VRAM) or higher-tier machine. This process doesn't require more VRAM than standard AnimateDiff or Hotshot workflows, but it does take nearly twice as long, as it essentially runs the diffusion process twice—once for upsampling and once for resampling with the target style.\n\n## 3. Nodes Explanations and Settings Guide\n\n### Node: Custom Sampler\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1128/readme01.webp\" alt=\"Unsampling: Custom Sampler\" width=\"450\"/>\n\nThe main part of this is using the Custom Sampler which splits all the settings you usually see in the regular KSampler into pieces:\n\nThis is the main KSampler node - for unsampling adding noise/seed do not have any effect (that I am aware of). CFG matters - generally speaking the higher the CFG is on this step the closer the video will look to your original. Higher CFG forces the unsampler to more closely match the input.\n\n### Node: KSampler Select\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1128/readme02.webp\" alt=\"Unsampling: KSampler Select\" width=\"450\"/>\n\n**The most important thing is to use a sampler that converges! This is why we are using euler over euler a as the latter results in more randomness/instability.** Ancestral samplers that add noise at each step prevent the unsampling from cleanly converging. If you want to read more about this I have always found [this article](https://stable-diffusion-art.com/samplers/) useful. @spacepxl on reddit suggests that DPM++ 2M Karras is perhaps the more accurate sampler depending on use case.\n\n### Node: Align Your Step Scheduler\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1128/readme03.webp\" alt=\"Unsampling: Align Your Step Scheduler\" width=\"450\"/>\n\nAny scheduler will work just fine here - Align Your Steps (AYS) however gets good results with 16 steps so I have opted to use that to reduce compute time. More steps will converge more fully but with diminishing returns.\n\n### Node: Flip Sigma\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1128/readme04.webp\" alt=\"Unsampling: Flip Sigma\" width=\"450\"/>\n\nFlip Sigma is the magic node that causes unsampling to occur! By flipping the sigma schedule, we reverse the diffusion process to go from a clean input image to representative noise.\n\n### Node: Prompt\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1128/readme05.webp\" alt=\"Unsampling: Prompt\" width=\"450\"/>\n\nPrompting matters quite a bit in this method for some reason. A good prompt can really improve coherence to the video especially the more you want to push the transformation. For this example I have fed the same conditioning to both the unsampler and the resampler. It seems to work well generally - nothing stops you however from putting blank conditioning in the unsampler - I find it helps improve the style transfer, perhaps with a bit of loss of consistency.\n\n### Node: Resampling\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1128/readme06.webp\" alt=\"Unsampling: Resampling\" width=\"450\"/>\n\n**For resampling it is important to have add noise turned off** (although having empty noise in the AnimateDiff sample settings has the same effect - I have done both for my workflow). If you add noise during resampling you will get an inconsistent, noisy result, at least with default settings. Otherwise I suggest starting with a fairly low CFG combined with weak ControlNet settings as that seems to give the most consistent results while still allowing the prompt to influence the style.\n\n### Other Settings\n\nThe remainder of my settings are personal preference. I have simplified this workflow as much as I think possible while still including the key components and settings.\n\n## 4. Workflow Information\n\nThe default workflow uses the SD1.5 model. However, you can switch to SDXL by simply changing the checkpoint, VAE, AnimateDiff model, ControlNet model, and step schedule model to SDXL.\n\n## 5. Important Notes/Issues\n\n- Flashing - If you look at the decoded and previewed latents created by unsampling in my workflows, you will notice some with obvious color abnormalities. The exact cause is unclear to me, and generally, they do not affect the final results. These abnormalities are especially apparent with SDXL. However, they can sometimes cause flashing in your video. The main cause seems to be related to the ControlNets - so reducing their strength can help. Changing the prompt or even slightly altering the scheduler can also make a difference. I still encounter this issue at times - if you have a solution, please let me know!\n    - DPM++ 2M can sometimes improve flashing.\n\n## 6. Where to Go From Here?\n\nThis feels like a whole new way to control video consistency, so there's a lot to explore. If you want my suggestions:\n\n- Try combining/masking noise from several source videos.\n- Add IPAdapter for consistent character transformation.\n\n## About Author\n\n**Inner-Reflections**\n\n- https://x.com/InnerRefle11312\n- https://civitai.com/user/Inner_Reflections_AI\n"
    },
    {
        "id": "1129",
        "readme": "The ComfyUI Consistent Character workflow is a powerful tool that allows you to create characters with remarkable consistency and realism. Whether you're developing a story, designing a virtual influencer, or creating a game character, this workflow can help you achieve your goals. In this tutorial, we will guide you through the steps of using the ComfyUI Consistent Character workflow effectively.\n\n## 1. Upload Input Image\n\nThe first step in using the ComfyUI Consistent Character workflow is to select the perfect input image. This image should embody the essence of your character and serve as the foundation for the entire process. Take your time to choose an image that aligns with your artistic vision, considering factors such as facial features and overall aesthetic.\n\n## 2. Configure IPAdapter+InstantID Settings\n\nThe IPAdapter+InstantID component plays a vital role in maintaining character consistency. It works by first extracting the facial features and other important details from the input image. It then intelligently applies these features to the generated character, taking into account factors such as facial structure, eye shape, nose shape, mouth shape, and overall proportions. The result is a consistent character that retains the essence of the original input.\n\n## 3. Set Up ControlNet\n\nControlNet is another essential component in the Consistent Character workflow. It enables you to control the pose, composition, and structure of your generated characters, allowing for more precise and intentional character creation.\n\nControlNet works by incorporating additional input data, such as depth maps, segmentation masks, or keypoint annotations, to guide the character generation process. By providing this extra information, ControlNet helps to ensure that the generated consistent character follows the desired pose, maintains the correct proportions, and preserves the overall composition of the scene.\n\n## Tip: Balancing InstantID and ControlNet\n\nTo achieve the best results, it's important to find a balance between the InstantID and ControlNet components. Experiment with different weights/strengths, start percentage, and end percentage for each component until you find the optimal combination that generates a consistent character while still allowing for pose control.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1129/readme01.webp\" alt=\"ComfyUI Consistent Character: InstantID\" width=\"550\"/>\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1129/readme02.webp\" alt=\"ComfyUI Consistent Character: ControlNet\" width=\"550\"/>\n\n## 4. Generate the Consistent Character\n\nOnce you have configured the IPAdapter+InstantID and ControlNet settings, it's time to generate your consistent character. The KSampler component takes the adapted model, positive and negative conditioning, and the latent representation as input to create the final consistent character output. This process brings your character to life based on the specified parameters.\n\n## 5. Upscale and Filter the Consistent Character\n\nTo enhance the visual quality of your generated consistent character, you can use the Upscale and Adjustments components.\n\nThe ImageScaleBy node allows you to increase the resolution and sharpen the details of your character, resulting in a clearer and more refined output.\n\nThe Filter Adjustments node provides various options to fine-tune the appearance of your character. Adjust settings such as brightness, contrast, saturation, gamma, hue, and sharpness to achieve the desired aesthetic and ambiance for your consistent character.\n\n\nBy following these steps and leveraging the power of the ComfyUI ComfyUI Consistent Character workflow, you can create consistent characters that are realistic and aligned with your creative vision. Whether you're a storyteller, influencer, or game developer, this ComfyUI workflow offers a gateway to bringing your consistent character ideas to life\n"
    },
    {
        "id": "1130",
        "readme": "## 1. What is the ComfyUI Advanced Live Portrait Workflow?\n\nThe **Advanced Live Portrait** workflow in ComfyUI is a powerful tool that allows you to create lifelike animations by modifying facial expressions directly in photos and videos. It takes facial animation to the next level by providing a set of nodes and parameters that enable you to fine-tune various aspects of the face, such as head movements, eye blinks, eyebrow movements, pupil positions, mouth shapes, and smiles.\n\nCompared to the earlier **Live Portrait** tool, which could animate static images by referring to a source video, **Advanced Live Portrait** offers more advanced features and greater control over the facial expressions. With **Advanced Live Portrait**, you can adjust individual parameters to precisely modify the facial expressions in your images and create unique animations.\n\n## 2. Benefits of Advanced Live Portrait:\n\n- Brings static photos to life by adding natural movements and expressions, making them more engaging and dynamic.\n- Offers a high level of control over the generated animations through customizable parameters, allowing you to fine-tune every aspect of the facial expressions.\n- Allows you to combine multiple motions and expressions to create complex and realistic animations.\n- Supports the use of reference videos to drive the facial expressions and movements in the animated output.\n\n## 3. How to Use the Advanced Live Portrait Workflow in ComfyUI\n\nAdvanced Live Portrait is a powerful tool within ComfyUI that allows you to create lifelike animations by modifying facial expressions in photos and videos. This tutorial will guide you through the process of using the Advanced Live Portrait workflow to achieve stunning results.\n\n### 3.1. Editing Facial Expressions in Photos with Advanced Live Portrait\n\nThe first step in the Advanced Live Portrait workflow is to use the **Expression Editor (PHM)** node to modify facial expressions in your photos. This node provides a wide range of parameters that allow you to fine-tune various aspects of the face, such as head movements, eye blinks, eyebrow movements, pupil positions, mouth shapes, and smiles.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1130/readme01.webp\" alt=\"Advanced Live Portrait\" width=\"350\"/>\n\n#### Expression Editor Parameters in Advanced Live Portrait:\n\n- **rotate_pitch**: Controls the up-and-down movement of the head.\n- **rotate_yaw**: Adjusts the side-to-side movement of the head.\n- **rotate_roll**: Determines the tilt angle of the head.\n- **blink**: Controls the intensity of eye blinks.\n- **eyebrow**: Adjusts eyebrow movements.\n- **wink**: Controls winking.\n- **pupil_x**: Moves the pupils horizontally.\n- **pupil_y**: Moves the pupils vertically.\n- **aaa**: Controls the mouth shape for the \"aaa\" vowel sound.\n- **eee**: Controls the mouth shape for the \"eee\" vowel sound.\n- **woo**: Controls the mouth shape for the \"woo\" vowel sound.\n- **smile**: Adjusts the degree of a smile.\n- **src_ratio**: Determines the ratio of the source expression to be applied.\n- **sample_ratio**: Determines the ratio of the sample expression to be applied.\n- **sample_parts**: Specifies which parts of the sample expression to apply (\"OnlyExpression\", \"OnlyRotation\", \"OnlyMouth\", \"OnlyEyes\", \"All\").\n- **crop_factor**: Controls the cropping factor of the face region.\n\nBy adjusting these parameters in the Advanced Live Portrait workflow, you can precisely control the facial expressions in your photos and create the desired look.\n\n### 3.2. Adding Multiple Motions in Advanced Live Portrait\n\nAdvanced Live Portrait allows you to create complex animations by combining multiple motions. To add more motions, simply duplicate the **Expression Editor (PHM)** node and adjust the parameters accordingly. Each additional **Expression Editor (PHM)** node represents a new motion (e.g., motion 2, motion 3, etc.).\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1130/readme02.webp\" alt=\"Advanced Live Portrait\" width=\"750\"/>\n\nThe sequence of the **Expression Editor (PHM)** nodes determines the motion index used in the subsequent **Advanced Live Portrait** nodes. In the provided workflow, there are 4 **Expression Editor (PHM)** nodes, representing motion1, motion2, motion3, and motion4. You can easily add or remove nodes to create more advanced facial expressions and movements in the Advanced Live Portrait workflow.\n\n### 3.3. Combining Motions with the Advanced Live Portrait Node\n\nThe **Advanced Live Portrait** node is the core component of the Advanced Live Portrait workflow. It combines multiple motions to generate a video. The node allows you to specify the sequence and duration of each motion using a specific format in the \"command\" input:\n\n- The first number represents the motion index (e.g., 0 for the original source motion, 1 for motion 1, etc.).\n- The second number represents the changing frame length (e.g., 5 means it takes 5 frames to transition to the next motion).\n- The third number represents the length of frames waiting for the next motion (e.g., 3 means it stays on the current motion for 3 frames before transitioning).\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1130/readme03.webp\" alt=\"Advanced Live Portrait\" width=\"750\"/>\n\n\nFor example, \"1 = 5:3\" means motion 1 takes 5 frames to transition from the previous image and stays on motion 1 for 3 frames before transitioning to the next image.\n\n### 3.4. Using a Source Video in Advanced Live Portrait (Optional)\n\nAdvanced Live Portrait also allows you to use a source video as a reference for facial expressions and movements. \n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1130/readme04.webp\" alt=\"Advanced Live Portrait\" width=\"500\"/>\n\nHere's how you can incorporate a source video into the Advanced Live Portrait workflow:\n\n1. Connect the **Load Video** node to the \"driving_images\" input of the **Advanced Live Portrait** node.\n2. Set the \"tracking_src_vid\" parameter to True in the **Advanced Live Portrait** node to enable tracking of the source video.\n3. The **Advanced Live Portrait** node will process the source video frame by frame and apply the corresponding motions based on the specified sequence and duration in the \"command\" input.\n4. The facial expressions and movements from the source video will be used as a starting point for each frame.\n5. The motions defined by the **Expression Editor (PHM)** nodes will be applied on top of the source video's expressions and movements.\n6. The \"retargeting_eyes\" and \"retargeting_mouth\" parameters in the **Advanced Live Portrait** node control the influence of the source video's eye and mouth movements on the output video.\n7. The resulting facial expressions and movements in the output video will be a combination of the source video and the applied motions.\n\nIf you prefer to generate an animated video without using a source video, set the \"animate_without_vid\" parameter to True in the **Advanced Live Portrait** node.\n\n### **More Information**\n\nFor additional details on the Advanced Live Portrait workflow, please visit [PowerHouseMan](https://github.com/PowerHouseMan)/[ComfyUI-Advanced Live Portrait](https://github.com/PowerHouseMan/ComfyUI-AdvancedLivePortrait)\n"
    },
    {
        "id": "1131",
        "readme": "Have you ever wanted to create stunning, Houdini-like animations but felt intimidated by the complexity of 3D software? Look no further! This innovative ComfyUI workflow will guide you through the process of applying depth (Z-Depth) manipulations and generating captivating animations using only 2D images.\n\nWhether you're an artist, designer, or enthusiast, you'll be able to generate stunning animations that look and feel like they were created using advanced 3D techniques, all while working with the simplicity and ease of 2D images in ComfyUI.\n\n### Step 1: Creating a Starting Image\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1131/readme01.webp\" alt=\"ComfyUI Z-Depth Maps Workflow\" width=\"350\"/>\n\nTo begin, you'll need to generate a starting image that will serve as the basis for your animation. Once you have your image, scale it down to a smaller size, like 1 megapixel, to keep the file size manageable.\n\n### Step 2: Creating a Mask Animation from a Single Image\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1131/readme02.webp\" alt=\"ComfyUI Z-Depth Maps Workflow\" width=\"750\"/>\n\nIn this step, you'll transform your single image into a mask animation. Begin by creating a video from your single image by repeating it for a desired number of frames, such as 32. To isolate your subject, remove the background using a background removal model like InSPyR-Net. After removing the background, slightly contract the resulting mask by a few pixels to ensure it works well with the depth manipulation process.\n\nNext, extract the depth information from your single frame and connect it to the control net depth. Repeat the depth image for the same number of frames as your video. Pass the repeated depth frames to the Time Feature and Flex Mask Depth Chamber nodes. The Time Feature node will guide the animation through time with a fixed frame rate. Set the effect time to \"smooth\" and choose a low speed value, like 0.1.\n\nThe time features will control the slicing of your subject along the Z-depth axis. Adjust the starting position of the Z-front and Z-back planes to begin the action mid-way through your subject. Move both planes using the \"feature\" parameter with a \"squeeze\" motion to create the desired slicing effect. To enhance the differential diffusion effect and ensure smoother transitions, add a small amount of blur using the \"grow with blur\" setting.\n\nBy the end of this step, you will have converted your single image into a mask animation that is ready for the depth manipulation and slicing process. This mask animation will serve as the foundation for creating the Houdini-like effect in the subsequent steps of the workflow.\n\n### Step 3: Generating the Animation\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1131/readme03.webp\" alt=\"ComfyUI Z-Depth Maps Workflow\" width=\"750\"/>\n\nIn this step, you'll bring your animation to life by generating frames using the prepared mask animation and depth information. Begin by loading a checkpoint, such as the 1.5 checkpoint for AnimateDiff, along with any desired LoRAs to enhance the quality and style of your generated frames.\n\nSet up a simple conditioning step with a positive prompt that describes the portion of the resulting mask that will be sliced. This prompt will guide the generation process and help maintain consistency throughout the animation. Apply a ControlNet depth based on your original subject's depth to ensure the generated frames maintain the desired depth characteristics.\n\nIf you have larger areas that need in-painting, you can optionally enable the IP Adapter group to handle those regions more effectively. Perform the inference with Differential Diffusion and set the latent noise mask. Encode the 32 frames of your original image and manipulate the latent space with noise for each frame, but only in the corresponding white area that has been sliced.\n\nTo further enhance the smoothness and fluidity of your animation, pass the resulting frames through a RIFE (Real-Time Intermediate Flow Estimation) node for frame interpolation. This process will double the number of frames, effectively making your video longer and smoother. By interpolating between the generated frames, RIFE helps to create a more seamless and visually appealing animation.\n\nBy the end of this step, you will have generated a complete animation based on your original image, depth information, and mask animation. \n\n### Step 4: Blending Different Animations (Optional)\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1131/readme04.webp\" alt=\"ComfyUI Z-Depth Maps Workflow\" width=\"750\"/>\n\nIf you want to take your animation to the next level, you can generate two different animations, such as fire and ice, using the same process as before but with different prompts and reconditioning the depth. Divide the image interval into five sections. Take the first 25% of one animation and the remaining 25% (75% to 100%) of the other animation as the starting and ending points. Blend the middle points (25% to 40%, 40% to 60%, 60% to 75%) of both animations so that the resulting blend starts more like the first animation and gradually becomes more like the second animation.\n"
    },
    {
        "id": "1132",
        "readme": "This ComfyUI motion graphics workflow empowers you to create stunning motion graphics animation effects by transforming pre-existing video inputs into captivating animations with 3D effects and smooth transitions. \n\n## Step 1: Input Selection and Preprocessing for Motion Graphics\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1132/readme01.webp\" alt=\"ComfyUI Motion Graphics Workflow\" width=\"300\"/>\n\n1.1. Begin by selecting your input video using the LoadVideoInput node. Adjust the \"Skip First Frames\" and \"Select Every Nth Frame\" parameters to control which frames from the video will be used in the motion graphics animation. This is useful for reducing the number of frames to process, especially if your input video has a high frame rate or if you're working with limited hardware resources.\n\n1.2. Scale the selected frames to your desired resolution using the ImageScaleToMegapixels node. A resolution of around 1 megapixel (e.g., 1280x720) is generally sufficient for most motion graphics animations, but you can adjust this based on your specific needs and hardware capabilities.\n\n## Step 2: Subject Segmentation and Mask Generation for Motion Graphics\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1132/readme02.webp\" alt=\"ComfyUI Motion Graphics Workflow\" width=\"750\"/>\n\n2.1. Load the GroundingDINO and SAM models using their respective nodes. These models will be used to segment the main subject from the input video frames, which is crucial for creating compelling motion graphics.\n\n2.2. Use the GroundingDinoSAMSegment node to extract the subject based on a text prompt. This node takes the preprocessed frames from step 1 and generates a segmentation mask for each frame, isolating the main subject you want to animate in your motion graphics project.\n\n2.3. To ensure a smooth and seamless motion graphics animation, slightly grow the segmentation mask using the GrowMaskWithBlur node. This will help to include any edges or details that may have been missed during the initial segmentation.\n\n2.4. Convert the grown mask into an image format using the MaskToImage node. This step prepares the mask for use in the subsequent motion graphics animation steps.\n\n## Step 3: Feature Manipulation and Animation Control for Motion Graphics\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1132/readme03.webp\" alt=\"ComfyUI Motion Graphics Workflow\" width=\"300\"/>\n\n3.1. Create a time-based animation feature using the TimeFeatureNode. This node extracts features from the input frames and generates a control signal that can be used to manipulate various parameters throughout the motion graphics animation, such as z-depth and mask position.\n\n3.2. Visualize and adjust the animation curve using the FeatureScaler node. This node allows you to fine-tune the behavior of the time feature, such as setting the effect type (e.g., smooth, accelerate) and scale type (e.g., linear, logarithmic, exponential). By tweaking these settings, you can control how the z-depth plane moves through the motion graphics animation and achieve the desired timing and pacing.\n\n## Step 4: Z-Depth Masking and 3D Effect for Motion Graphics\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1132/readme04.webp\" alt=\"ComfyUI Motion Graphics Workflow\" width=\"500\"/>\n\n4.1. Generate a depth-based inpainting mask using the FlexMaskDepthChamber node. This node takes the segmented subject mask from step 2, the time feature from step 3, and a depth map generated from the input frames. It then creates an animated z-depth mask that will be used to add a stunning 3D effect to your motion graphics video.\n\n4.2. Adjust the starting position of the z-depth plane using the \"Z Front\" and \"Z Back\" parameters in the FlexMaskDepthChamber node. These settings control where the plane begins and ends in relation to the subject's depth, allowing you to create visually striking motion graphics with a sense of depth and dimensionality.\n\n## Step 5: Motion Graphics Video Generation and Frame Interpolation\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1132/readme05.webp\" alt=\"ComfyUI Motion Graphics Workflow\" width=\"750\"/>\n\n5.1. This section covers the core components of the generative process. Load your checkpoint model and apply additional LoRAs. Set your prompts and apply ControlNet if desired. Use the AnimateDiff model and associated nodes for video generation. \n\n5.2. To further enhance the smoothness and fluidity of your generated motion graphics video, interpolate between the frames using the RIFE VFI node. This node uses advanced algorithms to create transitional frames between the generated ones, effectively doubling the frame rate and reducing any jitter or abrupt movements in your motion graphics animation.\n\n## Step 6:  Multi-Generation Blending for Advanced Motion Graphics (Optional)\n\nFor even more impressive motion graphics results, you can combine the best elements from multiple generation passes using the following steps:\n\n6.1. Run the pipeline multiple times with different prompts to create a variety of styles and visual elements for your motion graphics project. Each pass will generate a unique animation based on the given prompts, allowing you to explore different creative directions for your motion graphics.\n\n6.2. Use the ImageIntervalSelectPercentage node to select specific frames from each generation pass based on their position in the timeline. For example, you might choose frames from the beginning of one pass, the middle of another, and the end of a third to create a dynamic and visually engaging motion graphics animation.\n\n6.3. Blend the selected frames from each pass using the ImageBlend node. This will create smooth transitions between the different styles, allowing you to combine the best aspects of each generation into a cohesive motion graphics piece.\n\n6.4. Use the ImageBatchMulti node to combine the blended frames from each transition stage into a single cohesive sequence. This node takes the output from multiple ImageBlend nodes and concatenates them in the specified order, resulting in a seamless motion graphics animation.\n\n6.5. Finally, interpolate the combined frames once more using the RIFE VFI node to increase the frame rate and overall smoothness of your final motion graphics animation.\n"
    },
    {
        "id": "1133",
        "readme": "This workflow, created by [**Inner-Reflections**](https://x.com/InnerRefle11312), explores Unsampling as a method for consistent style transfer in ComfyUI.  For a comprehensive guide on using Unsampling, refer to: [Consistent Style Transfer with Unsampling](https://www.runcomfy.com/comfyui-workflows/consistent-style-transfer-with-unsampling-in-stable-diffusion).\n\nBy utilizing this workflow, you can effortlessly convert your video into a **parchment style** illustration. The process is simple, requiring minimal effort to transform your video seamlessly, resulting in stunning **parchment style** outputs.\n\nThe **Unsampling** process in **ComfyUI** transforms the input video into latent noise that retains the original motion and composition. Instead of relying on random noise, this representational noise serves as the starting point for the diffusion process. This allows you to apply the target **parchment style** while maintaining temporal consistency in the output video.\n\nKey Components of the **Parchment Style Unsampling Workflow**:\n\n**Inputs**: Upload your input video here. The `frame_load_cap` setting limits the number of frames loaded from the input video. The `select_every_nth` setting skips frames to reduce the total number of frames processed. Increasing the `select_every_nth` value speeds up processing but may impact the smoothness of the output.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1133/readme01.webp\" alt=\"ComfyUI Parchment Style Workflow\" width=\"350\" />\n\n**Unsample**: The **Unsample** component uses `euler` sampling to locate noise that represents the original video. Increasing the `cfg` scales forces the unsampler to better match the input. The `Flip Sigma` node reverses the diffusion process, converting the input frames into representative latent noise, critical for the style transfer to **parchment**.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1133/readme02.webp\" alt=\"ComfyUI Parchment Style Workflow\" width=\"750\" />\n\n**Prompt**: A well-crafted prompt enhances coherence, especially when intensifying the style transfer. For **parchment style**, a positive prompt could be \"ink drawing minimalist illustration on parchment\". You can also include more detailed instructions to enhance the texture and intricacy, making the parchment style even more refined and expressive.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1133/readme03.webp\" alt=\"ComfyUI Parchment Style Workflow\" width=\"350\" />\n\n**ControlNet**: The ControlNet component utilizes Depth ControlNet, preprocessed by `DepthAnythingV2`, to help maintain the structure and geometry of the original video during resampling. The `strength` setting controls the influence of ControlNet on the output, with higher values adhering more closely to the depth map.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1133/readme04.webp\" alt=\"ComfyUI Parchment Style Workflow\" width=\"600\" />\n\n**Resample**: The **Resample** component processes the unsampled latent noise by reapplying the diffusion process, this time incorporating the **parchment style** to enhance the texture and overall visual effect.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1133/readme05.webp\" alt=\"ComfyUI Parchment Style Workflow\" width=\"600\" />\n\n**Outputs**: The decoded frames are saved as images and combined into an upscaled MP4 video at the original frame rate.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1133/readme06.webp\" alt=\"ComfyUI Parchment Style Workflow\" width=\"350\" />\n\nThe **parchment style** is just one example of the many creative possibilities available. Feel free to experiment with this **Unsampling** workflow to achieve your desired aesthetic!\n"
    },
    {
        "id": "1134",
        "readme": "This workflow, created by [**Inner-Reflections**](https://x.com/InnerRefle11312), explores Unsampling as a method for consistent style transfer in ComfyUI. For a detailed guide on how to use Unsampling, refer to: [Consistent Style Transfer with Unsampling](https://www.runcomfy.com/comfyui-workflows/consistent-style-transfer-with-unsampling-in-stable-diffusion).\n\nBy using this workflow, you can transform your video into a **clay style** animation. The process is simple, requiring minimal effort to achieve a seamless and striking **claymation** look, reminiscent of traditional stop-motion clay animations.\n\nThe **Unsampling** process in **ComfyUI** converts the input video into latent noise that preserves the original motion and structure. Unlike random noise, this representational noise serves as the foundation for the diffusion process, enabling the AI to apply the **clay style** while maintaining temporal consistency across the video frames.\n\nKey Components of the **Clay Style Unsampling Workflow**:\n\n**Inputs**: This is where you upload your input video. The `frame_load_cap` setting limits the number of frames loaded from the input. The `select_every_nth` setting skips frames to reduce the overall workload. Increasing the `select_every_nth` value speeds up the process but may affect the smoothness of the final **clay style** animation.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1134/readme01.webp\" alt=\"ComfyUI Clay Style Workflow\" width=\"350\" />\n\n**Unsample**: The **Unsample** component utilizes `euler` sampling to find noise that accurately represents the input video. By adjusting the `cfg` scales, you can fine-tune the Unsampling process to match the input more closely. The `Flip Sigma` node reverses the diffusion process, converting the input frames into latent noise.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1134/readme02.webp\" alt=\"ComfyUI Clay Style Workflow\" width=\"750\" />\n\n**Prompt**: A well-constructed prompt can significantly improve the final result, especially when aiming for a distinctive **clay style**. For this style, a good positive prompt might be \"(claymation:1.15), stop motion, (everything made of clay:1.1), beautiful woman dancing.\" This prompt guides the model to recreate the animation in a way that mimics real clay movements and textures.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1134/readme03.webp\" alt=\"ComfyUI Clay Style Workflow\" width=\"350\" />\n\n**ControlNet**: The ControlNet component uses Depth ControlNet, preprocessed by `DepthAnythingV2`, to maintain the depth and geometry of the original video. This ensures that the resampling process adheres to the structure of the input video. The `strength` setting controls how much influence ControlNet has on the output, with higher values leading to more precise structure retention.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1134/readme04.webp\" alt=\"ComfyUI Clay Style Workflow\" width=\"600\" />\n\n**Resample**: The **Resample** component takes the unsampled latent noise and runs the diffusion process again, applying the **clay style** to generate a smooth and consistent **claymation** effect.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1134/readme05.webp\" alt=\"ComfyUI Clay Style Workflow\" width=\"600\" />\n\n**Outputs**: The final output frames are decoded, saved as individual images, and then combined into an upscaled MP4 video, preserving the original frame rate while showcasing the new **clay style** effect.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1134/readme06.webp\" alt=\"ComfyUI Clay Style Workflow\" width=\"350\" />\n\nThe **clay style** is just one of many exciting possibilities you can achieve with this Unsampling workflow in **ComfyUI**. Experiment with different prompts and settings to create your unique style, and enjoy the process of transforming your videos into dynamic and visually stunning animations!\n"
    },
    {
        "id": "1135",
        "readme": "IDM-VTON, short for \"Improving Diffusion Models for Authentic Virtual Try-on in the Wild,\" is an innovative diffusion model that allows you to realistically try on garments virtually using just a few inputs. What sets IDM-VTON apart is its ability to preserve the unique details and identity of the garments while generating virtual try-on results that look incredibly authentic.\n\n## 1. Understanding IDM-VTON\n\nAt its core, IDM-VTON is a diffusion model that's been specifically engineered for virtual try-on. To use it, you simply need a representation of a person and a garment you want to try on. IDM-VTON then works its magic, rendering a result that looks like the person is actually wearing the garment. It achieves a level of garment fidelity and authenticity that surpasses previous diffusion-based virtual try-on methods.\n\n## 2. The Inner Workings of IDM-VTON\n\nSo, how does IDM-VTON pull off such realistic virtual try-on? The secret lies in its two main modules that work together to encode the semantics of the garment input:\n1. The first is an image prompt adapter, or IP-Adapter for short. This clever component extracts the high-level semantics of the garment - essentially, the key characteristics that define its appearance. It then fuses this information into the cross-attention layer of the main UNet diffusion model.\n2. The second module is a parallel UNet called GarmentNet. Its job is to encode the low-level features of the garment - the nitty-gritty details that make it unique. These features are then fused into the self-attention layer of the main UNet.\n\nBut that's not all! IDM-VTON also makes use of detailed textual prompts for both the garment and the person inputs. These prompts provide additional context that enhances the authenticity of the final virtual try-on result.\n\n## 3. Putting IDM-VTON to Work in ComfyUI\n\n### 3.1 The Star of the Show: The IDM-VTON Node\n\nIn ComfyUI, the \"IDM-VTON\" node is the powerhouse that runs the IDM-VTON diffusion model and generates the virtual try-on output.\n\nFor the IDM-VTON node to work its magic, it needs a few key inputs:\n\n1. Pipeline: This is the loaded IDM-VTON diffusion pipeline that powers the whole virtual try-on process.\n2. Human Input: An image of the person who will be virtually trying on the garment.\n3. Pose Input: A preprocessed DensePose representation of the human input, which helps IDM-VTON understand the person's pose and body shape.\n4. Mask Input: A binary mask that indicates which parts of the human input are clothing. This mask needs to be converted into an appropriate format.\n5. Garment Input: An image of the garment to be virtually tried on.\n\n### 3.2 Getting Everything Ready\n\nTo get the IDM-VTON node up and running, there are a few preparation steps:\n\n1. Loading the Human Image: A LoadImage node is used to load the image of the person.\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1135/readme01.webp\" alt=\"IDM-VTON\" width=\"500\" />\n2. Generating the Pose Image: The human image is passed through a DensePosePreprocessor node, which computes the DensePose representation that IDM-VTON needs.\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1135/readme02.webp\" alt=\"IDM-VTON\" width=\"500\" />\n3. Obtaining the Mask Image: There are two ways to get the clothing mask:\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1135/readme03.webp\" alt=\"IDM-VTON\" width=\"500\" />\n\na. Manual Masking (Recommended)\n\n- Right-click on the loaded human image and choose \"Open in Mask Editor.\"\n- In the mask editor UI, manually mask the clothing regions.\n    \nb. Automatic Masking\n    \n- Use a GroundingDinoSAMSegment node to automatically segment the clothing.\n- Prompt the node with a text description of the garment (like \"t-shirt\").\n    \nWhichever method you choose, the obtained mask needs to be converted to an image using a MaskToImage node, which is then connected to the \"Mask Image\" input of the IDM-VTON node.\n    \n4. Loading the Garment Image: It is used to load the image of the garment.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1135/readme04.webp\" alt=\"IDM-VTON\" width=\"500\" />\n\nFor a deeper dive into the IDM-VTON model, don't miss the original paper, \"[Improving Diffusion Models for Authentic Virtual Try-on in the Wild](https://idm-vton.github.io/)\". And if you're interested in using IDM-VTON in ComfyUI, be sure to check out the dedicated nodes [here](https://github.com/TemryL/ComfyUI-IDM-VTON). Huge thanks to the researchers and developers behind these incredible resources.\n"
    },
    {
        "id": "1136",
        "readme": "This \"Flux Consistent Characters Workflow Series 1 (Input Test)\" was created by [Mickmumpitz on his YouTube channel](https://www.youtube.com/watch?v=MbQv8zoNEfY). We highly recommend checking out his detailed tutorial to learn how to use this powerful Consistent Characters workflow effectively. While we've reproduced the Consistent Characters workflow and set up the environment for your convenience, all credit goes to Mickmumpitz for his excellent work on developing this Flux-based Consistent Characters solution.\n\nIf you want to create consistent characters using an existing image, please use the [\"Flux Consistent Characters Workflow Series 2 (Input Image)\"](https://www.runcomfy.com/comfyui-workflows/flux-consistent-characters-input-image) by Mickmumpitz.\n\n\n## The Flux Consistent Characters (Input Test)\n\nThe Flux Consistent Characters (Text Input) Workflow is a tool that helps you create AI characters that look the same based on your text descriptions. This workflow makes it easy to get characters that maintain their appearance from different viewpoints. By using the Flux.1 dev model, the generated characters are more reliable. This workflow is perfect for creating AI movies, children's books, or any other project where you want the characters to have a consistent look and feel based on your text input.\n\n## How to Use the Flux Consistent Characters (Input Test) Workflow?\n\nThis Flux Consistent Characters Workflow is divided into four modules (Characters Generation, Upsacle + Face Fix, Poses, Emotions), each designed to streamline the process of generating Consistent Characters with a uniform appearance across multiple outputs.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1136/readme01.webp\" alt=\"Consistent Characters with Flux\" width=\"750\"/>\n\n### Fast Groups Muter (rgthree) Node\n\nAfter loading the Fast Groups Muter (rgthree) node, modules 2, 3, and 4 are executed automatically, and no additional setup is required. This node controls the switches for all four modules, making the Consistent Characters process smoother and more efficient.\n\nFast Groups Muter (rgthree) Node Control module switch (yes/no).\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1136/readme02.webp\" alt=\"Consistent Characters with Flux\" width=\"500\"/>\n\n\n### 1. Characters Generation\n\nThis module utilizes the Flux models and Flux ControlNet model to generate a Consistent Characters table by guiding the generation process with prompts. The input, which is a pose sheet, serves as a reference to direct the Consistent Characters generation. By crafting appropriate prompts, you can steer the model to create desired Consistent Characters sheets.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1136/readme03.webp\" alt=\"Consistent Characters with Flux\" width=\"750\"/>\n\nHere are some prompt examples:\n\nA character sheet featuring an American woman wearing a coat, dressed in autumn fashion, with a neutral expression. The sheet should have a white background, multiple views from various angles, and a visible face portrait. The overall style should resemble a masterpiece photography.\n\nA character sheet depicting an elven ranger wearing a cloak made of autumn leaves, dressed in forest colors, with a determined expression. The sheet should have a parchment background, multiple views from different angles, and a visible face portrait. The ranger should be accompanied by a majestic stag, carrying a longbow and quiver on her back. The overall style should resemble a masterpiece digital painting of a female elf with long golden hair.\n\nTip: If the generated Consistent Characters sheet does not meet your expectations, try adjusting the seed value to regenerate the output with variations.\n\n\n### 2. Upsacle + Face Fix\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1136/readme04.webp\" alt=\"Consistent Characters with Flux\" width=\"400\"/>\n\n#### 2.1 Ultimate SD Upscale\nUltimate SD Upscale is a node used in image generation pipelines to enhance image resolution by dividing the image into smaller tiles, processing each tile individually, and then stitching them back together. This process allows for generating high-resolution images while managing memory usage and reducing artifacts that can occur when upscaling.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1136/readme05.webp\" alt=\"Consistent Characters with Flux\" width=\"300\"/>\n\n**Parameters**:\n- `upscale_by`: The factor by which the image's width and height are multiplied. For precise dimensions, use the \"No Upscale\" version.\n- `seed`: Controls randomness in the generation process. Using the same seed yields the same results.\n- `control_after_generate`: Adjusts the image details post-generation.\n- `steps`: The number of iterations during image generation. More steps result in finer details but longer processing time.\n- `cfg`: Classifier-Free Guidance scale that adjusts how strictly the model follows the input prompts.\n- `sampler_name`: Specifies the sampling method used for image generation.\n- `scheduler`: Defines how computation resources are allocated throughout the generation.\n- `denoise`: Controls the level of noise removal, influencing the detail retention from the original image. Recommended: 0.35 for enhancement, 0.15-0.20 for minimal changes.\n- `mode_type`: Determines the mode of processing, such as how tiles are processed.\n- `tile_width` and `tile_height`: Dimensions of the tiles used in processing. Larger sizes reduce seams but require more memory.\n- `mask_blur`: Blurs the edges of the masks used for tile blending, smoothing transitions between tiles.\n- `tile_padding`: Number of pixels from neighboring tiles considered during processing to reduce seams.\n- `seam_fix_mode`: Method for correcting visible seams between tiles:\n  - Bands pass: Fixes seams along rows and columns.\n  - Half tile offset pass: Applies an offset to better blend seams.\n  - Half tile offset + intersections pass: Includes additional passes at intersections.\n- `seam_fix_denoise`: Strength of noise reduction during seam fixing.\n- `seam_fix_width`: Width of the areas processed during seam fixing.\n- `seam_fix_mask_blur`: Blurs the mask for smoother seam corrections.\n- `seam_fix_padding`: Padding around seams during correction to ensure smoother results.\n- `force_uniform_tiles`: Ensures tiles maintain a uniform size by extending edge tiles when needed, minimizing artifacts.\n- `tiled_decode`: Processes image tiles individually to reduce memory usage during high-resolution generation.\n- **Target size type**: Determines how the final image size is set:\n  - From img2img settings: Uses default width and height.\n  - Custom size: Allows manual width and height setting (max 8192px).\n  - Scale from image size: Scales based on the initial image size.\n- **Upscaler**: The method for upscaling images before further processing (e.g., ESRGAN).\n- **Redraw**: Controls how the image is redrawn:\n  - Linear: Processes tiles sequentially.\n  - Chess: Uses a checkerboard pattern for processing to reduce artifacts.\n  - None: Disables redraw, focusing only on seam fixing.\n\n#### 2.2. FaceDetailer (pipe)\n\n**FaceDetailerPipe** is a node designed to enhance facial details in images, using advanced image processing techniques to improve the sharpness and clarity of facial features. It is part of the **ComfyUI Impact Pack**, aiming to provide high-quality facial detail enhancements for various applications.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1136/readme06.webp\" alt=\"Consistent Characters with Flux\" width=\"300\"/>\n\n**Parameters**:\n- `image`: The input image to be enhanced, serving as the main subject for facial detailing.\n- `guide_size`: A parameter that controls the size of the guidance area used for enhancing facial features, influencing how much context is considered.\n- `guide_size_for`: Determines whether the guide size should be applied for specific regions.\n- `max_size`: Sets the maximum size limit for processed images, ensuring memory management.\n- `seed`: Controls randomness in the image enhancement process, allowing reproducible results when using the same seed.\n- `steps`: The number of iterations for enhancing details. More steps result in finer details but require more processing time.\n- `cfg`: The Classifier-Free Guidance scale, which adjusts how closely the model follows the input guidance.\n- `sampler_name`: Defines the sampling method used for detail refinement.\n- `scheduler`: Determines the computational scheduling strategy during processing.\n- `denoise`: Controls the strength of noise reduction applied during the enhancement process. Lower values retain more original details, while higher values produce smoother results.\n- `feather`: Controls the smoothness of the transition between enhanced and original areas, helping to blend the changes seamlessly.\n- `noise_mask`: Enables or disables the use of a noise mask to target specific areas for noise reduction.\n- `force_inpaint`: Forces inpainting in regions that need additional enhancement or corrections.\n- `bbox_threshold`: Sets the threshold for detecting bounding boxes around facial features, influencing sensitivity.\n- `bbox_dilation`: Expands the detected bounding box areas to ensure that all relevant features are included during enhancement.\n- `bbox_crop_factor`: Adjusts the cropping factor for detected bounding boxes, controlling the area of focus for enhancement.\n- `sam_detection_hint`: Specifies additional hints or guides for the detection process.\n- `sam_dilation`: Adjusts the dilation applied to the detected regions, allowing for broader coverage.\n- `sam_threshold`: Defines the threshold for detection sensitivity within the SAM (Segment Anything Model) process.\n- `sam_bbox_expansion`: Expands the bounding boxes detected by the SAM, helping to include more surrounding context.\n- `sam_mask_hint_threshold`: Adjusts the threshold for the mask hints provided by SAM, controlling how regions are defined for masking.\n- `sam_mask_hint_use_negative`: Determines if negative hints should be used, influencing the masking of certain regions.\n- `drop_size`: Sets the size of the drops applied during the enhancement process, which can influence the level of refinement.\n- `refiner_ratio`: Controls the ratio for refining facial details, balancing between preserving original features and adding clarity.\n- `cycle`: Specifies the number of refinement cycles to apply, affecting the depth of enhancement.\n- **inpaint_model (Optional)**: Enables the use of an inpainting model for filling in missing or unclear areas during the detailing process.\n- `noise_mask_feather`: Adjusts the feathering of the noise mask, providing a smoother transition between noisy and denoised areas.\n\n### 3. Poses\n\nThis module allows you to use the image crop node to separate each pose from the generated character sheet and save individual poses of the character for further use or adjustments.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1136/readme07.webp\" alt=\"Consistent Characters with Flux\" width=\"750\"/>\n\n### 4. Emotions\n\nThis module utilizes the **Photo Expression Editor (PHM)** node to adjust facial expressions in photos. The parameters allow for fine-tuning various facial aspects, such as head movements, blinking, and smiling\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1136/readme08.webp\" alt=\"Consistent Characters with Flux\" width=\"750\"/>\n\nExpression Editor Parameters：\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1136/readme09.webp\" alt=\"Consistent Characters with Flux\" width=\"300\"/>\n\n- `rotate_pitch`: Controls the up-and-down movement of the head.\n- `rotate_yaw`: Adjusts the side-to-side movement of the head.\n- `rotate_roll`: Determines the tilt angle of the head.\n- `blink`: Controls the intensity of eye blinks.\n- `eyebrow`: Adjusts eyebrow movements.\n- `wink`: Controls winking.\n- `pupil_x`: Moves the pupils horizontally.\n- `pupil_y`: Moves the pupils vertically.\n- `aaa`: Controls the mouth shape for the \"aaa\" vowel sound.\n- `eee`: Controls the mouth shape for the \"eee\" vowel sound.\n- `woo`: Controls the mouth shape for the \"woo\" vowel sound.\n- `smile`: Adjusts the degree of a smile.\n- `src_ratio`: Determines the ratio of the source expression to be applied.\n- `sample_ratio`: Determines the ratio of the sample expression to be applied.\n- `sample_parts`: Specifies which parts of the sample expression to apply (\"OnlyExpression\", \"OnlyRotation\", \"OnlyMouth\", \"OnlyEyes\", \"All\").\n- `crop_factor`: Controls the cropping factor of the face region.\n\nFlux models and the workflow's streamlined modules make it a breeze for you to maintain Consistent Characters appearances across various outputs. You'll be able to bring your characters to life like never before, creating a truly immersive experience for your audience!\n"
    },
    {
        "id": "1137",
        "readme": "Stability AI has unveiled [Stable Diffusion 3.5 (SD3.5)](https://stability.ai/news/introducing-stable-diffusion-3-5), an open-source multimodal generative AI model that includes several variants such as Stable Diffusion 3.5 (SD3.5) Large, Stable Diffusion 3.5 (SD3.5) Large Turbo, and Stable Diffusion 3.5 (SD3.5) Medium. These models are highly customizable, capable of running on consumer hardware. The SD3.5 Large and Large Turbo models are immediately available, while the Medium version will be released on October 29, 2024. \n\n## 1. How Stable Diffusion 3.5 (SD3.5) Works\n\nAt a technical level, Stable Diffusion 3.5 (SD3.5) takes a text prompt as input, encodes it into a latent space using transformer-based text encoders, and then decodes that latent representation into an output image using a diffusion-based decoder. The transformer text encoders, such as the CLIP (Contrastive Language-Image Pre-training) model, map the input prompt into a semantically meaningful compressed representation in the latent space. This latent code is then iteratively denoised by the diffusion decoder over multiple timesteps to generate the final image output. The diffusion process involves gradually removing noise from a initially noisy latent representation, conditioned on the text embedding, until a clean image emerges.\n\nThe different model sizes in Stable Diffusion 3.5 (SD3.5) (Large, Medium) refer to the number of trainable parameters - 8 billion for the Large model and 2.5 billion for Medium. More parameters generally allow the model to capture more knowledge and nuance from its training data. The Turbo models are distilled versions that sacrifice some quality for much faster inference speeds. Distillation involves training a smaller \"student\" model to mimic the outputs of a larger \"teacher\" model, aiming to retain most of the capability in a more efficient architecture.\n\n## 2. Strengths of the Stable Diffusion 3.5 (SD3.5) Models\n\n### 2.1. Customizability\n\nThe Stable Diffusion 3.5 (SD3.5) models are designed to be easily fine-tuned and built upon for specific applications. Query-Key Normalization was integrated into the transformer blocks to stabilize training and simplify further development. This technique normalizes the attention scores in the transformer layers, which can make the model more robust and easier to adapt to new datasets via transfer learning.\n\n### 2.2. Diversity of Outputs\n\nStable Diffusion 3.5 (SD3.5) aims to generate images representative of the world's diversity without the need for extensive prompting. It can depict people with varying skin tones, features, and aesthetics. This is likely due to the model being trained on a large and diverse dataset of images from across the internet.\n\n### 2.3. Broad Range of Styles\n\nThe Stable Diffusion 3.5 (SD3.5) models are capable of generating images in a wide variety of styles including 3D renders, photorealism, paintings, line art, anime, and more. This versatility makes them suitable for many use cases. The style diversity emerges from the diffusion model's ability to capture many different visual patterns and aesthetics in its latent space.\n\n### 2.4. Strong Prompt Adherence\n\nEspecially for the Stable Diffusion 3.5 (SD3.5) Large model, SD3.5 does well at generating images that align with the semantic meaning of the input text prompts. It ranks highly compared to other models on prompt matching metrics. This ability to accurately translate text into images is powered by the transformer text encoder's language understanding capabilities.\n\n## 3. Limitations and Drawbacks of the Stable Diffusion 3.5 (SD3.5) Models\n\n### 3.1. Struggles with Anatomy and Object Interactions\n\nLike most text-to-image models, Stable Diffusion 3.5 (SD3.5) still has difficulty rendering realistic human anatomy, especially hands, feet, and faces in complex poses. Interactions between objects and hands are often distorted. This is likely due to the challenge of learning all the nuances of 3D spatial relationships and physics from 2D images alone.\n\n### 3.2. Limited Resolution\n\nThe Stable Diffusion 3.5 (SD3.5) Large model is ideal for 1 megapixel images (1024x1024), while the Medium tops out around 2 megapixels. Generating coherent images at higher resolutions is challenging for SD3.5. This limitation stems from the computational and memory constraints of the diffusion architecture.\n\n### 3.3. Occasional Glitches and Hallucinations\n\nDue to the Stable Diffusion 3.5 (SD3.5) models allowing for broad diversity of outputs from the same prompt with different random seeds, there can be some unpredictability. Prompts lacking specificity may lead to glitchy or unexpected elements appearing. This is an inherent property of the diffusion sampling process, which involves randomness.\n\n### 3.4. Falls Short of Absolute Cutting-Edge\n\nAccording to some early tests, in terms of image quality and coherence, Stable Diffusion 3.5 (SD3.5) does not currently match the performance of state-of-the-art text-to-image models like Midjourney. And early comparisons between Stable Diffusion 3.5 (SD3.5) and FLUX.1 reveal that each model excels in different areas. While FLUX.1 seems to have an advantage in producing photorealistic images, SD3.5 Large has greater proficiency in generating anime-style artwork without requiring additional fine-tuning or modifications. \n\n## 4. Stable Diffusion 3.5 in ComfyUI\n\n**At RunComfy, we've made it easy for you to start using the Stable Diffusion 3.5 (SD3.5) models by preloading them for your convenience. You can jump right in and run inferences using the example workflow**\n\nThe example workflow begins with the CheckpointLoaderSimple node, which loads the pre-trained Stable Diffusion 3.5 Large model. And to help translate your text prompts into a format the model can understand, the TripleCLIPLoader node is used to load the corresponding encoders. These encoders are crucial in guiding the image generation process based on the text you provide.\n\nThe EmptySD3LatentImage node then creates a blank canvas with the specified dimensions, typically 1024x1024 pixels, which serves as the starting point for the model to generate the image. The CLIPTextEncode nodes process the text prompts you provide, using the loaded encoders to create a set of instructions for the model to follow.\n\nBefore these instructions are sent to the model, they undergo further refinement through the ConditioningCombine, ConditioningZeroOut, and ConditioningSetTimestepRange nodes. These nodes remove the influence of any negative prompts, specify when the prompts should be applied during the generation process, and combine the instructions into a single, cohesive set.\n\nFinally, you can fine-tune the image generation process using the ModelSamplingSD3 node, which allows you to adjust various settings such as the sampling mode, number of steps, and model output scale. Finally, the KSampler node gives you control over the number of steps, the strength of the instructions' influence (CFG scale), and the specific algorithm used for generation, enabling you to achieve the desired results.\n"
    },
    {
        "id": "1138",
        "readme": "## Stable Diffusion 3.5 VS FLUX.1\n\nGet ready to witness the awe-inspiring capabilities of two cutting-edge models: Stable Diffusion 3.5 (SD3.5) and FLUX.1? With this ComfyUI workflow, you can now input a text prompt and generate images with these two models simultaneously, allowing you to compare the results and choose the one you like best.\n\nWe have tested some cases using SD3.5-large and Flux.1-schnell, and the results reveal that Stable Diffusion 3.5 (SD3.5) and FLUX.1 each excel in different areas. While FLUX.1 has an advantage in producing photorealistic images, SD3.5 demonstrates greater proficiency in generating anime-style artwork without requiring additional fine-tuning or modifications.\n\nDon't just take our word for it – experience the magic for yourself!\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1138/readme01.webp\" alt=\"Stable Diffusion 3.5 VS FLUX.1\" width=\"800\"/>\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1138/readme02.webp\" alt=\"Stable Diffusion 3.5 VS FLUX.1\" width=\"800\"/>\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1138/readme03.webp\" alt=\"Stable Diffusion 3.5 VS FLUX.1\" width=\"800\"/>\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1138/readme04.webp\" alt=\"Stable Diffusion 3.5 VS FLUX.1\" width=\"800\"/>\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1138/readme05.webp\" alt=\"Stable Diffusion 3.5 VS FLUX.1\" width=\"800\"/>\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1138/readme06.webp\" alt=\"Stable Diffusion 3.5 VS FLUX.1\" width=\"800\"/>\n"
    },
    {
        "id": "1139",
        "readme": "The ComfyUI FLUX Controlnet Inpainting Workflow combines FLUX.1-dev and ControlNet to improve Image Inpainting. It uses ControlNet to guide FLUX.1-dev in generating accurate, intention-aligned repairs, maintaining consistency with the original image style.\n\n## FLUX Controlnet Inpainting Key Features\n\n1. Masked Area Repair: Repairs specific regions using input masks, blending seamlessly with the original image.\n2. ControlNet Integration: Utilizes edge maps, sketches, and depth maps for enhanced inpainting.\n3. High-Quality Output: Inherits FLUX.1-dev’s ability to create realistic and natural images.\n\nThis workflow provides advanced AI processing to improve the repair experience.\n\n## FLUX Controlnet Inpainting  vs. SDXL Inpainting\n\n1. Precise Control: FLUX Controlnet Inpainting offers better control over inpainting through ControlNet, allowing more tailored results.\n2. Superior Realism: FLUX.1-dev provides more natural, high-quality outputs.\n3. Advanced Style Adaptation: Adapts to user descriptions, offering versatile style adjustments.\n\nIn Alpha testing, FLUX Controlnet Inpainting has shown its potential to outperform models like SDXL Inpainting, offering a more intuitive and high-quality solution.\n\n## How to use ComfyUI FLUX Controlnet Inpainting Workflow\n\nTo effectively use the ComfyUI FLUX Controlnet Inpainting workflow, follow these steps:\n\n### 1. Configure the Model\n\n- Diffusion Model: Load the FLUX.1-dev diffusion model, which is the foundation for high-quality image generation.\n- Clip Model: Integrate the clip model for better text-to-image correspondence.\n- VAE Model: Ensure the required VAE model for FLUX is preloaded.\n\n### 2. Load and Mask the Image for Inpainting\n\n- In the \"Load image” node, upload the target image you wish to modify.\n- Right-click the image and choose \"Open in Masked Editor”. Use this tool to mask the areas you want to modify or repair, then click \"Save to node”.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1139/readme01.webp\" alt=\"FLUX Controlnet Inpainting\" width=\"750\"/>\n\n### 3. Set Up CLIPTextEncode Node\n\n- Enter your desired text prompt in the node's properties to guide the FLUX Inpainting process and achieve your desired outcome.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1139/readme02.webp\" alt=\"FLUX Controlnet Inpainting\" width=\"550\"/>\n\n### 4. Use the ControlNetInpaintingAliMamaApply Node\n\n- This node utilizes ControlNet technology to repair and inpaint images, blending existing visuals with input guidance (like masks or prompts) for seamless results.\n- Parameters:\n    - Strength: Controls the intensity of the inpainting effect. Higher values make the changes more pronounced, while lower values yield subtler adjustments.\n    - Start Percent: Specifies when to begin applying the inpainting effect during the process, expressed as a percentage (e.g., 0% means it starts immediately).\n    - End Percent: Determines when to stop applying the effect, with 100% meaning the effect is applied throughout the entire process.\n\n### 5. Generate the Image\n\n- Click Queue Prompt to start the image generation. The process will use the FLUX Controlnet Inpainting to produce the final image based on your input settings and masked areas.\n\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1140",
        "readme": "The **FLUX Outpainting** workflow is perfect for seamlessly expanding images beyond their original boundaries. The process begins by using SDXL for the initial outpainting, which establishes a solid base (since we found that starting with FLUX alone didn’t provide optimal results). After this, **FLUX** is applied to refine and repaint the expanded areas, enhancing details and ensuring the final output is more realistic and polished. The power of **FLUX** is evident in how it elevates the quality of the outpainted sections, blending them flawlessly with the existing content. This comprehensive tutorial will walk you through each step of the workflow, from preparing the image to upscaling the final **FLUX** output.\n\nThis FLUX Outpainting workflow was created by My AI Force, and we encourage you to visit [My AI Force’s YouTube channel](https://www.youtube.com/watch?v=iMxP6sS_nw4) for more in-depth insights into FLUX Outpainting. All credit goes to him for his contribution to the community.\n\n## 1. Control Panel Setup\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1140/readme01.webp\" alt=\"FLUX Outpainting\" width=\"750\"/>\n\n### Initial Preparation\n\nStart by loading your source image with the `LoadImage` node. For the best results, choose high-resolution images (between 512x512 and 2048x2048 pixels). High-resolution images provide enough detail for the outpainting process, ensuring the final result remains sharp and clear.\n\n### ImageBlendAdvance Node Configuration\n\n1. **Empty Latent Size Picker**:\n    - **Resolution**: Choosing the right resolution is crucial for the quality and proportions of the outpainting.\n2. **Positioning Parameters**: Adjusting the position and size of the outpainting canvas helps control the space allocated for the expansion.\n    - **X Position**: Adjusts the horizontal alignment of the outpainting.\n    - **Y Position**: Controls vertical alignment.\n    - **Scale**: Changes the size of the outpainting within the given background.\n\n## 2. SDXL Outpainting Configuration\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1140/readme02.webp\" alt=\"FLUX Outpainting\" width=\"750\"/>\n\nIn this phase, the **SDXL model** is applied for the initial outpainting, extending the content beyond its current boundaries. The SDXL model fills in the new areas, providing a smooth and coherent background that matches the established style and content. This initial fill is essential for creating the base for the outpainting.\n\nTo refine the outpainting and ensure the expanded areas align perfectly, **ControlNet** is used. ControlNet adds additional guidance to the outpainting model, ensuring that the new sections are generated according to specific rules or patterns that maintain the consistency of the overall design. It fine-tunes how the SDXL outpainting handles new areas, resulting in smoother transitions and a more cohesive final composition.\n\n## 3. Detail Restoration Process\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1140/readme03.webp\" alt=\"FLUX Outpainting\" width=\"750\"/>\n\nThe **Detail Restoration Process** ensures that the features within the outpainting that may have been altered are preserved. This step ensures that the new outpainted areas integrate naturally with the extended composition\n\n- **Overlaying the Original Image**: Use the **ImageBlend** node to place the original image over the outpainted version. This brings back the finer details that the outpainting process may have blurred or changed. While this step helps maintain original features, it can sometimes create visible seams or mismatched edges between the two images.\n- **Seam Elimination Using a Mask**: To fix any visible seams, create a mask that traces the outline of the original image. This mask allows for selective blending, ensuring that the overlay is applied only where needed. Using the **Image Blend by Mask** node, the two images are merged smoothly, reducing any noticeable edges and achieving a seamless transition.\n\n## 4. FLUX Repainting Process\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1140/readme04.webp\" alt=\"FLUX Outpainting\" width=\"750\"/>\n\nThe **FLUX Repainting Process** is where the outpainting is refined to improve detail and realism. This crucial stage re-renders the expanded sections, blending them smoothly with the rest of the composition. FLUX provides fine control over textures and details, enhancing the outpainted areas for a more unified, polished result. This process ensures that the outpainted sections are enhanced with intricate textures, making the entire work appear cohesive and natural.\n\n## 5. FLUX Upscaling Outpainting Image\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1140/readme05.webp\" alt=\"FLUX Outpainting\" width=\"750\"/>\n\nThe **Upscaling Process** is the final step, where the resolution is increased to enhance the overall clarity. This stage sharpens edges and refines details, ensuring the expanded areas of the outpainting blend seamlessly. By upscaling, the outpainting becomes more defined, ensuring that the final result is both high-quality and polished. This step is essential for producing high-resolution outpainting that looks professional and seamlessly integrated with the rest of the composition.\n"
    },
    {
        "id": "1141",
        "readme": "The [ComfyUI-MochiWrapper nodes](https://github.com/kijai/ComfyUI-MochiWrapper) and its associated workflow are fully developed by Kijai. We give all due credit to Kijai for this innovative work. On the RunComfy platform, we are simply presenting Kijai’s contributions to the community. It is important to note that there is currently no formal connection or partnership between RunComfy and Kijai. We deeply appreciate Kijai’s work!\n\n## Mochi 1\n\nMochi 1 is an advanced text-to-video model from [Genmo](https://www.genmo.ai/) that brings creative ideas to life by transforming textual descriptions into vivid, visually engaging videos. Built with intuitive AI capabilities, Mochi 1 can interpret a range of prompts, creating dynamic animations, realistic scenes, or artistic visualizations that align with user intent. Whether it’s for storytelling, advertising, educational content, or entertainment, Mochi 1 provides flexibility by supporting diverse video styles, from 2D animations to cinematic renders. Mochi 1 is designed to empower creators and make video production more accessible.\n\n\n## How to Use ComfyUI Mochi 1 Workflow?\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1141/readme01.webp\" alt=\"Mochi 1\" width=\"750\"/>\n\n\n---\n### 1.1 Mochi 1 Sampler\n\nThis is the Main Node of the workflow, in which you define most of the settings like number of frames, resolutions, steps, cfg ...etc for video generation with Mochi 1.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1141/readme02.webp\" alt=\"Mochi 1 Wrapper\" width=\"600\"/>\n\n\n**Parameters**:\n- `Width`: The width of the video generated by Mochi 1. \n- `Height`: The height of the video generated by Mochi 1.\n- `num_frames`: Controls the number frames generated in a Mochi 1 video.\n- `steps`: The number of iterations for enhancing details in Mochi 1 output. More steps result in finer details but require more processing time.\n- `cfg`: The Classifier-Free Guidance scale, which adjusts how closely Mochi 1 follows the input guidance.\n- `seed`: Controls randomness in the Mochi 1 generation process. Using the same seed yields the same results.\n- `control_after_generate`: Adjusts the image details post-generation in Mochi 1.\n\n\n### 1.2 Positive and Negative Prompts. \n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1141/readme03.webp\" alt=\"Mochi 1\" width=\"750\"/>\n\n- `Positive Prompt`: Describes what you want Mochi 1 to generate, focusing on the desired elements. For example, “a blonde woman smiling in natural sunlight” or “a serene park with blooming flowers.”\n- `Negative Prompt`: Describes what you do not want Mochi 1 to generate in the video.\n- `Strength`: It determines the strength of the prompts and how closely Mochi 1 will follow the prompt.\n\n\n### 1.3 Model AutoDownloader \n\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1141/readme04.webp\" alt=\"Mochi 1\" width=\"600\"/>\n\n- This is the auto downloader and loader for the Mochi 1 models.\n- It will take 3-5 mins to download the Mochi 1 Model 19.1 GB Model when running for the first time.\n\n\n### 1.4 Mochi 1 Decode\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1141/readme05.webp\" alt=\"Mochi 1\" width=\"600\"/>\n\n\n**Parameters**:\n- `enable_vae_tiling`: Enables/Disables the Tile Decoding for low vram Consumption.\n- `auto_tile_size`: Automatically determines the Tile Decoding Size.\n- `frames_batch_size`: Determines the number of frames to decode in one go. Use Lower values to consume less GPU but will take more time. \n- `tile_sample_mine_height`: Tile min height for decoding Mochi 1 outputs. \n- `tile_sample_mine_width`: Tile min width for decoding Mochi 1 outputs.\n- `tile_overlap_factor_height`: Tile min overlapping height for decoding Mochi 1.\n- `tile_overlap_factor_width`: Tile min overlapping width for decoding Mochi 1.\n\n\n### 1.5 Video Combine\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1141/readme06.webp\" alt=\"Mochi 1\" width=\"500\"/>\n\n**Parameters**:\n\n- `frame_rate`: Sets the frame rate for the Mochi 1 output. In this case, it is set to 12, meaning the video will display 12 frames per second (fps).\n- `loop_count`: Defines how many times the video will loop. A value of 0 typically means infinite looping, though this may depend on the specific software implementation.\n- `filename_prefix`: This adds a prefix to the filename of the Mochi 1 output file. In the image, it's set to 1141, so the output file would start with that number.\n- `format`: Specifies the Mochi 1 output video format. Here, it's set to video/h264-mp4, which means the video will be encoded in H.264 codec and saved as an MP4 file.\n- `pix_fmt`: Defines the pixel format for the video. The value yuv420p indicates that it will use the YUV color space with 4:2:0 chroma subsampling, which is commonly used due to its balance of quality and compression.\n- `crf`: Stands for Constant Rate Factor, which controls the quality and file size of the video in relation to the bitrate. A value of 19 provides a good balance between video quality and file size, where lower values give better quality and larger file size, while higher values reduce quality and file size.\n- `save_metadata` : When enabled (true), this saves metadata (such as settings and properties) into the output file.\n- `pingpong`: This setting, when enabled, causes the video to play forward and then in reverse, creating a \"ping-pong\" effect. It is set to false here, so this effect will not be applied.\n- `save_output`: When set to true, the final output from Mochi 1 will be saved to disk.\n\n---\n\nDesigned for accessibility, Mochi 1 allows users to customize details like color schemes, camera angles, motion dynamics, and more, making it adaptable for both novice and experienced creators. Mochi 1's underlying algorithms are trained on a vast dataset of visuals, enabling it to understand context, emotion, and visual storytelling nuances. Mochi 1 integrates seamlessly with popular editing platforms, enabling users to polish generated videos or add custom effects.\n\nBy combining efficiency and creativity, Mochi 1 opens new possibilities for content creators, saving time while boosting creativity and enabling rapid prototyping. This makes Mochi 1 ideal for creating engaging YouTube videos, social media content, or interactive stories, empowering users to express ideas in compelling visual formats without needing in-depth video editing expertise.\n"
    },
    {
        "id": "1142",
        "readme": "The [ComfyUI-CogVideoXWrapper ](https://github.com/kijai/ComfyUI-CogVideoXWrapper)nodes and its associated workflow are fully developed by Kijai. We give all due credit to Kijai for this innovative work. On the RunComfy platform, we are simply presenting Kijai’s contributions to the community. It is important to note that there is currently no formal connection or partnership between RunComfy and Kijai. We deeply appreciate Kijai’s work!\n\n## CogVideoX Tora \n\nTora introduces a novel framework for generating high-quality videos by leveraging trajectory-based guidance in a diffusion transformer model. By focusing on motion trajectories, Tora achieves more realistic and temporally coherent video synthesis. This approach bridges the gap between spatial-temporal modeling and generative diffusion frameworks.\n\nPlease note that this version of Tora is based on the CogVideoX-5B model and is intended for academic research purposes only. For licensing details, please refer [here](https://github.com/alibaba/Tora/blob/main/LICENSE).\n\n\n## 1.1 How to Use CogVideoX Tora Workflow?\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1142/readme01.webp\" alt=\"Tora\" width=\"750\"/>\n</p>\n\nThis is the CogVideoX Tora workflow, Left Side nodes are inputs, Middle are processing tora nodes, and right are the outputs node.\n- Drag and drop your horizontal image in the input node.\n- Write your action prompts\n- Make a trajectory path\n\n---\n\n\n\n### 1.2 Load Input Image\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1142/readme02.webp\" alt=\"Tora\" width=\"750\"/>\n</p>\n\n- Upload, Drag and drop or Copy and Paste (Ctrl+V) your image in the load image node\n\n> [!CAUTION] \n> Only Horizontal Format Images of Dimensions - 720*480 will work. Other Dimensions will give error.\n\n\n\n### 1.3 Add Your Positive and Negative Prompts\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1142/readme03.webp\" alt=\"Tora\" width=\"750\"/>\n</p>\n\n- `Positive`: Enter the actions taking place with the Subject based of Trajectory defined in the trajectory node (moving, flowing....etc). \n- `Negative`: Enter what you don't want to happen (Distorted hands, blurry...etc) \n\n\n\n### 1.4 Make Trajectory for motion\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1142/readme04.webp\" alt=\"Tora\" width=\"750\"/>\n</p>\n\nHere you set the Trajectory path of the motion of the subject in the uploaded photo. \n- `points_to_sample`: This Set the Number of frames for Rendering, or Duration of your video in frames. \n- `mask_width`: Default is 720. DO NOT CHANGE!\n- `mask_height `: Default is 480. DO NOT CHANGE!\n  \nNode Guide: \n- Shift + click to add control point at end. Ctrl + click to add control point (subdivide) between two points.\n- Right click on a point to delete it.\n- Note that you can't delete from start/end.\n- Right click on canvas for context menu:\n- These are purely visual options, doesn't affect the output:\n\nToggle handles visibility\n- Display sample points: display the points to be returned.\n- points_to_sample value sets the number of samples\n- returned from the drawn spline itself, this is independent from the\n- actual control points, so the interpolation type matters.\n\nSampling_method:\n- time: samples along the time axis, used for schedules\n- path: samples along the path itself, useful for coordinates\n\n\n\n\n### 1.5 Load CogVideoX & Tora Models\n\n<p align=\"center\">\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1142/readme05.webp\" alt=\"Tora\" width=\"750\"/>\n</p>\n\nThese are the model downloader nodes, it will automatically download models in your comfyui in 2-3 mins. \n\n\n\n\n### 1.6 CogVideo Sampler\n\n<p align=\"center\">\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1142/readme06.webp\" alt=\"Tora\" width=\"750\"/>\n</p>\n\n- `Steps`: This value decides the quality of your render. Keep in between 25 - 35 for best and effcient value.\n- `cfg`: Default value is 6.0 for CogVideo Sampling.\n- `denoising strength` and `Scheduler`: Do not change this.  \n\n\n\n\n### 1.7 Trajectory Weights and Strength\n\n<p align=\"center\">\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1142/readme07.webp\" alt=\"Tora\" width=\"750\"/>\n</p>\n\nThis node will set the strength of your motion trajectory. \n\n- `strength`: High value will give distorted figure or flying dot. Use between 0.5 - 0.9.\n- `start_percent`: Use this value to ease in the effect of strength motion.\n- `end_percent`: - High value will give distorted figure or flying dot. Use between 0.3 - 0.7\n\n\n\n\n### 1.8 Outputs\n\n<p align=\"center\">\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1142/readme08.webp\" alt=\"Tora\" width=\"750\"/>\n</p>\n\nThese nodes will give you 3 outputs.\n1) Output Rendered Video\n2) Trajectory path overlayed on rendered video\n3) Trajectory Video on black background\n\n--- \n\n\n\"CogVideoX Tora: Trajectory-oriented Diffusion Transformer for Video Generation\" presents an innovative approach to video generation by introducing trajectory-based guidance within a diffusion transformer framework. Unlike traditional video synthesis models that struggle to maintain temporal consistency and realistic motion, CogVideoX Tora explicitly focuses on modeling motion trajectories. This enables the system to generate coherent and visually convincing videos by understanding how objects and elements evolve over time. By combining the power of diffusion models, known for high-quality image generation, with the temporal reasoning capabilities of transformers, CogVideoX Tora bridges the gap between spatial and temporal modeling.\n\nCogVideoX Tora's trajectory-oriented mechanism provides fine-grained control over object movements and dynamic interactions, making it particularly suitable for applications requiring precise motion guidance, such as video editing, animation, and special effects generation. The model's ability to maintain temporal consistency and realistic transitions enhances its applicability in creating smooth and coherent video content. By integrating trajectory priors, CogVideoX Tora not only improves motion dynamics but also reduces artifacts often seen in frame-based generation. This breakthrough sets a new benchmark for video synthesis, offering a powerful tool for creators and developers in fields like filmmaking, virtual reality, and video-based AI.\n\n\n\n"
    },
    {
        "id": "1143",
        "readme": "The [Ultimate SD node](https://github.com/ssitu/ComfyUI_UltimateSDUpscale) is developed fully developed by ssitu. And the Workflow is made by [Jerry Davos](https://www.patreon.com/c/jerrydavos) using the RunComfy platform who is also our part time workflow developer. We are simply presenting their work to the community. It is important to note that there is currently no formal connection or partnership between RunComfy and ssitu. We deeply appreciate ssitu’s work!\n\n## Flux Ultimate 32k Upscaler\n\nThe *Flux Ultimate 32k Upscaler Workflow* is a powerful upscaler solution that enhances visuals to stunning resolutions of 4k, 8k, 16k, and up to an impressive 32k. Powered by the Ultimate SD Upscaler node, this Flux upscaler ensures that intricate details and sharpness are preserved across all output sizes. Perfect for professionals requiring flexibility, the Flux Upscaler adapts to various needs—whether for detailed 4k displays or ultra-high-definition 32k applications. With its seamless scaling capabilities, *Flux Upscaler* offers pristine, artifact-free results, making it ideal for any project demanding exceptional clarity and precision at any scale.\n\n\n## How to Use Flux Ultimate 32k Upscaler Workflow?\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1143/readme08.webp\" alt=\"Flux Upscaler\" width=\"750\"/>\n</p>\n\n---\n\n\n### 1.1 Load the Visual Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1143/readme01.webp\" alt=\"Flux Upscaler\" width=\"750\"/>\n</p>\n\n- Upload, Drag and drop or Copy and Paste (Ctrl+V) your image in the load image node\n- `Limit Image Size to MegaPixels:`  is default to 1.5 MP, This servers as the starting point for upscaling resolution. which will be upscaled 2x,4x,8x,16x,32x times. \n\n\n\n### 1.2 Models and Loras Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1143/readme02.webp\" alt=\"Flux Upscaler\" width=\"750\"/>\n</p>\n\n- `Load Checkpoint:` In this node you select the checkpoint which best suits your upload image's rendering style i.e. realistic, anime, toon...etc which will increase the accuracy of details \n- `Optional Lora Stack:` Using style loras like cloths, figures, style, character, artstyle or abstract loras of your image's element will further increase the accuracy of the rendered image.  \n- `Load VAE:` The default flux vae is fine to use. \n- `Dual Clip Loader:` The default clip models are loaded and need not to be changed.\n- Make a copy of lora stack and connect it in chain if need more than 3 loras. \n\n\n\n### 1.3 ControlNet Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1143/readme03.webp\" alt=\"Flux Upscaler\" width=\"750\"/>\n</p>\n\nIn this controlnet model [Flux.1-dev-Controlnet-Upscaler ](https://huggingface.co/jasperai/Flux.1-dev-Controlnet-Upscaler/tree/main) is used which act as a traditional tile controlnet. \n\n- `Strength:` The Strength for the Controlnet. Use value below 0.7 as higher value will give rough artifacts on the edges.\n- `Start Percent:` This value decides from which percent the controlnet will take affect. Let it be 0. \n- `End Percent:` This value decides till when percent the controlnet apply it's affect. Use Value under 0.7 . \n\n\n\n\n### 1.4 Additional Prompt\n\n\n<p align=\"center\">\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1143/readme04.webp\" alt=\"Flux Upscaler\" width=\"750\"/>\n</p>\n\nIn this text box, it can remain empty, or you can add your lora trigger prompts, style prompts which will improve your renders.\n\n\n### 1.5 Refiner Sampler\n\n<p align=\"center\">\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1143/readme05.webp\" alt=\"Flux Upscaler\" width=\"750\"/>\n</p>\n\nThis Ksampler will give you a preview of how the upscaling is going to progress. \n\n- Use Deis, Euler and Dpm_2 is seen to give best output. Feel free to try other sampers and schedulers. \n- Use Low Denoising (0.2 - 0.4) if major details are getting lost.\n- According to your selected Model, Loras and Prompts used, the image may slightly change from Original.\n\n\n### 1.6 Ultimate SD Sampler\n\n<p align=\"center\">\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1143/readme06.webp\" alt=\"Flux Upscaler\" width=\"750\"/>\n</p>\n\nThe SD sampler is set to best settings, just only with Denoise value and sampler name only. \n\nFollowing are the Detailed parameters: \n- `denoise:` Uses from default img2img field. We recommend 0.35 value for image enhancements, but if you don't want changes, use 0.15-0.20.\n- `target size type:` Where to get the size of the final image.\n  - `from img2img settings:` Default img2img width and height sliders.\n  - `custom size:` Built-in width and height sliders. Max values - 8192.\n  - `scale from image size:` Init image size multiplied by scale factor.\n- `redraw:` \n  - `upscaler:` Upscale image before redrawing. Use what you like. Our recommendation - ESRGAN for photorealistic images, R-ESRGAN 4x+ for others (less requirements).\n  - `type:`\n    - `linear:` All tiles processed one by one, column by column, row by row.\n    - `chess:` All tiles are processed in a checkerboard pattern, reducing the chance of seam artifacts.\n    - `none:` Disabled redraw. Use it when running generation without seam fix, seeing visible overlays or artifacts on seams, and wanting to run just seam pass. Don't forget to put upscaled image as the source before.\n  - `tile width:` The width to be processed. The larger the tile, the fewer artifacts in the final image. For 2k, 512px is usually enough.\n  - `tile height:` The height to be processed. The default is 0, in which case it is equal to the width. Larger tiles reduce artifacts. For 2k, 512px is usually enough.\n  - `padding:` How many pixels of neighboring tiles will be taken into account during processing.\n  - `mask blur:` It is blur of masks used in tile masking. Set it to 12-16 on 512-768px size. Increase if you see seams.\n- `seams fix:` Do not use if the result image has no visible grid; it's just another redraw pass.\n  - `type:`\n    - `bands pass:` Adds passes on seams (rows and columns) and covers a small area around them (width in UI). It requires less time than offset pass.\n    - `half tile offset pass:` Adds two passes like redraw pass, but with a half-tile offset. One pass for rows with vertical gradient mask and one for columns with horizontal gradient mask. This pass covers a bigger area than bands and mostly produces better results, but requires more time.\n    - `half tile offset + intersections pass:` Runs Half tile offset pass, then runs extra pass on intersections with radial gradient mask.\n    - `none:` Disabled seams fix. Default value.\n  - `denoise`: Denoise strength for seams fix.\n  - `width:` Redraw line width. Used only by \"Band pass.\"\n  - `padding:` Pixels near the seam will be taken into account when processing a tile.\n  - `mask blur:` It is the blur of masks used in tile masking. Set to 8-16 on 32px padding. Increase if padding is increased.\n- `save options:` \n  - `upscaled:` Enabled by default. Saves image from Redraw.\n  - `seams fix:` Disabled by default. Saves image after Seams fix.\n- `upscale_by:` Number to multiply the width and height of the image by. For exact width and height, use the \"No Upscale\" version of the node and perform upscaling separately (e.g., ImageUpscaleWithModel -> ImageScale -> UltimateSDUpscaleNoUpscale).\n- `force_uniform_tiles:` If enabled, tiles cut off by the image edges will expand using the rest of the image to keep consistent tile size as determined by tile_width and tile_height (similar to A1111 Web UI). If disabled, minimal tile size will be used, which may speed up sampling but can cause artifacts due to irregular tile sizes.\n\n\n\n### 1.7 Upscaled Previews\n\n<p align=\"center\">\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1143/readme07.webp\" alt=\"Flux Upscaler\" width=\"750\"/>\n</p>\n\nYou can see your upscaled preview here. These nodes will give the comparison of original image vs the upscaled image \n\n\n--- \n\nFlux Upscaler’s sophisticated processing pipeline incorporates customizable settings that allow users to fine-tune each stage of the Flux upscaling process. With features like adjustable denoise strength, seam-fixing algorithms, and tile padding, Flux Upscaler ensures smooth transitions across image sections, even at extreme resolutions. Users can optimize performance with tiling options, such as “Linear” or “Checkerboard,” and utilize upscale presets for photorealistic (ESRGAN) and general-purpose (R-ESRGAN 4x+) outputs to suit various stylistic needs. These capabilities make Flux Upscaler an adaptable choice for artists and designers aiming for flawless upscaling in both realistic and artistic renderings.\n\nBeyond its technical prowess, Flux Upscaler is crafted for workflow flexibility. Users can save intermediate and final upscaled images, apply multiple redraw passes, and set custom tile sizes to balance quality and processing speed. Advanced scaling options in Flux Upscaler allow users to either multiply image dimensions based on the original size or define exact output sizes as needed. By preserving high-definition imagery’s integrity and aesthetics, Flux Upscaler becomes an invaluable asset for projects demanding impressive visual detail, from large-scale digital art to cinematic-quality visuals.\n"
    },
    {
        "id": "1149",
        "readme": "The XLabs FLUX IPAdapter V2 elevates image-to-image and text-guided transformation in the FLUX series, built to support high-quality, detailed adaptations. While XLabs FLUX IPAdapter V2 introduces enhancements over V1, our tests reveal that it isn’t universally superior. Instead, both versions offer unique strengths, and the optimal choice depends on individual project needs. We encourage users to tweak the parameters in both XLabs FLUX IPAdapter V1 and V2, compare the results, and select the version that best aligns with their creative goals.\n\n## FLUX IPAdapter V2\n\nAs a major upgrade to V1, XLabs FLUX IPAdapter V2 enhances both resolution handling and training depth:\n\n- **Refined Training for Consistency and Detail**: FLUX IPAdapter V2 has undergone intensive training at 512x512 resolution for 150,000 steps and at 1024x1024 for 350,000 steps, far exceeding V1’s 50,000 and 25,000 steps at these resolutions. This training boost means V2 can capture complex details and execute nuanced transformations more reliably, making it ideal for professional-grade visuals and artistic applications.\n- **Aspect Ratio Preservation**: One of the standout features in V2 is its ability to keep the original aspect ratio of images during transformations, avoiding the distortions sometimes seen in V1. This update helps maintain the authentic look of input images—perfect for creators focused on preserving visual integrity.\n\nHere are some enhancements in XLabs FLUX IPAdapter V2 based on quick tests:\n\n- **Detailed Facial Feature Generation**: FLUX IPAdapter V2 excels at creating intricate facial details, making it ideal for character design.\n- **Anime Character Processing**: FLUX IPAdapter V2 is perfect for generating vivid, anime-style characters with high precision.\n- **Faster Processing Speed**: FLUX IPAdapter V2 offers faster rendering times for a more efficient creative process.\n\n## Using FLUX IPAdapter V2 in ComfyUI\n\nWith FLUX IPAdapter V2, ComfyUI users can seamlessly integrate and fine-tune transformations in a structured workflow. Here’s how to get the most out of this tool:\n\n1. **Upload the Base Image**: Begin by uploading the image you want to transform as the starting point for adaptations.\n2. **Model Loading**:\n    - **Diffusion Model**: Load the diffusion model to handle initial image processing.\n    - **DualCLIP Loader**: Add the DualCLIP model to enhance text-to-image connections.\n    - **VAE Model**: Include a Variational Autoencoder (VAE) to maximize image quality.\n3. **Fine-Tune Text Prompts**: Carefully adjust your text prompts to guide the model’s interpretation and ensure control over the output’s visual attributes, theme, or style.\n4. **Set Up the FLUX IPAdapter Model and Configure Sampling Parameters**:\n    - Use both **FLUX IPAdapter V1 and FLUX IPAdapter V2** models to allow for comparison between outputs.\n    - In the **XlabsSampler** node, configure the following critical parameters to achieve detailed, high-quality images:\n        - **steps**: Choose the number of sampling iterations based on desired clarity. For FLUX IPAdapter V1, try around 50 steps, while for FLUX IPAdapter V2, aim for approximately 40-50 steps.\n        - **true_gs**: The guidance scale. For FLUX IPAdapter V1, try around 3.5, while for FLUX IPAdapter V2, aim for approximately 1.\n5. **Preview and Compare Results**: Use side-by-side comparisons to examine how the different configurations affect image quality. This approach helps identify which settings enhance or detract from the desired visual outcome, especially when testing new features in FLUX IPAdapter V2.\n\n\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1150",
        "readme": "## About Pyramid Flow\n\nPyramid Flow is an innovative solution for video generation that effectively balances high-quality output with computational efficiency. Its unique approach uses a pyramidal structure that processes video at varying resolutions, starting with lower resolutions and gradually scaling up to full quality in the final stage, rather than maintaining full resolution throughout the entire process. For detailed information, please visit the official [Pyramid Flow](https://pyramid-flow.github.io/) website.\n\n## ComfyUI Pyramid Flow Workflow\n\nPyramid Flow is now integrated into ComfyUI, enabling both text-to-video and image-to-video generation. For additional details, visit the [Pyramid-Flow GitHub repository](https://github.com/jy0205/Pyramid-Flow) and check out [kijai's](https://github.com/kijai/ComfyUI-PyramidFlowWrapper) contributions. Special thanks to all contributors who made this possible!\n\nAt our platform, the default Pyramid Flow workflow is set to text-to-video, but you can easily switch to image-to-video mode by right-clicking and enabling the image input group.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1150/readme_01.webp\" alt=\"Pyramid Flow\" width=\"650\"/>\n\nHere are the key **Pyramid Flow** parameters you should know:\n\n**Pyramid Flow Sampler** Settings:\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1150/readme_02.webp\" alt=\"Pyramid Flow\" width=\"650\"/>\n\n- **Width/Height**: Set your desired video dimensions\n- **First Frame Steps**: Controls the quality of the initial frame through a three-phase process, specified as (number1, number2, number3):\n    - First number: Initial image generation steps\n    - Second number: Refinement phase steps\n    - Third number: Final quality enhancement steps\n    Example: (20, 20, 20) applies 20 steps to each phase\n- **Video Steps**: Similar to First Frame Steps, but for subsequent frames. Format: (number1, number2, number3)\n    - Determines the number of steps for generating each consecutive frame\n    Example: (10, 10, 10) applies 10 steps to each frame generation phase\n- **Itemp (16)**: Controls output randomness\n- **Guidance Scale**: Determines how strictly the generation follows the input prompt\n- **Seed**: Sets a specific random seed for reproducible results\n- **Keep Model Loaded**: Option to retain or release model resources after generation\n"
    },
    {
        "id": "1151",
        "readme": "This \"Flux Consistent Characters Workflow Series 2 (Input Image)\" was created by [Mickmumpitz on his YouTube channel](https://www.youtube.com/watch?v=Uls_jXy9RuU&t=715s). We highly recommend checking out his detailed tutorial to learn how to use this powerful Consistent Characters workflow effectively. While we've reproduced the Flux Consistent Characters workflow and set up the environment for your convenience, all credit goes to Mickmumpitz for his excellent work on developing this Flux-based Consistent Characters solution.\n\nIf you want to create Flux consistent characters with text input, please use the [\"Flux Consistent Characters Workflow Series 1 (Input Text)\"](https://www.runcomfy.com/comfyui-workflows/consistent-characters-with-flux-comfyui-workflow) by Mickmumpitz.\n\n## The Flux Consistent Characters (Input Image)\n\nThe Flux Consistent Characters (Input Image) Workflow is a powerful tool that enables you to generate AI characters that closely resemble a provided reference image. By utilizing the Flux-PuLID Model, this workflow ensures that the created characters maintain a high degree of visual consistency with your input images across various poses and angles. The Flux.1 dev model plays a crucial role in enhancing the stability and reliability of the generated characters. Whether you're working on animated films, illustrated storybooks, or any other visually-oriented project, this workflow streamlines the process of creating characters that accurately match your initial design, as depicted in the input image. With the Flux Consistent Characters (Input Image) Workflow, you can bring your character concepts to life with unparalleled fidelity and consistency.\n\n## How to Use the Flux Consistent Characters (Input Image) Workflow?\n\nHere's a detailed guide on using the [Flux Consistent Characters (Input Text)](https://www.runcomfy.com/comfyui-workflows/consistent-characters-with-flux-comfyui-workflow) Please refer to this guide for more information.\n\nThe \"Flux Consistent Characters (Input Image) Workflow\" differs from the text-based one by using an image as input instead of text. In this image-based workflow, the author employs the [Flux-PuLID model](https://github.com/ToTheBeginning/PuLID/tree/main)  to maintain consistent facial identity across generated images.\n\n### 1. Consistent Character Generation with Flux and Flux-PuLID\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1151/readme01.webp\" alt=\"Flux Consistent Characters (Input Image)\" width=\"750\"/>\n\nFlux-PuLID is an innovative AI model that combines the capabilities of PuLID (Pure and Lightning ID Customization) with the powerful text-to-image generation of the FLUX model. It allows users to customize images, especially faces, to a specific identity while preserving the core functionality and image quality of the underlying FLUX model.\n\nThe key strength of Flux-PuLID lies in its ability to generate images with high identity similarity to the provided reference image. It achieves this by utilizing a contrastive alignment technique that minimizes disruption to the original FLUX model. At the same time, it employs a Lightning T2I branch that enables rapid generation of high-quality images from noise in a manageable number of steps. This Lightning T2I branch also allows the calculation of an accurate identity loss, ensuring the generated images faithfully capture the desired identity.\n\nAnother notable feature of Flux-PuLID is its editability. Users can modify attributes, styles, and backgrounds of the generated images using text prompts, providing extensive creative control. The model strives to maintain consistency in image elements like lighting, composition, and overall style before and after the identity customization process.\n\nHowever, Flux-PuLID does have some limitations. While it generally performs well across a wide range of identities, the results may vary depending on the quality and characteristics of the input reference image. Very complex or abstract text prompts might also lead to less accurate identity preservation.\n\n### 2. Upsacle + Face Fix\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1151/readme02.webp\" alt=\"Flux Consistent Characters (Input Image)\" width=\"400\"/>\n\n### 3. Poses\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1151/readme03.webp\" alt=\"Flux Consistent Characters (Input Image)\" width=\"750\"/>\n\n### 4. Emotions\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1151/readme04.webp\" alt=\"Flux Consistent Characters (Input Image)\" width=\"750\"/>\n\n### 5. Add background\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1151/readme05.webp\" alt=\"Flux Consistent Characters (Input Image)\" width=\"750\"/>\n\n\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1152",
        "readme": "[CatVTON (A Revolutionary Approach to Virtual Try-On Using Diffusion Models)](https://github.com/Zheng-Chong/CatVTON?tab=readme-ov-file) represents a groundbreaking advancement in virtual try-on technology, introducing a simplified yet powerful approach that relies on spatial concatenation of garment and person images. Now available as a ComfyUI workflow, CatVTON makes creating high-quality virtual try-on images more accessible than ever.\n\n## 1. Understanding CatVTON\n\nAt its core, CatVTON stands out for its elegantly simple architecture. Unlike traditional virtual try-on solutions that depend on complex components like additional image encoders, ReferenceNet, or text encoders, CatVTON streamlines the process by focusing solely on essential diffusion modules. This minimalist approach not only reduces the model's parameter count significantly but also enhances its efficiency and training process.\n\n## 2. Getting Started with CatVTON in ComfyUI\n\n### Basic Setup\n\n1. **Image Preparation**\n    - Upload a clear photo of your target person\n    - Provide a reference image of the desired garment\n2. **Mask Generation**\n    - Use the \"AutoMasker\" node\n    - Select the appropriate clothing category:\n        - Overall\n        - Upper body\n        - Lower body\n3. **Parameter Configuration**\n    - **Diffusion Steps**: Begin with 42 steps\n        - Higher values: Better quality but slower processing\n        - Lower values: Faster processing but potentially reduced quality\n    - **CFG Scale**: Start with a value of 50\n        - Higher values: More accurate garment transfer\n        - Lower values: Greater output variety\n\n### Optimization Tips\n\nFor the best possible results:\n\n- Use high-resolution, well-lit images for both person and garment inputs\n- Experiment with different clothing combinations\n- Fine-tune parameters based on your specific needs\n\n## Licensing Information\n\nCatVTON is released under the Creative Commons BY-NC-SA 4.0 license. This means you can:\n\n- Share and adapt the material\n- Use it for non-commercial purposes\n- Must provide appropriate attribution\n- Share any modifications under the same license terms\n\nFor more detailed information and technical specifications, visit the [official CatVTON repository](https://github.com/Zheng-Chong/CatVTON).\n\n---\n\nSo, are you looking for an amazing virtual try-on solution? CatVTON is your answer! Dive into the world of CatVTON and start creating amazing virtual try-on experiences today. At RunComfy, we've set up all the models and environments for you to try! Happy creating!\n"
    },
    {
        "id": "1153",
        "readme": "Create stunning video animations by transforming your subject (dancer) and give them a dynamic audioreactive background made up of various intricate geometries and psychedelic patterns. You can use this workflow with single or multiple subjects. With this workflow, you can produce mesmerizing audioreactive visual effects that perfectly sync with the rhythm of the music, offering an immersive experience. The workflow allows you to use it with a single subject or multiple subjects, all enhanced with audioreactive elements.\n\n## How to use Audioreactive Dancers Evolved Workflow:\n\n1. Upload a subject video in the Input section\n2. Select the desired width and height of the final video, along with how many frames of the input video should be skipped with “every_nth”. You can also limit the total number of frames to render with “frame_load_cap”.\n3. Fill out the positive and negative prompt. Set batch frame times to match when you’d like the scene transitions to occur.\n4. Upload images for each of the default IP Adapter subject mask colors:\n5. Red, Green, Blue = subject(s)\n6. Black = Background\n7. White = White audioreactive dilation mask\n8. Yellow, Magenta = Background Noise mask patterns\n9. Load a good LCM checkpoint (I use ParadigmLCM by Machine Delusions) in the “Models” section.\n10. Add any loras using the Lora stacker below the model loader\n11. Hit Queue Prompt\n\n## Video Guide\n\n[Audioreactive Dancers Evolved WalkthroughJ_4](https://youtu.be/RQdZ1R7gJ_4)\n\n## Node and Group Color\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme01.webp\" alt=\"Node and Group Color\" width=\"400\"/>\n\n- For this workflow I have color-coordinated nodes based on their functionality within each group.\n- Group Section titles are color coordinated for easier differentiation.\n\n## Input\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme02.webp\" alt=\"Input\" width=\"750\"/>\n\n- Upload your desired subject video to the Load Video (Upload) node.\n- You can adjust the output width and height using the top left two inputs.\n- **every_nth** sets whether to use every other frame, every third frame and so on (2 = every other frame). Left at 1 by default.\n- **skip_frames** is used to skip frames at the beginning of the video. (100 = skip the first 100 frames of the input video). Left at 0 by default.\n- **frame_load_cap** is used to specify how many total frames of the input video should be loaded. Best to keep low when testing settings (30 - 60 for example) and then increase or set to 0 (no frame cap) when rendering the final video.\n- The number fields in bottom right display info about the uploaded input video: total frames, width, height, and FPS from top to bottom.\n- If you already have a mask video of the subject generated , you can un-mute the “Upload Subject Mask” section and upload the mask video. Optionally you can mute the “Segment Dancer” section to save some processing time.\n- Sometimes the segmented subject will not be perfect, you can check the mask quality using the preview box in the bottom right seen above. If that is the case you can play around with the prompt in the “Florence2Run” node to target different body parts such as “head”, “chest”, “legs”, etc. and see if you get a better result.\n\n## Prompt\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme03.webp\" alt=\"Prompt\" width=\"300\"/>\n\n- Set the positive prompt using batch formatting:\n    - e.g. “0”: “4k, masterpiece, 1girl standing on the beach, absurdres”, “25”: “HDR, sunset scene, 1girl with black hair and a white jacket, absurdres”, …\n- Negative prompt is normal format, you can add embeddings if desired.\n\n## Audio Processing\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme04.webp\" alt=\"Audio Processing\" width=\"750\"/>\n\n- This section takes in audio from the input video, extracts the stems (bass, drums, vocals, etc.) and then converts it to a normalized amplitude synced with the input video frames, to create audioreactive visuals.  \n- amp_control = total range the amplitude can travel.\n- amp_offset = the minimum value the amplitude can take.\n    - Example: amp_control = 0.8 and amp_offset = 0.2 means the signal will travel between 0.2 and 1.0.\n- Sometimes the Drums stem will have the actual Bass notes of the song, preview each to see which to use for your audioreactive masks.\n- Use the graphs to get a good understanding of how the signal for that stem changes over the length of the video\n\n## Voronoi Generators\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme05.webp\" alt=\"Voronoi Generators\" width=\"750\"/>\n\n- This section generates Voronoi noise patterns using two FAI_Voronoi_Generator custom nodes per group which are composited together using a Multiply.\n- You can increase the Randomness Scheduler values in the parenthesis from 0 to break up symmetrical patterns in the final output.\n- Increase the Detail Scheduler value in the parenthesis to increase the detail count in the output noise patterns. Lower values results in lower noise differentiation.\n- Change the “formula” parameters in  the FAI Scale Scheduler node to have a large impact on the final noise pattern movement.\n- You can also change the “distance_metric” function on the FAI_Voronoi_Generator nodes themselves to greatly affect the generated patterns and shapes of the resulting noise.\n\n## Audio Masks\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme06.webp\" alt=\"Audio Masks\" width=\"750\"/>\n\n- This section is used to convert the voronoi noise image batches into colored masks to be composited with the subject, as well as synchronizes their movements with the beat of either the bass or drums audio stems. These masks are essential for creating audioreactive effects.\n- Increase the “lag_factor” in the AK Lag Chop node to increase how “spiky” the final amplitude graphs will be. This will cause the output noise movement to speed up and slow down more suddenly, whereas a lower lag_factor will result in a more gradual deceleration of movement after each beat. This is used to avoid the noise mask animation from appearing too “jumpy” and rigid.\n- The AK Rescale Float List is used to re-map the normalize amplitude values from 0-1 to new_min and new_max. A value of 1.0 represents 30FPS playback speed of the noise animation, whereas 0.5 represents 15FPS, 2.0 represents 60FPS, etc. Adjust this value to change how slow the audioreactive noise pattern animates off the beat (amplitude 0.0), and how fast it moves on the beat (amplitude 1.0).\n- The Keyframe Scheduler has a large affect on the appearence of the mask. It creates a list of float values to specify the threshold of pixel brightness values to use for the noise input images which will result in part of the noise getting cropped and turned into the final mask. Lower this value to retain more of the input noise, and increase to retain less of the noise.\n\n## Dilate Masks\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme07.webp\" alt=\"Dilate Masks\" width=\"750\"/>\n\n- Each colored group corresponds to the color of dilation mask that will be generated by it.\n- You can set the min and max radius of the dilation mask, as well as the shape using the following node:\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme08.webp\" alt=\"Dilate Mask Settings\" width=\"400\"/>\n\n- shape: “circle” is the most accurate but takes longer to generate. Set this when you are ready to perform the final rendering. “square” is fast to compute but less accurate, best for testing out the workflow and deciding on IP adapter images.\n- max_radius: The radius of the mask in pixels when the amplitude value is max (1.0).\n- min_radius: The radius of the mask in pixels when the amplitude value is min (0.0).\n- If you already have a composite mask video generated you can un-mute the “Override Composite Mask” group and upload it. It’s recommended to bypass the dilation mask groups if overriding to save on processing time.\n\n## Latent Noise Mask\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme09.webp\" alt=\"Latent Noise Mask\" width=\"750\"/>\n\n- Use latent noise masks to control which masks are actually diffused (dreamed over) by the ksampler. Bypass the group corresponding to the colored mask that you *don’t* want diffused (i.e. want to have elements from the original video appear on).\n- Leaving all mask groups enabled results in a white final noise mask (everything will be diffused).\n- **Example:** Bypass the Red Subject Mask group by clicking the Fast Bypasser node to have your dancer or subject appear in the final output.\n\n### Original Input Video:\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme10.webp\" alt=\"Original Input Video\" width=\"350\"/>\n\n### Bypassing Red and Yellow Mask Groups:\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme11.webp\" alt=\"Bypassing Red and Yellow Mask Groups\" width=\"350\"/>\n\n## Composite Mask\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme12.webp\" alt=\"Composite Mask\" width=\"750\"/>\n\n- This section creates the final composite of the voronoi noise masks with the subject mask (and audioreactive dilation mask if enabled).\n\n## Models\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme13.webp\" alt=\"Models\" width=\"350\"/>\n\n- Use a good LCM model for the checkpoint. I recommend ParadigmLCM by Machine Delusions.\n- You can merge multiple models together using the Model Merge Stack to get various interesting effects. Make sure the weights add up to 1.0 for the enabled models.\n- You can optionally specify the AnimateLCM_sd15_t2v_lora.safetensors with a low weight of 0.18 to further enhance the final result.\n- Add any additional Loras to the model using the Lora stacker below the model loader.\n\n## AnimateDiff\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme14.webp\" alt=\"AnimateDiff\" width=\"250\"/>\n\n- You can set a different Motion Lora instead of the one I used (LiquidAF-0-1.safetensors)\n- Increase/decrease the Scale and Effect floats to increase/decrease the amount of motion in the output.\n\n## IP Adapters\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme15.webp\" alt=\"IP Adapters\" width=\"750\"/>\n\n- Here you can specify the reference images that will be used to render the backgrounds for each of the dilation masks, as well as your video subject(s).\n- The color of each group represents the mask it targets:\n\n### Red, Green, Blue:\n\n- Subject mask reference images.\n\n### Black:\n\n- Background mask image, upload a reference image for the background.\n\n### White:\n\n- Dilation mask reference images, upload a reference image for each color dilation mask in use.\n\n### Yellow, Magenta\n\n- Voronoi noise mask reference images.\n\n## ControlNet\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme16.webp\" alt=\"ControlNet\" width=\"750\"/>\n\n- This workflow makes use of 5 different controlnets, including AD, Lineart, QR Code, Depth, and OpenPose.\n- All of the inputs to the controlnets are generated automatically\n- You can choose to override the input video for the Lineart, Depth, and Openpose controlnets if desired by un-muting the “Override ” groups as seen below:\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme17.webp\" alt=\"ControlNet Overrides\" width=\"750\"/>\n\n- It is recommended you also mute the “Generate” groups if overriding to save processing time.\n\nTip:\n\n- Bypass the Ksampler and commence a render with your full input video. Once all the preprocessor videos are generated save them and upload them to the respective overrides. From now on when testing the workflow you will not have to wait for each preprocessor video to be generated individually.\n\n## Sampler\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme18.webp\" alt=\"Sampler\" width=\"750\"/>\n\n- By default the HiRes Fix sampler group will be muted to save processing time when testing\n- I recommend bypassing the Sampler group as well when trying to experiment with dilation mask settings to save time.\n- On final renders you can un-mute the HiRes Fix group which will upscale and add details to the final result.\n\n## Output\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1153/readme19.webp\" alt=\"Output\" width=\"750\"/>\n\n- There are two output groups: the left is for standard sampler output, and the right is for the HiRes Fix sampler output.\n- You can change where files will be saved by changing the “custom_directory” string in the “FileNamePrefixDateDirFirst” nodes. By default this node will save output videos in a timestamped directory in the ComfyUI “output” directory\n    - e.g. `…/ComfyUI/output/240812/<custom_directory>/<my_video>.mp4`\n\n---\nCreating an audioreactive video can add immersive, pulsing energy to your subject, with every frame responding to the beat in real-time. So, dive into the world of audioreactive art and enjoy the rhythm-led transformations!\n\n## About Author\n\nAkatz AI:\n\n- Website: [https://akatz.ai](https://akatz.ai/)\n- [http://patreon.com/Akatz](http://patreon.com/Akatz)\n- [https://civitai.com/user/akatz](https://civitai.com/user/akatz)\n- [https://www.youtube.com/@akatz_ai](https://www.youtube.com/@akatz_ai)\n- [https://www.instagram.com/akatz.ai/](https://www.instagram.com/akatz.ai/)\n- [https://www.tiktok.com/@akatz_ai](https://www.tiktok.com/@akatz_ai)\n- [https://x.com/akatz_ai](https://x.com/akatz_ai)\n- [https://github.com/akatz-ai](https://github.com/akatz-ai)\n\nContacts:\n\n- Email: **akatzfey@sendysoftware.com**\n"
    },
    {
        "id": "1157",
        "readme": "## 1. Flux PuLID for Realistic Face Swapping\nFlux PuLID, an advanced machine learning model, has significantly improved the process of face swapping. Designed to generate realistic portraits, Flux PuLID takes an input image and a textual description to create a face-swapped image that closely matches the original face while also enhancing hand details. By utilizing the capabilities of Flux PuLID, you can produce high-quality face-swapped images that preserve identities and seamlessly blend faces. This tutorial will provide an in-depth exploration of Flux PuLID and guide you through the process of using Flux PuLID for face swapping within the ComfyUI framework.\n\n## 2. How Flux PuLID Enhances Face Swapping\nFlux PuLID brings a new level of sophistication to face swapping, thanks to its state-of-the-art facial recognition and identity preservation techniques. Here's how Flux PuLID elevates the face swapping process:\n### 2.1. Precise Facial Feature Extraction\nFlux PuLID employs advanced algorithms to accurately identify and extract facial features from the source image, capturing the essence of the person's identity.\n### 2.2. Robust Identity Preservation\nWith Flux PuLID, the swapped face retains the original person's identity by preserving key facial characteristics, ensuring a high degree of resemblance to the source face.\n### 2.3. Seamless Blending\nFlux PuLID utilizes sophisticated blending techniques to seamlessly integrate the swapped face into the target image, creating a natural and realistic appearance.\n### 2.4. Attention to Detail\nFlux PuLID pays meticulous attention to details such as skin tone, lighting, and facial expressions, ensuring that the swapped face looks authentic and matches the context of the target image.\n\n## 3. Using Flux PuLID for Face Swapping in ComfyUI\nTo unleash the power of Flux PuLID for face swapping in ComfyUI, follow these detailed steps:\n\n### 3.1. Load the Flux PuLID Model\n    - Use the `PulidFluxModelLoader` node to load the Flux PuLID model.\n    - Specify the path to the Flux PuLID model file (e.g., \"flux/pulid_flux_v0.9.0.safetensors\").\n### 3.2. Load the Input Images\n    - Use the `LoadImage` node to load the source face image.\n    - Connect the output of the `LoadImage` nodes to the corresponding inputs of the `ApplyPulidFlux` node.\n### 3.3. Configure the ApplyPulidFlux Node\n    - Connect the loaded Flux PuLID model and input images to the `ApplyPulidFlux` node.\n    - Adjust the settings in the `ApplyPulidFlux` node according to your requirements:\n        - Set the fusion method (e.g., \"mean\", \"max\") to control how the swapped face is blended with the target image.\n        - Specify the number of training steps to fine-tune the face swapping process.\n        - Experiment with different weight values to balance the influence of the source face on the target image.\n### 3.4. Guide the Image Generation\n    - Connect the output of the `ApplyPulidFlux` node to the `BasicGuider` node.\n    - Configure the `BasicGuider` node with the desired conditioning (e.g., text prompts) to guide the face swapping process.\n### 3.5. Adjust the Flux Guidance\n    - Use the `FluxGuidance` node to control the strength of the face swapping effect.\n    - Experiment with different guidance scales to achieve the desired balance between the swapped face and the target image.\n### 3.6. Generate Face Swapping Image\n    - Once you have configured the Flux PuLID nodes and connected them properly in the ComfyUI workflow, it's time to generate your face-swapped image. Feel free to experiment with different input images and settings to achieve various face swapping effects.\n\n---\nFlux PuLID is an easy-to-use tool for anyone looking to create realistic face-swapped images. Its ability to maintain the identity of the original face while seamlessly blending it with another image is truly impressive. With Flux PuLID in ComfyUI, you have the power to generate face swaps that look so natural, people might do a double-take!\n\nIf you're ready to take your face swapping projects to the next level, it's time to start experimenting with Flux PuLID. Let Flux PuLID work its magic and elevate your face swapping game today!\n\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1158",
        "readme": "The **In-Context LoRA (IC-LoRA)** project, developed by Lianghua Huang, Wei Wang, Zhi-Fan Wu, and their collaborators at Tongyi Lab, represents a groundbreaking approach to task-agnostic image generation using diffusion transformers (DiTs). Their innovative framework leverages DiTs’ in-context generation capabilities by introducing minimal tuning and a streamlined pipeline, allowing for efficient task-specific tuning with small datasets. To learn more about their work and access their resources, visit the official repository at [GitHub](https://github.com/ali-vilab/In-Context-LoRA).\n\n## 1. About In-Context LoRA (IC-LoRA)\n\nIn-Context LoRA (IC-LoRA) is a powerful and flexible framework that enables existing text-to-image diffusion transformer models to perform a wide variety of image generation tasks with minimal additional training. The key idea behind In-Context LoRA is to leverage the inherent in-context learning capabilities of these models by providing them with carefully curated training data and using a simple yet effective fine-tuning approach.\n\nHere's how In-Context LoRA works in a nutshell:\n\n1. Image Concatenation: Instead of generating individual images, IC-LoRA concatenates a set of related items into a single large composite. This allows the model to learn the relationships and consistency required across the set..\n2. Prompt Engineering: The text prompts for the set are also concatenated into a single prompt. This prompt starts with an overall description of the set, followed by specific details for each individual item. By crafting the prompt this way, the model can understand the high-level task as well as the low-level requirements.\n3. Low-Rank Adaptation (LoRA): Rather than fine-tuning the entire diffusion model, which would be computationally expensive, IC-LoRA uses LoRA to adapt the model to each specific task. LoRA only trains a small set of auxiliary parameters, keeping the original model weights frozen. This makes the fine-tuning process much more efficient.\n4. Small Training Datasets: Another key insight of IC-LoRA is that you don't need massive datasets to trigger the in-context learning capabilities. Just 20-100 high-quality image sets per task are sufficient to achieve impressive results. This greatly reduces the data collection and computation burden.\n\nThe beauty of In-Context LoRA is that it is a task-agnostic framework. The same approach can be applied to a wide range of tasks, such as storyboard generation, font design, product design, visual effects, and more. By providing task-specific training data, IC-LoRA can adapt to each task without requiring any change to the model architecture itself.\n\n## 2. 10 In-Context LoRA models and their recommend settings\n\n<table style={{ width: '100%', border: '1px solid grey', borderCollapse: 'collapse' }}>\n  <thead>\n    <tr>\n      <th style={{ border: '1px solid grey', textAlign: 'center' }}><strong>Task</strong></th>\n      <th style={{ border: '1px solid grey', textAlign: 'center' }}><strong>Model</strong></th>\n      <th style={{ border: '1px solid grey', textAlign: 'center' }}><strong>Recommend Settings</strong></th>\n      <th style={{ border: '1px solid grey', textAlign: 'center' }}><strong>Example Prompt</strong></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}><strong>1. Couple Profile Design</strong></td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>\n        <a href=\"https://huggingface.co/ali-vilab/In-Context-LoRA/blob/main/couple-profile.safetensors\" target=\"_blank\" rel=\"noopener noreferrer\">couple-profile.safetensors</a>\n      </td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>width: 2048, height: 1024</td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>This two-part image portrays a couple of cartoon cats in detective attire; [LEFT] a black cat in a trench coat and fedora holds a magnifying glass and peers to the right, while [RIGHT] a white cat with a bow tie and matching hat raises an eyebrow in curiosity, creating a fun, noir-inspired scene against a dimly lit background.</td>\n    </tr>\n    <tr>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}><strong>2. Film Storyboard</strong></td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>\n        <a href=\"https://huggingface.co/ali-vilab/In-Context-LoRA/blob/main/film-storyboard.safetensors\" target=\"_blank\" rel=\"noopener noreferrer\">film-storyboard.safetensors</a>\n      </td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>width: 1024, height: 1536</td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>[MOVIE-SHOTS] In a vibrant festival, [SCENE-1] we find &lt;Leo&gt;, a shy boy, standing at the edge of a bustling carnival, eyes wide with awe at the colorful rides and laughter, [SCENE-2] transitioning to him reluctantly trying a daring game, his friends cheering him on, [SCENE-3] culminating in a triumphant moment as he wins a giant stuffed bear, his face beaming with pride as he holds it up for all to see.</td>\n    </tr>\n    <tr>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}><strong>3. Font Design</strong></td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>\n        <a href=\"https://huggingface.co/ali-vilab/In-Context-LoRA/blob/main/font-design.safetensors\" target=\"_blank\" rel=\"noopener noreferrer\">font-design.safetensors</a>\n      </td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>width: 1792, height: 1216</td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>The four-panel image showcases a playful bubble font in a vibrant pop-art style. [TOP-LEFT] displays \"Pop Candy\" in bright pink with a polka dot background; [TOP-RIGHT] shows \"Sweet Treat\" in purple, surrounded by candy illustrations; [BOTTOM-LEFT] has \"Yum!\" in a mix of bright colors; [BOTTOM-RIGHT] shows \"Delicious\" against a striped background, perfect for fun, kid-friendly products.</td>\n    </tr>\n    <tr>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}><strong>4. Home Decoration</strong></td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>\n        <a href=\"https://huggingface.co/ali-vilab/In-Context-LoRA/blob/main/home-decoration.safetensors\" target=\"_blank\" rel=\"noopener noreferrer\">home-decoration.safetensors</a>\n      </td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>width: 1344, height: 1728</td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>This four-panel image showcases a rustic living room with warm wood tones and cozy decor elements; [TOP-LEFT] features a large stone fireplace with wooden shelves filled with books and candles; [TOP-RIGHT] shows a vintage leather sofa draped in plaid blankets, complemented by a mix of textured cushions; [BOTTOM-LEFT] displays a corner with a wooden armchair beside a side table holding a steaming mug and a classic book; [BOTTOM-RIGHT] captures a cozy reading nook with a window seat, a soft fur throw, and decorative logs stacked neatly.</td>\n    </tr>\n    <tr>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}><strong>5. Portrait Illustration</strong></td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>\n        <a href=\"https://huggingface.co/ali-vilab/In-Context-LoRA/blob/main/portrait-illustration.safetensors\" target=\"_blank\" rel=\"noopener noreferrer\">portrait-illustration.safetensors</a>\n      </td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>width: 1152, height: 1088</td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>This two-panel image presents a transformation from a realistic portrait to a playful illustration, capturing both detail and artistic flair; [LEFT] the photograph shows a woman standing in a bustling marketplace, wearing a wide-brimmed hat, a flowing bohemian dress, and a leather crossbody bag; [RIGHT] the illustration panel exaggerates her accessories and features, with the bohemian dress depicted in vibrant patterns and bold colors, while the background is simplified into abstract market stalls, giving the scene an animated and lively feel.</td>\n    </tr>\n    <tr>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}><strong>6. Portrait Photography</strong></td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>\n        <a href=\"https://huggingface.co/ali-vilab/In-Context-LoRA/blob/main/portrait-photography.safetensors\" target=\"_blank\" rel=\"noopener noreferrer\">portrait-photography.safetensors</a>\n      </td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>width: 1344, height: 1728</td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>This [FOUR-PANEL] image illustrates a young artist's creative process in a bright and inspiring studio; [TOP-LEFT] she stands before a large canvas, brush in hand, adding vibrant colors to a partially completed painting, [TOP-RIGHT] she sits at a cluttered wooden table, sketching ideas in a notebook with various art supplies scattered around, [BOTTOM-LEFT] she takes a moment to step back and observe her work, adjusting her glasses thoughtfully, and [BOTTOM-RIGHT] she experiments with different textures by mixing paints directly on the palette, her focused expression showcasing her dedication to her craft.</td>\n    </tr>\n    <tr>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}><strong>7. PPT Template</strong></td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>\n        <a href=\"https://huggingface.co/ali-vilab/In-Context-LoRA/blob/main/ppt-templates.safetensors\" target=\"_blank\" rel=\"noopener noreferrer\">ppt-templates.safetensors</a>\n      </td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>width: 1984, height: 1152</td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>This four-panel image showcases a rustic-themed PowerPoint template for a culinary workshop; [TOP-LEFT] introduces \"Farm to Table Cooking\" in warm, earthy tones; [TOP-RIGHT] organizes workshop sections like \"Ingredients,\" \"Preparation,\" and \"Serving\"; [BOTTOM-LEFT] displays ingredient lists for seasonal produce; [BOTTOM-RIGHT] includes chef profiles with short bios.</td>\n    </tr>\n    <tr>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}><strong>8. Sandstorm Visual Effect</strong></td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>\n        <a href=\"https://huggingface.co/ali-vilab/In-Context-LoRA/blob/main/sandstorm-visual-effect.safetensors\" target=\"_blank\" rel=\"noopener noreferrer\">sandstorm-visual-effect.safetensors</a>\n      </td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>width: 1408, height: 1600</td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>[SANDSTORM-PSA] This two-part image showcases the transformation of a cyclist through a sandstorm visual effect; [TOP] the upper panel features a cyclist in vibrant gear pedaling steadily on a clear, open road with a serene sky in the background, highlighting focus and determination, [BOTTOM] the lower panel transforms the scene as the cyclist becomes enveloped in a fierce sandstorm, with sand particles swirling intensely around the bike and rider against a stormy, darkened backdrop, emphasizing chaos and power.</td>\n    </tr>\n    <tr>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}><strong>9. Sparklers Visual Effect</strong></td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>\n        <a href=\"https://huggingface.co/ali-vilab/In-Context-LoRA/blob/main/sparklers-visual-effect.safetensors\" target=\"_blank\" rel=\"noopener noreferrer\">sparklers-visual-effect.safetensors</a>\n      </td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>width: 960, height: 1088</td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>[REAL-SPARKLERS-OVERLAYS] The two-part image vividly illustrates a woodland proposal transformed by sparkler overlays; [TOP] the first panel depicts a man kneeling on one knee with an engagement ring before his partner in a forest clearing at dusk, with warm, natural lighting, [BOTTOM] while the second panel introduces glowing sparklers that form a heart shape around the couple, amplifying the romance and joy of the moment.</td>\n    </tr>\n    <tr>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}><strong>10. Visual Identity Design</strong></td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>\n        <a href=\"https://huggingface.co/ali-vilab/In-Context-LoRA/blob/main/visual-identity-design.safetensors\" target=\"_blank\" rel=\"noopener noreferrer\">visual-identity-design.safetensors</a>\n      </td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>width: 1472, height: 1024</td>\n      <td style={{ border: '1px solid grey', textAlign: 'center' }}>The two-panel image showcases the joyful identity of a produce brand, with the left panel showing a smiling pineapple graphic and the brand name “Fresh Tropic” in a fun, casual font on a light aqua background; [LEFT] while the right panel translates the design onto a reusable shopping tote with the pineapple logo in black, held by a person in a market setting, emphasizing the brand’s approachable and eco-friendly vibe.</td>\n    </tr>\n  </tbody>\n</table>\n\n## 3. Use Flux and In-Context LoRA in ComfyUI\n\nThis Flux and In-Context LoRA workflow utilizes the powerful combination of the Flux model and the In-Context LoRA to generate a set of related images based on a text prompt. Let's break down how it works step by step.\n\n### 3.1. Preloaded Flux and In-Context LoRA Models for Efficient Workflow\n\nOur platform already has the Flux model and 10 In-Context LoRA models ready for you to use. This makes your workflow easier and saves you time. Just choose the group you want and start creating.\n\n### 3.2. Crafting the Perfect Prompt based on the preset prompts\n\nThe most important part of this workflow is the text description that captures the main idea you want to create. We have already written prompts for each In-Context LoRA model in the list mentioned earlier. When you write your own prompt, please use these examples as a guide.\n\n### 3.3. Customizing Resolution and Dimensions\n\nTo create visuals that fit your needs perfectly, change the width and height settings to match the size you want. We have also provided recommended sizes of each In-Context LoRA model for you to use as a starting point.\n\n### 3.4. Flux Sampler\nThe Flux Sampler node manages the Flux sampling process and optimizes generation parameters for better results. Key parameters include:\n- Seed (1): The seed value ensures consistent outputs under the same settings. Adjusting the seed allows the Flux generation to produce varied results.\n- Steps (50): This parameter defines the number of steps in the Flux sampling process. Higher step counts enhance quality but require more processing time. Here, 50 steps offer a balanced option.\n- Guidance (3): Determines guidance strength. Higher values (e.g., 3) align the Flux-generated output more closely with input prompts, ensuring accurate representations of your intent.\n- Max Shift (null): This parameter defines the maximum displacement or transformation range. A \"null\" setting indicates default or unlimited range usage.\n- Base Shift (null): Similar to max shift, it adjusts the base transformation strength. Setting it to \"null\" applies the default configuration.\n- Denoise (1): Adjusts denoising strength during Flux generation. A value of 1 applies light denoising, effectively removing minor inconsistencies while retaining clarity.\n\n---\nThe combination of Flux and In-Context LoRA opens up a world of possibilities for creating interesting visual content. Try out these new models and unleash your creativity like never before.\n\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1159",
        "readme": "The [ComfyUI-MochiEdit](https://github.com/logtd/ComfyUI-MochiEdit) nodes and its associated workflow are fully developed by logtd and Kijai. We give all due credit to logtd and Kijai for this innovative work. On the RunComfy platform, we are simply presenting their contributions to the community. We deeply appreciate logtd and Kijai’s work!\n\nThe Mochi Edit workflow is a tool designed to let users modify video content using text-based prompts. It supports tasks such as adding or altering elements (e.g., placing hats on characters), adjusting the overall style, or replacing subjects within the footage.\n\n\n## 1. Mochi Edit Unsampling \n\nAt the heart of Mochi Edit lies its unsampling technique. Mochi Edit's unsampling technique leverages a simplified approach to editing videos and images, allowing for transformations through multi-modal prompts without requiring additional preprocessing steps or external network modules. The core idea behind Mochi Edit's unsampling is to manipulate the video's latent representation directly, rather than performing complex operations like face detection or pose estimation, which are common in traditional image generation pipelines. This method is aligned with the broader goal of creating a more flexible and streamlined image-generation process, much like GPT's ability to generate text from any input prompt. With Mochi Edit's unsampling technique, users can generate various styles and modifications directly from a multi-modal description, making the process far more intuitive and efficient.\n\n\nIn short, Mochi Edit allows you to create small variations of the video you upload. Like copy and translate motion of the subject to another subject or change background settings change subject properties..etc\n \n\n## 2. How to Use Mochi Edit Workflow?\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1159/readme01.webp\" alt=\"Mochi Edit\" width=\"750\"/>\n</p>\n\nIn this workflow, Left Green nodes are the inputs for Video and text, Middle Purple nodes are the mochi unsampler and sampler nodes, and Right Blue is the video output node.\n\n---\n\n\n### 2.1 Load Video Node\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1159/readme02.webp\" alt=\"Mochi Edit\" width=\"750\"/>\n</p>\n\n- Click and Upload your video in the load video node\n- `frame_load_cap`:  is default to 32 frames. Above 32 frames, Jumping Artifacts are observerd. Keep under 3 seconds (32) frames for best results.\n- `skip_frames`: Skip frames if you want to start from a specific frame\n\nUse Square Format (512 x 512) or Horizontal (848 x 480) dimension in the upscale node. Others are giving error.\n\n### 2.2 Prompts\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1159/readme03.webp\" alt=\"Mochi Edit\" width=\"550\"/>\n</p>\n\nThis is experimental usage, sometimes it may work or sometimes not or sometimes fully change the original video. \n\n- Use small variation of the subject as prompt. \n- Strong Variation may distort and change the image fully.\n- Try different seed if you are not getting proper results.  \n\n\n\n### 2.3 Sampling and Unsampling Node Groups \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1159/readme04.webp\" alt=\"Mochi Edit\" width=\"750\"/>\n</p>\n\nThe Sampling and Unsampling Ksampler are set to best setting by the author. If settings edited vaguely may result in weird undesirable results. Feel free to play with: \n\n- Sampler's `Seed` for varations\n- `num_steps` and `linear_step` to Change rendering quality or speed. \n- `eta` , `start_step` and `end_Step` to change the unsampling strength, start and end percent. \n\n\n### 2.4 Mochi Models\n\n\n<p align=\"center\">\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1159/readme05.webp\" alt=\"Mochi Edit\" width=\"750\"/>\n</p>\n\nModels are downloaded from this [HuggingFace Repo](https://huggingface.co/Kijai/Mochi_preview_comfy/tree/main) in your comfyui Automatically. It will take around 5-10 mins for the first time to download the 10.3 GB model.\n\n\n--- \n\n\nMochi Edit's unsampling technique revolutionizes video and image editing by simplifying the process and removing the need for complex preprocessing or additional modules. This innovative approach empowers users to generate high-quality, customized visuals effortlessly through multi-modal prompts. By combining flexibility and accessibility, Mochi Edit paves the way for a more intuitive and creative image-generation future.\n\n"
    },
    {
        "id": "1160",
        "readme": "The [OmniGen-ComfyUI](https://github.com/AIFSH/OmniGen-ComfyUI) nodes and its associated workflow are fully developed by AIFSH and saftle. We give all due credit to AIFSH and saftle for this innovative work. On the RunComfy platform, we are simply presenting AIFSH's and saftle’s contributions to the community. It is important to note that there is currently no formal connection or partnership between RunComfy and AIFSH and saftle. We deeply appreciate AIFSH's and saftle’s work!\n\nThe ComfyUI OmniGen workflow enables you to manipluate a photo to change subject's cloths, background, Clothes Virtual Try On, Selfie with Celbrity, add or remove elements and much more. You can add upto 3 reference images for getting your desired results. \n\n\n## OmniGen \n\nOmniGen is a versatile, unified image generation model designed to create diverse visuals from multi-modal prompts with simplicity and flexibility. Unlike traditional models, it eliminates the need for extra modules like ControlNet or preprocessing steps such as pose estimation, enabling direct output through text-based instructions. Inspired by GPT's seamless approach to text generation, OmniGen empowers users to fine-tune it for any creative or specialized task with minimal effort. Its innovative framework makes image generation more accessible, fostering limitless possibilities for visual creativity. OmniGen is a step forward in universal image generation, inspiring the next wave of transformative AI tools.\n\n\n## 1.1 How to Use OmniGen Workflow?\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1160/readme01.webp\" alt=\"OmniGen\" width=\"750\"/>\n</p>\n\nIn this workflow, Left Green nodes are the inputs for Reference Images and Resolutions. Middle Pink and Purple nodes are the OmniGen unsampler and OmniGen Model Loader and Right Blue is the Image Save node.\n\n1) Upload your reference Images.\n2) Enter your resolution and prompts\n3) Click Queue\n\nNo need to setup anything, Rendered as simple as that.\n\n---\n\n\n## 1.2 Load Reference Input Images\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1160/readme02.webp\" alt=\"OmniGen\" width=\"750\"/>\n</p>\n\n- Upload 1, 2 or upto 3 Reference Images, elements like clothes, persons, celebrities, animals, etc.\n- These Images will also be Used in Prompts with Trigger word as \"image_1\" , \"image_2\" and \"image_3\" respectively. \n\n## 1.3 Resolution\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1160/readme03.webp\" alt=\"OmniGen\" width=\"550\"/>\n</p>\n\n- In this node set your resolution for your output image.\n- In higher image resolution (above 1k) more Vram will be consumed also chances of double figures or subjects may occour. \n- You can change the batch_size and the seed to randomize in the sampler to generate variations.\n\n## 1.4 OmniGen Sampler and Prompts\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1160/readme04.webp\" alt=\"OmniGen\" width=\"750\"/>\n</p>\n\n- Prompt: Add your prompts based on your reference images. Denote input images as \"image_1\" , \"image_2\" and \"image_3\". \n    - Example: The Person in image_1 is wearing jacket from image_2. \n- `num_inferene_step:` This set the sampling steps for OmniGen, Use value around 25-35 for good results\n- `img_guidance_scale:` This sets the weightage/Strength for the reference image(s). \n- `max_input_image_size:` This caps the resolutions of the inputs.\n   \n\n## 1.5 OmniGen Models\n\n<p align=\"center\">\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1160/readme05.webp\" alt=\"OmniGen\" width=\"750\"/>\n</p>\n\nModels are downloaded from this [Shitao's Hugging Face Repo](https://huggingface.co/Shitao/OmniGen-v1), in your comfyui/model/OmniGen Folder manually by the runcomfy server.\nIt may take upto 3-5 mins to make a local copy on your machine.\n  \nYou can Upload OmniGen other models here when they are released.\n\n\n--- \n\nOmniGen redefines image generation by offering a unified, flexible approach that simplifies workflows and removes the need for additional tools or preprocessing. Its easy fine-tuning and multi-modal capabilities open new possibilities for creative and specialized tasks. OmniGen sets the stage for more accessible and innovative AI-driven visual generation in the future.\n"
    },
    {
        "id": "1161",
        "readme": "The [ComfyUI-Fluxtapoz](https://github.com/logtd/ComfyUI-Fluxtapoz) nodes and its associated workflow are fully developed by logtd. We give all due credit to logtd for this innovative work. On the RunComfy platform, we are simply presenting his contributions to the community. It is important to note that there is currently no formal connection or partnership between RunComfy and logtd. We deeply appreciate logtd’s work!\n\n\n## Fluxtapoz - RF Inversion and Stylization\n\nFluxtapoz, a powerful set of custom nodes for ComfyUI, leverages [RF Inversion](https://rf-inversion.github.io/) to redefine image recovery and editing. By transforming real images into structured noise using rectified flow models, Fluxtapoz unlocks unparalleled creative flexibility. Fluxtapoz innovative approach bridges the gap between visuals and editable representations, enabling seamless refinement and artistic control, all within an intuitive workflow.\n\n## 1.1 How to Use Fluxtapoz Workflow?\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1161/readme01.webp\" alt=\"Workflow\" width=\"750\"/>\n</p>\n\nHow to Use Fluxtapoz Instructions: \n\n1) Upload your Image in load image node.\n2) Choose RF Inversion or Stylization Group\n3) Enter Prompts if needed.\n4) Click Queue.\n\nNo need to setup anything, Rendered as simple as that.\n\n\n## 1.2 Fluxtapoz - Setup Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1161/readme02.webp\" alt=\"Load Image\" width=\"750\"/>\n</p>\n\n- Upload your main image ( This will also be used as image dimension for empty latent). \n- Limit your image's resolution to 512,768 or 1024. See \"Get Image Size and Count\" to check your image dimensions.\n\n\n## 1.3 Fluxtapoz - Main \n\n### Fluxtapoz - RF Inversion Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1161/readme03.webp\" alt=\"RF Inversion\" width=\"750\"/>\n</p>\n\n### Fluxtapoz - Stylization Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1161/readme04.webp\" alt=\"Model\" width=\"750\"/>\n</p>\n\n\nPlay with the followwing Fluxtapoz Settings to have variations: \n\n\n### Flux Forward ODE Sampler - Settings:\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1161/readme05.webp\" alt=\"Load Image\" width=\"450\"/>\n</p>\n\n- `Gamma` -  Lower than 0.5 will give random results, Above 0.5 will give give more close to original input.\n- `seed`: Controls randomness in the image enhancement process, allowing reproducible results when using the same seed.\n- `steps`: The number of iterations for enhancing details. More steps result in finer details but require more processing time.\n\n### Flux Reverse ODE Sampler - Settings:\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1161/readme06.webp\" alt=\"Load Image\" width=\"450\"/>\n</p>\n\n\n(The Behavior is Similar to Tile Controlnet and it also depends on Total Sampler Steps)\n\n- `eta` - It's the Strength of the image on the Latent. \n- `start_step` - The Starting Step from where the effect will take place. \n- `end_step` - The Ending step till where the effect will take place. \n\n\n### Ksamplers - Settings:\n- `seed -` Controls randomness in the image enhancement process, allowing reproducible results when using the same seed.\n- `steps -` The number of iterations for enhancing details. More steps result in finer details but require more processing time.\n- `cfg -` The Classifier-Free Guidance scale, which adjusts how closely the model follows the input guidance.\n- `sampler_name -` Defines the sampling method used for detail refinement.\n- `scheduler -` Determines the computational scheduling strategy during processing.\n\n--- \n\n\nWith Fluxtapoz and RF Inversion, image editing becomes a seamless blend of precision and creativity. Redefine what's possible with Fluxtapoz—refine, recover, and transform like never before.\n\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1162",
        "readme": "UltraRealistic Lora V2 is an incredibly powerful LoRA (Low-rank Adaptation) model that takes realism and quality to new heights when used with the FLUX model. By using this workflow, you will leverage UltraRealistic Lora V2 with FLUX to achieve breathtakingly lifelike results.\n\nThis Flux UltraRealistic Lora V2 workflow was masterfully developed by [Danrisi](https://civitai.com/models/796382/ultrarealistic-lora-project?modelVersionId=1026423). All credit goes to Danrisi. To learn more about the work, be sure to visit Danrisi's Civitai page.\n\n## Key Features of UltraRealistic Lora V2\n\nUltraRealistic Lora V2 brings several important improvements over the original version:\n\n- Increased stability and consistency\n- Better anatomy and proportions\n- Improved overall image quality\n- Ability to adapt to various quality targets based on prompts\n\nThis means the LoRA can flexibly produce anything from high-definition photorealism to intentionally lower-quality or stylized aesthetics. It puts more control in your hands as the artist.\n\n## Recommended Settings for Flux and UltraRealistic Lora V2\n\n- CFG Scale = 1\n- Guidance Strength = 2.5\n- Scheduler = Beta\n- Sampler = dpmpp_2m\n- Steps = 40\n- LoRA Strength = 0.8 to 1.0 (lower to around 0.87 if hands degrade)\n\nThese settings provide a good balance of staying true to your prompt while leveraging the realism enhancements of the LoRA. Adjust as needed for your specific subject.\n\n## Prompt\n\nTo get the most out of UltraRealistic Lora V2, include some of the following keywords and phrases in your prompts:\n\n- \"amateurish photo\"\n- \"low lighting\"\n- \"in motion\"\n- \"overexposed\" / \"underexposed\"\n- \"GoPro lens\"\n- \"eerie atmosphere\"\n- \"smeared background\"\n- \"smeared foreground\"\n\nThese help guide the LoRA to produce the gritty realism it excels at. Experiment with different combinations to achieve your desired look.\n\n## How to use Flux UltraRealistic Lora V2 Workflow\n\nLoad Checkpoint (Default Flux): Begin by loading the base Flux checkpoint in your ComfyUI workflow, which sets the foundation for generating high-quality, realistic images that UltraRealistic Lora V2 can further enhance.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1162/readme01.webp\" alt=\"Flux UltraRealistic Lora V2\" width=\"550\"/>\n\nLoad LoRA (Default UltraRealistic Lora V2): Load the UltraRealistic Lora V2 model to work alongside the Flux checkpoint, bringing significant improvements to stability, anatomy, and overall image quality for incredibly lifelike results.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1162/readme02.webp\" alt=\"Flux UltraRealistic Lora V2\" width=\"550\"/>\n\nFlux Clip Prompting: Use Flux's clip prompting capabilities to guide image generation by crafting detailed, descriptive prompts that convey your desired scene, characters, emotions, and aesthetics, exploring the vast creative possibilities of UltraRealistic Lora V2 and FLUX.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1162/readme03.webp\" alt=\"Flux UltraRealistic Lora V2\" width=\"550\"/>\n\nSampling: Select the appropriate sampler (dpmpp_2m recommended for UltraRealistic Lora V2) and adjust sampling settings based on prompt complexity and desired output quality for optimal realism and detail.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1162/readme04.webp\" alt=\"Flux UltraRealistic Lora V2\" width=\"550\"/>\n\nImg2Img (Optional): Utilize the Img2Img functionality in FLUX to apply the style and realism enhancements of UltraRealistic Lora V2 to an existing input image, transforming it into a more lifelike and visually striking result.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1162/readme05.webp\" alt=\"Flux UltraRealistic Lora V2\" width=\"550\"/>\n\nPulid Integration: Incorporate the Pulid library into your workflow to optimize generated images for realism and aesthetics through post-processing techniques like color grading, contrast adjustment, and subtle effects for a polished, professional look.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1162/readme06.webp\" alt=\"Flux UltraRealistic Lora V2\" width=\"550\"/>\n\nFix Face: Fine-tune face-related prompts and settings to ensure realistic, properly proportioned faces with desired emotions, utilizing face-specific editing tools or techniques to refine and enhance facial details as needed.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1162/readme07.webp\" alt=\"Flux UltraRealistic Lora V2\" width=\"550\"/>\n\nFix Hands (Optional: The results might not always be stable): Carefully review and adjust hand-related prompts and settings to ensure natural, anatomically correct hands that match intended gestures or actions, employing hand-specific editing or post-processing techniques for refinement.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1162/readme08.webp\" alt=\"Flux UltraRealistic Lora V2\" width=\"550\"/>\n\nUpscale: After generating a satisfactory base image using UltraRealistic Lora V2 and FLUX, consider upscaling it to a higher resolution using reliable methods like ESRGAN to enhance sharpness, clarity, and detail while maintaining the achieved realism and quality.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1162/readme09.webp\" alt=\"Flux UltraRealistic Lora V2\" width=\"550\"/>\n\n---\n\nWith UltraRealistic Lora V2 and FLUX, you have immense power to generate stunningly realistic AI art. Have fun exploring the creative possibilities and bringing your visions to life! Let the realism flow through you.\n"
    },
    {
        "id": "1163",
        "readme": "This guide provides a comprehensive introduction to creating 3D content with the 'Stable Fast 3D' model, utilizing the advanced capabilities of \"ComfyUI 3D Pack\" nodes to streamline workflows and enhance output quality.\n\n## Part 1: ComfyUI 3D Pack\n\n### 1.1. Introduction to ComfyUI 3D Pack\n\nComfyUI 3D Pack is an extensive node suite that enables ComfyUI to process 3D inputs such as Mesh & UV Texture using cutting edge algorithms and models. It integrates advanced 3D processing algorithms like 3DGS (Gaussian Splatting) and NeRF (Neural Radiance Fields), along with state-of-the-art models including Hunyuan3D**,** StableFast3D, InstantMesh, CRM, TripoSR and others.\n\nWith the ComfyUI 3D Pack, users can import, manipulate and generate high quality 3D content within the intuitive ComfyUI interface. It supports a wide range of 3D file formats like OBJ, PLY, GLB enabling easy integration of existing 3D models. The pack also includes powerful mesh processing utilities to edit, clean, and optimize 3D geometry.\n\nOne of the key highlights is the integration of NeRF technology which allows photorealistic 3D reconstruction from 2D images. The 3DGS nodes enable point cloud rendering and stylization. InstantMesh and TripoSR models allow high-resolution upscaling and super-resolution of 3D meshes. CRM (Convolutional Reconstruction Model) enables recovering 3D shape from multi-view images and CCM (Color Correction Map).\n\nComfyUI 3D Pack was developed by [MrForExample](https://github.com/MrForExample), with all credit going to MrForExample. For detailed information, please see [ComfyUI 3D Pack](https://github.com/MrForExample/ComfyUI-3D-Pack/tree/main).\n\n### 1.2. ComfyUI 3D Pack: Ready to Run on RunComfy\n\nNow ComfyUI 3D Pack is fully setup and ready to use on the RunComfy website. Users don't need to install any additional software or dependencies. All the required models, algorithms and tools are pre-configured and optimized to run efficiently in the web-based ComfyUI environment.\n\n## Part 2: Using Stable Fast 3D model with ComfyUI 3D Pack Nodes\n\n### 2.1. What is Stable Fast 3D?\n\nStable Fast 3D (SF3D) is an advanced single-view 3D mesh reconstruction technique that generates high-quality, textured 3D meshes from a single image. Stable Fast 3D builds upon the TripoSR model but incorporates several key enhancements to produce superior 3D assets suitable for various downstream applications such as gaming, e-commerce, and AR/VR.\n\nStable Fast 3D was developed by [Stability-AI](https://github.com/Stability-AI), with all credit going to Stability-AI. For detailed information, please see [Stable Fast 3D](https://github.com/Stability-AI/stable-fast-3d)\n\n### 2.2. Technical Innovations in Stable Fast 3D\n\nStable Fast 3D employs a novel approach that explicitly trains the model for mesh generation. It integrates a fast UV unwrapping method to enable rapid texture generation instead of relying on vertex colors. Additionally, Stable Fast 3D learns to predict material parameters and normal maps, which significantly enhance the visual quality of the reconstructed 3D meshes. A key feature of Stable Fast 3D is its delighting step, which effectively removes low-frequency illumination effects, ensuring that the generated meshes can be easily used under different lighting conditions.\n\n### 2.3. Advantages/Potential Limitations of Stable Fast 3D\n\n1. High-speed generation: Stable Fast 3D can create high-quality 3D meshes in just 0.5 seconds on an H100 GPU.\n2. Illumination disentanglement: The delighting step in Stable Fast 3D removes baked-in shadows and lighting effects, making the 3D assets more versatile.\n3. UV unwrapping: Stable Fast 3D's fast UV unwrapping technique enables efficient texture mapping, resulting in smaller file sizes and faster rendering compared to vertex coloring.\n4. Smooth meshes: By using DMTet and learned vertex displacements with normal maps, Stable Fast 3D produces smoother mesh surfaces without Marching Cubes artifacts.\n5. Material properties: Stable Fast 3D predicts non-spatially varying material properties, enhancing the visual quality of the generated objects under different illuminations.\n\nHowever,Stable Fast 3D may have limitations in handling highly complex or detailed objects, as it relies on a single input image for reconstruction.\n\n### 2.4. Step-by-Step Guide: Implementing the Stable Fast 3D Workflow in ComfyUI\n\nThe ComfyUI 3D Pack provides a seamless integration of the Stable Fast 3D workflow, making it easy for users to generate high-quality 3D meshes from single images. The workflow involves the following steps:\n\nLoad the pre-trained Stable Fast 3D model using the \"[Comfy3D] Load SF3D Model\" node.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1163/readme01.webp\" alt=\"ComfyUI 3D Pack\" width=\"400\"/>\n\nPrepare the input image and mask using the \"LoadImage\" and \"InvertMask\" nodes.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1163/readme02.webp\" alt=\"ComfyUI 3D Pack\" width=\"400\"/>\n\nResize and preprocess the input image and mask using the \"[Comfy3D] Resize Image Foreground\" node.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1163/readme03.webp\" alt=\"ComfyUI 3D Pack\" width=\"400\"/>\n\nFeed the preprocessed image, mask, and loaded Stable Fast 3D model into the \"[Comfy3D] StableFast3D\" node to generate the 3D mesh.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1163/readme04.webp\" alt=\"ComfyUI 3D Pack\" width=\"400\"/>\n\nAdjust the mesh's axis and scale using the \"[Comfy3D] Switch Mesh Axis\" node if needed.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1163/readme05.webp\" alt=\"ComfyUI 3D Pack\" width=\"400\"/>\n\nSave the generated 3D mesh and preview the final 3D mesh using the \"[Comfy3D] Preview 3DMesh\" node.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1163/readme06.webp\" alt=\"ComfyUI 3D Pack\" width=\"400\"/>\n\nBy leveraging the ComfyUI 3D Pack nodes, users can easily integrate the Stable Fast 3D workflow into their projects, enabling them to create high-quality, textured 3D meshes from single images with just a few clicks. This powerful combination of Stable Fast 3D and ComfyUI opens up new possibilities for AI-assisted 3D content creation, making it more accessible and efficient for artists, designers, and developers alike.\n\n## License\n\nFor detailed licensing information when using Stable Fast 3D model, please see the [Stable Fast 3D License](https://github.com/Stability-AI/stable-fast-3d/blob/main/LICENSE.md).\n"
    },
    {
        "id": "1164",
        "readme": "The Hallo2 technique was developed by Jiahao Cui, Hui Li, Yao Yao, Hao Zhu, Hanlin Shang, Kaihui Cheng, Hang Zhou, Siyu Zhu, and Jingdong Wang from Fudan University and Baidu Inc. For more information, visit [Hallo2 GitHub](https://github.com/fudan-generative-vision/hallo2/tree/main). ComfyUI_Hallo2 nodes and workflow was developed by smthemex. For more details, visit [ComfyUI_Hallo2 GitHub](https://github.com/smthemex/ComfyUI_Hallo2). All credits to their contributions.\n\n## 1. About Hallo2\n\nHallo2 is a cutting-edge model for generating high-quality, long-duration, 4K resolution audio-driven portrait animation videos. It builds upon the original Hallo model with several key improvements:\n\n1. Supports generating much longer videos, up to tens of minutes or even hours long\n2. Generates videos at 4K resolution\n3. Allows controlling expression and pose using textual prompts in addition to audio\n\nHallo2 achieves this by using advanced techniques like data augmentation to maintain consistency over long durations, vector quantization of latent codes for 4K resolution, and an improved denoising process guided by both audio and text.\n\n## 2. Technical Features of Hallo2\n\nHallo2 combines several advanced AI models and techniques to create its high-quality portrait videos:\n\n1. **Diffusion Model**: This is the core \"engine\" that generates the video frames. It starts with random noise and gradually refines it to match the desired output, guided by the audio and text prompts.\n2. **3D U-Net**: This is a type of neural network that acts as the \"sculptor\" in the diffusion process. It looks at the current noisy frame, the audio, and the text instructions, and suggests how to change the noise to make it look more like the final portrait.\n3. **Audio Encoder**: Hallo2 uses a model called Wav2Vec2 as its \"ears\" to understand the audio, converting the raw waveform into a compact representation that captures tone, speed, and speech content.\n4. **Face Detector**: To help it focus on animating the face, Hallo2 uses a face detection model to automatically locate the portrait's face in the reference image. It then knows where to apply the lip and expression movements.\n5. **Image Compressor**: To efficiently work with high-res 4K images, Hallo2 uses a special type of autoencoder model (VQ-VAE) to compress them into a smaller \"latent\" representation, and then decode them back to 4K at the end. This is like how JPEGs shrink image file sizes while preserving quality.\n6. **Augmentation Tricks**: To help maintain quality over long videos, Hallo2 applies some clever \"data augmentations\" to the previous generated frames before using them to influence the next frame. These include occasionally erasing random patches or adding subtle noise. This helps prevent compounding errors that could otherwise build up and ruin the consistency over time.\n\nSo in summary - Hallo2 takes in audio and a portrait image, has an AI \"agent\" that sculpts video frames to match them while staying true to the original portrait, and employs some extra tricks to keep everything synced and coherent even in long videos. All of these parts work together in a multi-step pipeline to produce the impressive results you see.\n\n## 3. How to Use ComfyUI Hallo2 Workflow\n\nHallo2 has been integrated into ComfyUI via a custom workflow with several specialized nodes. Here's how to use it:\n\n1. Load your reference portrait image using the `LoadImage` node. This should be a clear front-facing portrait. (Tips: The better framed and lit your reference portrait is, the better the results will be. Avoid side profiles, occlusions, busy backgrounds etc.) \n2. Load your driving audio using the `LoadAudio` node. It should match the mood you want the portrait to emote.\n3. Connect the image and audio to the `HalloPreImgAndAudio` node. This preprocesses the image and audio into embeddings. Key parameters:\n    - `audio_separator`: Model for separating speech from background noise. Generally leave at default.\n    - `face_expand_ratio`: How much to expand the detected face region by. Higher values include more of the hair/background.\n    - `width`/`height`: Generation resolution. Higher values are slower but more detailed. 512-1024 square is a good balance.\n    - `fps`: Target video FPS. 25 is a good default.\n4. Load the core Hallo2 model using the `HalloLoader` node. Point it to your Hallo2 checkpoint, VAE, and motion module files.\n5. Connect the preprocessed image and audio embeddings along with the loaded model to the `HalloSampler` node. This performs the actual video generation. Key parameters:\n    - `seed`: Random seed which determines minor details. Change it if you don't like the first result.\n    - `pose_scale`/`face_scale`/`lip_scale`: How much to scale the intensity of pose, facial expression, and lip movements. 1.0 = full intensity, 0.0 = frozen.\n    - `cfg`: Classifier-free guidance scale. Higher = follows conditioning more closely but is less diverse.\n    - `steps`: Number of denoising steps. More steps = better quality but slower.\n6. At this point, you can view the generated video. To further improve quality with super-resolution, add the `HallosUpscaleloader` and `HallosVideoUpscale` nodes to the end of the chain. The upscale loader reads in a pretrained upscaling model, while the upscaler node actually performs the upscaling to 4K.\n"
    },
    {
        "id": "1165",
        "readme": "## Update:\n\nThe latest [Hunyuan3D-2.0](https://www.runcomfy.com/comfyui-workflows/hunyuan3d-2-workflow-in-comfyui-create-3d-assets-from-images) model has been released! Click the link to try it out.\n\nBelow is an introduction to the Hunyuan3D-1.0 model.\n\n---\n\nThis guide provides a comprehensive introduction to creating 3D content with the 'Hunyuan3D-1.0' model, utilizing the advanced capabilities of \"ComfyUI 3D Pack\" nodes to streamline workflows and enhance output quality.\n\n## Part 1: ComfyUI 3D Pack\n\n### 1.1. Introduction to ComfyUI 3D Pack\n\nComfyUI 3D Pack is an extensive node suite that enables ComfyUI to process 3D inputs such as Mesh & UV Texture using cutting edge algorithms and models. It integrates advanced 3D processing algorithms like 3DGS (Gaussian Splatting) and NeRF (Neural Radiance Fields), along with state-of-the-art models including Hunyuan3D**,** StableFast3D, InstantMesh, CRM, TripoSR and others.\n\nWith the ComfyUI 3D Pack, users can import, manipulate and generate high quality 3D content within the intuitive ComfyUI interface. It supports a wide range of 3D file formats like OBJ, PLY, GLB enabling easy integration of existing 3D models. The pack also includes powerful mesh processing utilities to edit, clean, and optimize 3D geometry.\n\nOne of the key highlights is the integration of NeRF technology which allows photorealistic 3D reconstruction from 2D images. The 3DGS nodes enable point cloud rendering and stylization. InstantMesh and TripoSR models allow high-resolution upscaling and super-resolution of 3D meshes. CRM (Convolutional Reconstruction Model) enables recovering 3D shape from multi-view images and CCM (Color Correction Map).\n\nComfyUI 3D Pack was developed by [MrForExample](https://github.com/MrForExample), with all credit going to MrForExample. For detailed information, please see [ComfyUI 3D Pack](https://github.com/MrForExample/ComfyUI-3D-Pack/tree/main).\n\n### 1.2. ComfyUI 3D Pack: Ready to Run on RunComfy\n\nNow ComfyUI 3D Pack is fully setup and ready to use on the RunComfy website. Users don't need to install any additional software or dependencies. All the required models, algorithms and tools are pre-configured and optimized to run efficiently in the web-based ComfyUI environment.\n\n## Part 2: Using Hunyuan3D model with ComfyUI 3D Pack Nodes\n\n### 2.1. What is Hunyuan3D?\n\nHunyuan3D is an innovative 3D generation framework developed by Tencent that combines the power of multi-view diffusion models and sparse-view reconstruction models to create high-quality 3D assets from single images or text descriptions. The Hunyuan3D 1.0 framework is available in two versions: a lite version and a standard version, both supporting text- and image-conditioned generation. For detailed information, please see [Hunyuan3D-1](https://github.com/Tencent/Hunyuan3D-1).\n\n### 2.2. Techniques Behind Hunyuan3D\n\nHunyuan3D introduces several technical innovations to enhance the speed and quality of 3D generation:\n\na. Two-stage pipeline: \n\nIn the first stage, a multi-view diffusion model efficiently generates multi-view RGB images. These images capture rich details of the 3D asset from various viewpoints. \n\nThe second stage employs a feed-forward reconstruction model that rapidly reconstructs the 3D asset from the generated multi-view images.\n\nb. 0-elevation pose distribution\n\nHunyuan3D's multi-view generation uses a 0-elevation camera orbit, maximizing the visible area between generated views and improving reconstruction quality.\n\nc. Adaptive classifier-free guidance\n\nThis technique balances controllability and diversity for multi-view diffusion, ensuring consistent and high-quality results.\n\nd. Hybrid inputs\n\nThe sparse-view reconstruction model incorporates the uncalibrated condition image as an auxiliary view to compensate for unseen parts in the generated images, enhancing reconstruction accuracy.\n\n### 2.3. Advantages and Potential Limitations of Hunyuan3D\n\nAdvantages:\n- Fast 3D generation: Hunyuan3D can create high-quality 3D assets in just 10 seconds, significantly reducing generation time compared to optimization-based methods.\n- Improved generalization: By disentangling single-view generation tasks into multi-view image generation and sparse-view reconstruction, Hunyuan3D achieves better generalization to unseen objects.\n- Unified framework: Hunyuan3D supports both text- and image-conditioned 3D generation, making it a versatile tool for various applications.\n\nPotential Limitations:\n- Memory requirements: The standard version of Hunyuan3D has 3x more parameters than the lite version, which may require more memory for optimal performance.\n- Thin structure generation: Like other feed-forward methods, Hunyuan3D might struggle with generating thin, paper-like structures.\n\n### 2.4. How to use Hunyuan3D-1.0 Workflow in ComfyUI\n\nHere's a step-by-step guide for using Hunyuan3D workflow to generate high-quality 3D meshes from single images\n\nLoad the Hunyuan3D multi-view diffusion model using the \"[Comfy3D] Load Diffusers Pipeline\" node. Choose between the lite or standard version based on your GPU memory.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1165/readme01.webp\" alt=\"ComfyUI Hunyuan3D Workflow\" width=\"450\"/>\n\nLoad the Hunyuan3D reconstruction model using the \"[Comfy3D] Load Hunyuan3D V1 Reconstruction Model\" node.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1165/readme02.webp\" alt=\"ComfyUI Hunyuan3D Workflow\" width=\"450\"/>\n\nPrepare the input image and mask using the \"LoadImage\" and \"InvertMask\" nodes.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1165/readme03.webp\" alt=\"ComfyUI Hunyuan3D Workflow\" width=\"450\"/>\n\nFeed the loaded diffusion pipeline, input image, and mask into the \"[Comfy3D] Hunyuan3D V1 MVDiffusion Model\" node to generate multi-view images and a condition image.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1165/readme04.webp\" alt=\"ComfyUI Hunyuan3D Workflow\" width=\"600\"/>\n\nPreview the generated multi-view images using the \"PreviewImage\" node.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1165/readme05.webp\" alt=\"ComfyUI Hunyuan3D Workflow\" width=\"450\"/>\n\nInput the loaded reconstruction model, generated multi-view images, and condition image into the \"[Comfy3D] Hunyuan3D V1 Reconstruction Model\" node to create the 3D mesh. Also, you can adjust the mesh's axis and scale using the \"[Comfy3D] Switch Mesh Axis\" node if needed.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1165/readme06.webp\" alt=\"ComfyUI Hunyuan3D Workflow\" width=\"750\"/>\n\n(Optional) Convert vertex colors to textures using the \"[Comfy3D] Convert Vertex Color To Texture\" node for improved texture quality.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1165/readme07.webp\" alt=\"ComfyUI Hunyuan3D Workflow\" width=\"600\"/>\n\nNow you can unlock the full potential of Hunyuan3D to create stunning 3D assets from a single image. The Hunyuan3D model makes advanced 3D generation more accessible than ever!\n"
    },
    {
        "id": "1166",
        "readme": "This guide provides a comprehensive introduction to creating 3D content with the \"Era3D\" model, utilizing the advanced capabilities of \"ComfyUI 3D Pack\" nodes to streamline workflows and enhance output quality.\n\n## Part 1: ComfyUI 3D Pack\n\n### 1.1. Introduction to ComfyUI 3D Pack\n\nComfyUI 3D Pack is an extensive node suite that enables ComfyUI to process 3D inputs such as Mesh & UV Texture using cutting edge algorithms and models. It integrates advanced 3D processing algorithms like 3DGS (Gaussian Splatting) and NeRF (Neural Radiance Fields), along with state-of-the-art models including Hunyuan3D**,** StableFast3D, InstantMesh, CRM, TripoSR and others.\n\nWith the ComfyUI 3D Pack, users can import, manipulate and generate high quality 3D content within the intuitive ComfyUI interface. It supports a wide range of 3D file formats like OBJ, PLY, GLB enabling easy integration of existing 3D models. The pack also includes powerful mesh processing utilities to edit, clean, and optimize 3D geometry.\n\nOne of the key highlights is the integration of NeRF technology which allows photorealistic 3D reconstruction from 2D images. The 3DGS nodes enable point cloud rendering and stylization. InstantMesh and TripoSR models allow high-resolution upscaling and super-resolution of 3D meshes. CRM (Convolutional Reconstruction Model) enables recovering 3D shape from multi-view images and CCM (Color Correction Map).\n\nComfyUI 3D Pack was developed by [MrForExample](https://github.com/MrForExample), with all credit going to MrForExample. For detailed information, please see [ComfyUI 3D Pack](https://github.com/MrForExample/ComfyUI-3D-Pack/tree/main).\n\n### 1.2. ComfyUI 3D Pack: Ready to Run on RunComfy\n\nNow ComfyUI 3D Pack is fully setup and ready to use on the RunComfy website. Users don't need to install any additional software or dependencies. All the required models, algorithms and tools are pre-configured and optimized to run efficiently in the web-based ComfyUI environment.\n\n## Part 2: Using Era3D model with ComfyUI 3D Pack Nodes\n\n### 2.1. What is Era3D?\n\nEra3D is a state-of-the-art multiview diffusion method that generates high-quality, high-resolution multiview images from a single input image. It addresses several limitations of existing multiview generation methods, such as inconsistent camera priors, inefficient multiview attention, and low output resolution. Era3D achieves superior 3D reconstruction quality compared to baseline methods.\n\nEra3D was developed by a team of researchers from HKUST, HKU, DreamTech, PKU, and LightIllusion. All credit goes to their contributions and advancements in the field of multiview diffusion and 3D reconstruction. For detailed information, please see [**Era3D**](https://github.com/pengHTYX/Era3D)\n\n### 2.2. Techniques behind Era3D\n\n1. Diffusion-based camera prediction module: Era3D estimates the focal length and elevation of the input image, allowing it to generate images without shape distortions even when the input image is captured by cameras with different intrinsics.\n2. Row-wise multiview attention: Era3D introduces an efficient attention layer that enforces epipolar priors in the multiview diffusion process. By aligning epipolar lines with image rows in the canonical camera setting, Era3D significantly reduces computation complexity and memory consumption compared to dense multiview attention.\n3. High-resolution output: Era3D can generate multiview images with a resolution of up to 512×512, enabling the reconstruction of more detailed 3D meshes.\n\n### 2.3. Advantages and Potential Limitations of Era3D\n\nAdvantages:\n- Handles input images with arbitrary camera intrinsics and viewpoints\n- Efficient row-wise multiview attention for faster training and inference\n- Generates high-resolution (512×512) multiview images for detailed 3D reconstruction\n- Achieves state-of-the-art performance in single-view 3D reconstruction\n\nPotential Limitations:\n- Struggles with generating intricate geometries like thin structures due to the limited number of generated views\n- Cannot reconstruct meshes with open surfaces due to the use of Neural SDF for reconstruction\n\n### 2.4. How to use Era3D Workflow in ComfyUI\n\nThe ComfyUI 3D Pack provides a seamless integration of the Era3D model, allowing users to generate high-quality 3D meshes from a single input image. The workflow consists of the following steps:\n\nLoad the pre-trained Era3D diffusion pipeline using the \"[Comfy3D] Load Diffusers Pipeline\" node.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1166/readme01.webp\" alt=\"ComfyUI Era3D Workflow\" width=\"450\"/>\n\nLoad the input image and its corresponding mask using the \"LoadImage\" node. Invert the mask using the \"InvertMask\" node to ensure proper background handling.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1166/readme02.webp\" alt=\"ComfyUI Era3D Workflow\" width=\"450\"/>\n\nUse the \"[Comfy3D] Era3D MVDiffusion Model\" node to generate multiview images, normal maps, and orbit camera poses from the input image and mask. This node utilizes the pre-trained Era3D model to generate high-resolution, consistent multiview outputs.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1166/readme03.webp\" alt=\"ComfyUI Era3D Workflow\" width=\"750\"/>\n\nUse the \"[Comfy3D] InstantMesh Reconstruction Model\" node to reconstruct the 3D mesh from the generated multiview images and orbit camera poses. This node leverages the power of InstantMesh to create a high-quality 3D mesh. Adjust the mesh orientation using the \"[Comfy3D] Switch Mesh Axis\" node to ensure the correct alignment of the 3D model if needed.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1166/readme04.webp\" alt=\"ComfyUI Era3D Workflow\" width=\"750\"/>\n\n---\n\nBy leveraging the Era3D model and the ComfyUI 3D Pack nodes, users can easily generate high-quality 3D meshes from a single input image. The integration of Era3D into the ComfyUI enables a powerful and efficient workflow for single-view 3D reconstruction.\n"
    },
    {
        "id": "1168",
        "readme": "This guide provides a comprehensive introduction to creating 3D content with the \"Wonder3D\" model, utilizing the advanced capabilities of \"ComfyUI 3D Pack\" nodes to streamline workflows and enhance output quality.\n\n## Part 1: ComfyUI 3D Pack\n\n### 1.1. Introduction to ComfyUI 3D Pack\n\nComfyUI 3D Pack is an extensive node suite that enables ComfyUI to process 3D inputs such as Mesh & UV Texture using cutting edge algorithms and models. It integrates advanced 3D processing algorithms like 3DGS (Gaussian Splatting) and NeRF (Neural Radiance Fields), along with state-of-the-art models including Hunyuan3D**,** StableFast3D, InstantMesh, CRM, TripoSR and others.\n\nWith the ComfyUI 3D Pack, users can import, manipulate and generate high quality 3D content within the intuitive ComfyUI interface. It supports a wide range of 3D file formats like OBJ, PLY, GLB enabling easy integration of existing 3D models. The pack also includes powerful mesh processing utilities to edit, clean, and optimize 3D geometry.\n\nOne of the key highlights is the integration of NeRF technology which allows photorealistic 3D reconstruction from 2D images. The 3DGS nodes enable point cloud rendering and stylization. InstantMesh and TripoSR models allow high-resolution upscaling and super-resolution of 3D meshes. CRM (Convolutional Reconstruction Model) enables recovering 3D shape from multi-view images and CCM (Color Correction Map).\n\nComfyUI 3D Pack was developed by [MrForExample](https://github.com/MrForExample), with all credit going to MrForExample. For detailed information, please see [ComfyUI 3D Pack](https://github.com/MrForExample/ComfyUI-3D-Pack/tree/main).\n\n### 1.2. ComfyUI 3D Pack: Ready to Run on RunComfy\n\nNow ComfyUI 3D Pack is fully setup and ready to use on the RunComfy website. Users don't need to install any additional software or dependencies. All the required models, algorithms and tools are pre-configured and optimized to run efficiently in the web-based ComfyUI environment.\n\n## Part 2: Using Wonder3D model with ComfyUI 3D Pack Nodes\n\n### 2.1. What is Wonder3D?\n\nWonder3D is a cutting-edge method for efficiently generating high-quality textured meshes from single-view images. It leverages the power of cross-domain diffusion models to generate multi-view normal maps and their corresponding color images. Wonder3D aims to address the challenges of fidelity, consistency, generalizability, and efficiency in single-view 3D reconstruction tasks.\n\nWonder3D was developed by a team of researchers from The University of Hong Kong, Tsinghua University, VAST, University of Pennsylvania, Shanghai Tech University, MPI Informatik, and Texas A&M University, with Xiaoxiao Long and Yuan-Chen Guo as equal contribution first authors. All credit goes to their contribution; for more information, please see their project page at [here](https://github.com/xxlong0/Wonder3D).\n\n### 2.2. Techniques behind Wonder3D\n\nThe core of Wonder3D lies in its innovative cross-domain diffusion model. This model is designed to capture the joint distribution of normal maps and color images across multiple views. To achieve this, Wonder3D introduces a domain switcher and a cross-domain attention scheme. The domain switcher allows seamless generation of either normal maps or color images, while the cross-domain attention mechanism facilitates information exchange between the two domains, enhancing consistency and quality.\n\nAnother key component of Wonder3D is its geometry-aware normal fusion algorithm. This algorithm robustly extracts high-quality surfaces from the generated multi-view 2D representations, even in the presence of inaccuracies. By leveraging the rich surface details encoded in the normal maps and color images, Wonder3D reconstructs clean and detailed geometries.\n\n### 2.3. Advantages/Potential Limitations of Wonder3D\n\nWonder3D offers several advantages over existing single-view reconstruction methods. It achieves a high level of geometric detail while maintaining good efficiency, making it suitable for various applications. The cross-domain diffusion model allows Wonder3D to generalize well to different object categories and styles. The multi-view consistency enforced by the cross-domain attention mechanism results in coherent and plausible 3D reconstructions.\n\nHowever, like any method, Wonder3D may have some limitations. The quality of the generated meshes depends on the training data and the ability of the diffusion model to capture the underlying 3D structure. Highly complex or ambiguous shapes might pose challenges. Additionally, the current implementation of Wonder3D focuses on single objects, and extending it to handle multiple objects or entire scenes could be an area for future research.\n\n### 2.4. How to use Wonder3D Workflow\n\nLoad the pre-trained Wonder3D diffusion pipeline using the \"[Comfy3D] Load Diffusers Pipeline\" node, which imports the necessary model checkpoints and configurations.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1168/readme01.webp\" alt=\"ComfyUI Wonder3D workflow\" width=\"450\"/>\n\nProvide an input image and its corresponding mask using the \"LoadImage\" and \"InvertMask\" nodes. Then feed the input image and mask into the \"[Comfy3D] Wonder3D MVDiffusion Model\" node, which generates multi-view normal maps and color images. \n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1168/readme02.webp\" alt=\"ComfyUI Wonder3D workflow\" width=\"750\"/>\n\nProcess the generated multi-view images using the \"[Comfy3D] Large Multiview Gaussian Model\" node, which converts them into a 3D Gaussian Splatting (3DGS) representation, capturing the object's geometric details in a point cloud format.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1168/readme03.webp\" alt=\"ComfyUI Wonder3D workflow\" width=\"750\"/>\n\nTransform the 3DGS representation into a textured mesh using the \"[Comfy3D] Convert 3DGS to Mesh with NeRF and Marching Cubes\" node, which employs neural radiance fields (NeRF) and marching cubes algorithms to extract a high-quality mesh.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1168/readme04.webp\" alt=\"ComfyUI Wonder3D workflow\" width=\"750\"/>\n"
    },
    {
        "id": "1169",
        "readme": "The [Flux Fill Models](https://blackforestlabs.ai/flux-1-tools/) and nodes and its associated workflow are fully developed by Blackforest Labs. We give all due credit to Blackforest Labs for this innovative work. On the RunComfy platform, we are simply presenting Blackforest Labs' contributions to the community. It is important to note that there is currently no formal connection or partnership between RunComfy and Blackforest Labs. We deeply appreciate Blackforest Labs' work!\n\nThe Flux Tools - Flux Fill workflow enables you to manipluate a photo to Inpaint subject's cloths, background, add or remove elements and much more. And with Outpaint you can grow zoom out of the image canvas and generate content based on prompt.  \n\n\n## Flux Fill - Inpainting and Outpainting\n\nFlux Fill Cutting-edge inpainting and outpainting models enable accurate image editing and expansion, smoothly filling in gaps or extending an image past its original limits. Using sophisticated AI technology, Flux Fill adjust both real and AI-generated images according to a provided text description and mask, ensuring that the changes blend seamlessly with the existing image. By comprehending the context and intricate details, Flux Fill offers greater creative freedom in generating and modifying visual content.\n\n\n## 1.1 How to Use Flux Fill - Inpainting and Outpainting Workflow?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1169/readme01.webp\" alt=\"Flux Fill Workflow\" width=\"750\"/>\n</p>\n\nHow to Use Instructions: \n\n1) Upload your Images in Inpaint or Outpaint Group.\n2) Enter your Prompts\n3) Click Queue\n\nNo need to setup anything, Rendered as simple as that.\n\n\n## 1.2 Flux Fill - Inpainting Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1169/readme02.webp\" alt=\"Flux Fill Inpaint\" width=\"750\"/>\n</p>\n\n- Upload your image \n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1169/readme03.webp\" alt=\"Flux Fill Masking\" width=\"450\"/>\n</p>\n\n- Right click on the node and Click \"open in mask Editor\" \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1169/readme04.webp\" alt=\"Flux Fill Masking\" width=\"450\"/>\n</p>\n\n- Draw your mask and click save to node\n- Optional - Upscale Image if you feel the Masked area is very small to get proper details.\n\n\n\n## 1.3 Flux Fill - Outpainting Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1169/readme05.webp\" alt=\"Flux Fill Outpaint\" width=\"750\"/>\n</p>\n\n- Unmute the Nodes using Ctrl + B and\n- Upload your image in the Load Image Node.\n- Set Outpainting - Padding Value and side you want to expand image.\n- Set Mask Blur and Expand if seams are visible\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1169/readme06.webp\" alt=\"Flux Fill Reroute\" width=\"450\"/>\n</p>\n- Connect the reroute nodes to activate the outpainting pipline\n\n## 1.4 Prompts\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1169/readme07.webp\" alt=\"Flux Fill Prompt\" width=\"750\"/>\n</p>\n\n- Prompt: Add your prompts based on your the outcome you want for inpainting or Outpainting \n\n## 1.5 KSampler\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1169/readme08.webp\" alt=\"Flux Fill Ksampler\" width=\"750\"/>\n</p>\n\n### Ksampler:\n\n- `seed`: Controls randomness in the image enhancement process, allowing reproducible results when using the same seed.\n- `steps`: The number of iterations for enhancing details. More steps result in finer details but require more processing time.\n- `cfg:` The Classifier-Free Guidance scale, which adjusts how closely the model follows the input guidance.\n- `sampler_name`: Defines the sampling method used for detail refinement.\n- `scheduler`: Determines the computational scheduling strategy during processing.\n\n\n### Model\n[Flux Dev Model](https://github.com/black-forest-labs/flux) is used in this which will take about 5-10 mins to make a local copy of the model (23 GB) to your Machine. \n\n\n--- \n\nFlux Fill is a cutting-edge tool for image editing, enabling flawless inpainting and outpainting driven by text prompts and binary masks. This technology empowers creators to repair photos, refine AI-generated visuals, and design new extensions that seamlessly blend with existing content. With its advanced precision and context understanding, Flux Fill unlocks limitless potential for artists, designers, and innovators to produce stunning and imaginative creations with ease.\n\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1170",
        "readme": "The [Flux Redux Models](https://blackforestlabs.ai/flux-1-tools/) and nodes and its associated workflow are fully developed by Blackforest Labs. We give all due credit to Blackforest Labs for this innovative work. On the RunComfy platform, we are simply presenting Blackforest Labs' contributions to the community. It is important to note that there is currently no formal connection or partnership between RunComfy and Blackforest Labs. We deeply appreciate Blackforest Labs' work!\n\nThe Flux Tools - Flux Redux workflow enables you to generate variations and stylization for the uploaded image based on prompts.  \n\n\n## Flux Redux - Variation and Restyling\n\nFlux Redux helps you create new versions of any image with small changes. It’s easy to restyle your work using simple prompts. Perfect for refining or exploring creative ideas. Get variations that stay true to your original vision. Flux Redux turns your concepts into endless possibilities!\n\n\n## 1.1 How to Use Flux Redux - Variation and Restyling Workflow?\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1170/readme01.webp\" alt=\"Flux Redux Workflow\" width=\"750\"/>\n</p>\n\nHow to Use Instructions: \n\n1) Upload your Image in load image node.\n2) Enter your Style image and helping Prompt.\n3) Click Queue\n\nNo need to setup anything, Rendered as simple as that.\n\n\n\n## 1.2 Flux Redux - Load Image Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1170/readme02.webp\" alt=\"Flux Redux Load Image\" width=\"750\"/>\n</p>\n\n- Upload your main image ( This will also be used as image dimension for empty latent). \n- Resize Image if needed. See \"Get Image Size and Count\" to check your image dimensions.\n- Upload your Style Image.\n\n## 1.3 Flux Redux - Prompt Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1170/readme03.webp\" alt=\"Flux Redux Prompt\" width=\"750\"/>\n</p>\n\n- Prompt: Add your prompts based on your the outcome you want for stylzation and variations.\n\n## 1.4 Flux Redux Models\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1170/readme04.webp\" alt=\"Flux Redux Model\" width=\"750\"/>\n</p>\n\n[Sig Clip Vision Model](https://huggingface.co/Comfy-Org/sigclip_vision_384/tree/main) and [Flux Redux Model](https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev/tree/main) - which are downloaded which will take about 2-3 mins to make a local copy of the model to your Machine.\n\n- You can chain multiple \"Apply Style Model\" nodes if you want to mix multiple images together.\n\n\n## 1.5 KSampler \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1170/readme05.webp\" alt=\"Ksampler\" width=\"750\"/>\n</p>\n\n### Ksampler:\n\n- `seed`: Controls randomness in the image enhancement process, allowing reproducible results when using the same seed.\n- `steps`: The number of iterations for enhancing details. More steps result in finer details but require more processing time.\n- `cfg:` The Classifier-Free Guidance scale, which adjusts how closely the model follows the input guidance.\n- `sampler_name`: Defines the sampling method used for detail refinement.\n- `scheduler`: Determines the computational scheduling strategy during processing.\n\n\n--- \n\nFlux Redux opens the door to limitless creativity with effortless image variation and restyling. Whether refining details or exploring bold new ideas, it’s your ultimate tool for visual transformation. Let Flux Redux elevate your art and empower your imagination to lead the way!\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1171",
        "readme": "The [Flux Depth and Flux Canny Models](https://blackforestlabs.ai/flux-1-tools/) and nodes and its associated workflow are fully developed by Blackforest Labs. We give all due credit to Blackforest Labs for this innovative work. On the RunComfy platform, we are simply presenting Blackforest Labs' contributions to the community. It is important to note that there is currently no formal connection or partnership between RunComfy and Blackforest Labs. We deeply appreciate Blackforest Labs' work!\n\nThe Flux Tools - Flux Depth and Flux Canny workflow enables you to generate Similar images for your uploaded image based on prompts given and with the help of Canny and Depth Pass.  \n\n\n## Flux Depth and Flux Canny\n\nFlux Depth and Flux Canny models are the ultimate duo for next-level image generation. With Depth unlocking stunning spatial accuracy and dynamic realism through depth maps, and Canny delivering razor-sharp outlines with precision edge detection, these models redefine creative control. Get ready to craft visuals with unmatched depth and striking clarity!\n\n## 1.1 How to Use Flux Depth and Flux Canny Workflow?\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1171/readme01.webp\" alt=\"Workflow\" width=\"750\"/>\n</p>\n\nHow to Use Instructions: \n\n1) Upload your Image in load image node.\n2) Enter your Image and helping Prompt.\n3) Choose Depth or Canny Settings.\n4) Click Queue\n\nNo need to setup anything, Rendered as simple as that.\n\n\n\n## 1.2 Flux Depth and Flux Canny - Load Image Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1171/readme04.webp\" alt=\"Load Image\" width=\"750\"/>\n</p>\n\n- Upload your main image ( This will also be used as image dimension for empty latent). \n- Resize Image if needed. See \"Get Image Size and Count\" to check your image dimensions.\n\n\n## 1.3 Flux Depth and Flux Canny - Prompt Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1171/readme05.webp\" alt=\"Prompt\" width=\"750\"/>\n</p>\n\n- Prompt: Add your prompts based on your the outcome you want for stylzation and variations.\n\n## 1.4 Flux Depth and Flux Canny - Settings\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1171/readme03.webp\" alt=\"Model\" width=\"550\"/>\n</p>\n\nPick One model from the Load Diffusion model node, which you want to use - Depth or Canny\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1171/readme02.webp\" alt=\"Model\" width=\"550\"/>\n</p>\n\nThen According to the Model selected, Unmute the respective Group and connect the reroute to the top reroute node.\n\n-- \n\n[Flux.1 Depth Model](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev/tree/main) and [Flux.1 Canny Model ](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/tree/main)  - which are downloaded which will take about 4-5 mins to make a local copy of the model (23 GB) to your Machine.\n\n## 1.5 KSampler \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1171/readme06.webp\" alt=\"Ksampler\" width=\"750\"/>\n</p>\n\n### Ksampler:\n\n- `seed`: Controls randomness in the image enhancement process, allowing reproducible results when using the same seed.\n- `steps`: The number of iterations for enhancing details. More steps result in finer details but require more processing time.\n- `cfg:` The Classifier-Free Guidance scale, which adjusts how closely the model follows the input guidance.\n- `sampler_name`: Defines the sampling method used for detail refinement.\n- `scheduler`: Determines the computational scheduling strategy during processing.\n\n\n--- \n\n\nFlux Depth and Flux Canny bring unmatched precision and creativity to image generation, combining dynamic realism with sharp structural guidance. Elevate your visuals with tools designed to redefine what's possible in digital artistry.\n\n## License\n\nView license files:\n\n[flux/model_licenses/LICENSE-FLUX1-dev](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)\n\n[flux/model_licenses/LICENSE-FLUX1-schnell](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)\n\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\n\nIN NO EVENT SHALL BLACK FOREST LABS, INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.\n"
    },
    {
        "id": "1172",
        "readme": "EchoMimic is a tool for generating lifelike audio-driven portrait animations. It utilizes deep learning techniques to analyze input audio and generate corresponding facial expressions, lip movements, and head gestures that closely match the emotional and phonetic content of the speech.\n\nEchoMimic V2 was developed by a team of researchers from the Terminal Technology Department at Alipay, Ant Group, including Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma. For detailed information, please visit [antgroup](https://github.com/antgroup)/[echomimic_v2](https://github.com/antgroup/echomimic_v2). The ComfyUI_EchoMimic node was developed by [smthemex](https://github.com/smthemex)/[ComfyUI_EchoMimic](https://github.com/smthemex/ComfyUI_EchoMimic). All credit goes to their significant contribution.\n\n## EchoMimic V1 and V2\n\n- EchoMimic V1: Realistic Audio-Driven Portrait Animations with Customizable Landmark Control\n- EchoMimic V2: Simplified, Expressive, and Semi-Body Human Animations\n\nThe key difference is that EchoMimic V2 aims to achieve striking half-body human animation while simplifying unnecessary control conditions compared to EchoMimic V1. EchoMimic V2 uses a novel Audio-Pose Dynamic Harmonization strategy to enhance facial expressions and body gestures.\n\n## Strengths and Weaknesses of EchoMimic V2\n\nStrengths:\n\n- EchoMimic V2 generates highly realistic and expressive portrait animations driven by audio\n- EchoMimic V2 extends animation to the upper body, not just the head region\n- EchoMimic V2 reduces condition complexity while maintaining animation quality compared to EchoMimic V1\n- EchoMimic V2 seamlessly incorporates headshot data to enhance facial expressions\n\nWeaknesses:\n\n- EchoMimic V2 requires an audio source matched to the portrait for best results\n- EchoMimic V2 currently lacks pose synchronization code, using a default pose file\n- Generating longer high-quality animations with EchoMimic V2 can be computationally intensive\n- EchoMimic V2 works best on cropped portrait images rather than full-body shots\n\n## How to Use the ComfyUI EchoMimic Workflow\n\nIn the \"Echo_LoadModel\" node, you have the option to select between EchoMimic v1 and EchoMimic v2:\n\n1. EchoMimic v1: This version focuses on generating realistic audio-driven portrait animations with the ability to customize landmark control. It is well-suited for creating lifelike facial animations that closely match the input audio.\n2. EchoMimic v2: This version aims to simplify the animation process while delivering expressive and semi-body human animations. It extends the animation beyond just the facial region to include upper body movements. **However, please note that the pose synchronization feature for v2 is not yet implemented in the current version of the ComfyUI workflow. If you select 'None' for the pose path, the default official pose file will be used instead.**\n\nHere is a step-by-step guide on using the provided ComfyUI workflow:\n\nStep 1. Load your portrait image using the LoadImage node. This should be a close-up shot of the subject's head and shoulders.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1172/readme01.webp\" alt=\"EchoMimic\" width=\"400\"/>\n\nStep 2. Load your audio file using the LoadAudio node. The speech in the audio should match the identity of the portrait subject.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1172/readme02.webp\" alt=\"EchoMimic\" width=\"400\"/>\n\nStep 3. Use the Echo_LoadModel node to load the EchoMimic model. Key settings:\n    - Choose the version (V1 or V2).\n    - Select the inference mode, e.g. audio-driven mode.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1172/readme03.webp\" alt=\"EchoMimic\" width=\"400\"/>\n\nStep 4. Connect the image, audio, and loaded model to the Echo_Sampler node. Key settings:\n    - pose_dir: The directory path for the pose sequence files used in pose-driven animation modes. If set to \"none\", no pose sequence will be used.\n    - seed: The random seed for generating consistent results across runs. It should be an integer between 0 and MAX_SEED.\n    - cfg: The classifier-free guidance scale, controlling the strength of the audio conditioning. Higher values result in more pronounced audio-driven movements. The default value is 2.5, and it can range from 0.0 to 10.0.\n    - steps: The number of diffusion steps for generating each frame. Higher values produce smoother animations but take longer to generate. The default is 30, and it can range from 1 to 100.\n    - fps: The frame rate of the output video in frames per second. The default is 25, and it can range from 5 to 100.\n    - sample_rate: The sample rate of the input audio in Hz. The default is 16000, and it can range from 8000 to 48000 in increments of 1000.\n    - facemask_ratio: The ratio of the face mask area to the full image area. It controls the size of the region around the face that is animated. The default is 0.1, and it can range from 0.0 to 1.0.\n    - facecrop_ratio: The ratio of the face crop area to the full image area. It determines how much of the image is dedicated to the face region. The default is 0.8, and it can range from 0.0 to 1.0.\n    - context_frames: The number of past and future frames to use as context for generating each frame. The default is 12, and it can range from 0 to 50.\n    - context_overlap: The number of overlapping frames between adjacent context windows. The default is 3, and it can range from 0 to 10.\n    - length: The length of the output video in frames. It should be based on the duration of your input audio and the fps setting. For example, if your audio is 6 seconds long and the fps is set to 25, the length should be 150 frames. The length can range from 50 to 5000 frames.\n    - width: The width of the output video frames in pixels. The default is 512, and it can range from 128 to 1024 in increments of 64.\n    - height: The height of the output video frames in pixels. The default is 512, and it can range from 128 to 1024 in increments of 64.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1172/readme04.webp\" alt=\"EchoMimic\" width=\"400\"/>\n\nPlease note that video generation may take some time. For instance, creating a video from a 6-second audio clip using a 2XL machine on RunComfy takes about 20 minutes.\n"
    },
    {
        "id": "1173",
        "readme": "LTX Video is a diffusion-based video generation model developed by Lightricks. It is capable of generating videos from either text prompts (text-to-video) or a combination of image and text prompts (image+text-to-video). LTX Video produces 24 frames per second (FPS) videos at a resolution of 768x512 faster than they can be watched. The model has been trained on a large-scale dataset containing diverse videos, enabling it to generate realistic and varied video content at high resolutions.\n\nLTX Video Model and ComfyUI-LTXVideo Nodes were developed by Lightricks. All credit goes to their work in creating LTX Video. For more information about LTX Video and Lightricks' projects, please visit their GitHub repository at https://github.com/Lightricks/LTX-Video or their website at https://www.lightricks.com/ltxv.\n\n## Techniques behind LTX Model\n\nLTX Video utilizes a Diffusion-based approach for generating videos. Diffusion models work by gradually denoising a noisy input over multiple timesteps to generate the final output. In the case of LTX Video, the model takes a noisy latent representation as input and iteratively denoises it to produce a sequence of video frames. The denoising process is guided by the provided text or image+text prompts, which control the content and style of the generated video.\n\nThe key techniques employed by LTX Video include:\n\n- Diffusion-based video generation: By leveraging diffusion models, LTX Video can generate high-quality videos with realistic motion and consistency across frames.\n- Text-to-video synthesis: LTX Video can generate videos solely based on textual descriptions, enabling users to create custom videos from scratch using natural language prompts.\n- Image+text-to-video synthesis: LTX Video also supports generating videos by combining an initial image with a text prompt. This allows users to provide a starting point for the video and guide its content and style using text.\n\n## How to Use LTX Video Workflow in ComfyUI\n\n1. Prepare the Input:\n    - The default workflow is image + text-to-video generation. Provide an initial image along with a text prompt. The image serves as a starting point, and the model will generate a video based on both the image and the accompanying text. Note that this model requires long, descriptive prompts; if the prompt is too short, the quality will suffer greatly.\n2. Configure the Model Parameters:\n    - Set the desired resolution and number of frames for the generated content. The resolution should be divisible by 32, and the number of frames should be divisible by 8 + 1 (e.g., 257 frames). LTX works best with resolutions under 720x1280 pixels and fewer than 257 frames.\n    - Adjust other parameters such as the diffusion steps, noise schedule, and guidance scale according to your requirements. These parameters control the quality and diversity of the generated output.\n3. Generate the Content:\n    - The output will have the specified resolution and number of frames, and it will align with the provided input prompt.\n\n## Limitations of LTX Model\n\n- LTX Video is not intended or able to provide factual information.\n- As a statistical model, LTX Video might amplify existing societal biases present in the training data.\n- The generated videos may not perfectly match the provided prompts.\n- The quality of prompt following heavily depends on the prompting style used.\n\n## License\n\nPlease use the model for purposes under the [**license**](https://github.com/Lightricks/LTX-Video/blob/main/LICENSE)\n"
    },
    {
        "id": "1174",
        "readme": "## 1. What is the ComfyUI ReActor Face Swap Workflow?\n\nThe ComfyUI ReActor Face Swap workflow enables seamless face swapping in images and videos using advanced face detection, swapping, and enhancement models. This ReActor Face Swap implementation produces natural, high-quality results while providing flexible restoration and upscaling options.\n\n### Latest Updates\n\nRecent updates have brought significant enhancements to ReActor Face Swap workflow. Support for GPEN 1024/2048 restoration models improves face quality, while updates to video processing and overall performance optimization enable faster face-swapping operations.\n\n## 2. Benefits of ComfyUI ReActor Face Swap:\n* Produces realistic face replacements while maintaining natural facial expressions, lighting, and texture\n* Offers flexible restoration features with customizable enhancement settings\n* Includes upscaling support to improve image and video resolution for clearer outputs\n* Delivers smooth video outputs using frame interpolation for fluid motion\n\n## 3. How to Use the ComfyUI ReActor Face Swap Workflow\n\n### 3.1 Generation Methods with ComfyUI ReActor Face Swap\n\n#### Primary Method: Video-to-Image Face Replacement\n* Inputs:\n  * Target Video: Loaded via the Load Video (Upload) node\n  * Source Image: Loaded via the Load Image node\n* Procedure:\n  * Run the ReActor Face Swap workflow by clicking Queue Prompt for high-quality results\n  * The final video will be saved using the Video Combine node\n\n#### Alternative Methods\nThe ReActor Face Swap workflow supports both multi-face and image-only processing. For multiple face swap, adjust the `input_faces_index` parameter in the Fast Face Swap node to target specific faces (e.g., 0,1,2). For image-only operations, simply use the Load Image node instead of Load Video (Upload) for your target input.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1174/readme01.webp\" alt=\"ReActor Face Swap\" width=\"350\"/>\n\n## 4. Nodes and Parameters Reference of ComfyUI ReActor Face Swap \n\n### Load Video (Upload) Node\n* **Purpose**: Loads the target video for face swapping\n* **Key Parameters**:\n  * `force_rate`: Adjusts the frame rate for video extraction\n  * `select_every_nth`: Controls frame selection frequency\n    * Lower values: More frames, smoother but longer processing\n    * Higher values: Fewer frames, faster but choppier\n  * `skip_first_frames`: Skips specified number of initial frames\n    * Useful for trimming input video start\n* **Trade-offs**: Balance between video smoothness and processing time\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1174/readme02.webp\" alt=\"ReActor Face Swap\" width=\"350\"/>\n\n### Load Image Node\n* **Purpose**: Loads the source image containing the face to swap\n* **Key Parameters**:\n  * `image`: Path to the source face image\n    * Ensure high-quality, clear face images for best results\n* **Trade-offs**: Image quality directly impacts swap results\n\n### Fast Face Swap Node\n* **Purpose**: Performs the face swapping operation\n* **Key Parameters**:\n  * `swap_model`: Face swapping model selection\n    * inswapper_128.onnx: Default model optimized for quality\n  * `facedetection`: Face detection algorithm choice\n    * retinaface_resnet50: High accuracy, slower\n    * retinaface_mobile0.25: Faster, lower precision\n  * `face_restore_visibility`: Restoration blend strength (0.1–1.0)\n    * Higher: Stronger restoration, potential over-smoothing\n    * Lower: Subtle restoration, preserves original details\n  * `codeformer_weight`: Detail preservation balance (0.0–1.0)\n    * Higher: Smoother results, less detail\n    * Lower: More details, potential artifacts\n  * `input_faces_index`: Specifies faces to swap in multi-face processing\n* **Trade-offs**: Balance between quality, speed, and accuracy\n\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1174/readme03.webp\" alt=\"ReActor Face Swap\" width=\"350\"/>\n\n### Face Booster Node\n* **Purpose**: Enhances and restores swapped face before integration\n* **Key Parameters**:\n  * `interpolation`: Enhancement method selection (e.g., Bicubic)\n  * `visibility`: Restoration intensity (0.1–1.0)\n    * Higher: Stronger enhancement but may over-process\n    * Lower: Subtle enhancement, more natural look\n  * `codeformer_weight`: Detail balance (0.0–1.0)\n    * Higher: Smoother, less detail\n    * Lower: More detail, possible artifacts\n* **Trade-offs**: Enhanced quality vs natural appearance\n  \n<img src=\"https://cdn.runcomfy.net/workflow_assets/1174/readme04.webp\" alt=\"ReActor Face Swap\" width=\"350\"/>\n\n### Load Upscale Model Node\n* **Purpose**: Loads AI upscaling model for resolution enhancement\n* **Key Parameters**:\n  * `model_name`: Upscaling model selection (e.g., RealESRGAN_x4plus.pth)\n* **Trade-offs**: Higher resolution vs memory usage\n\n### Upscale Image Node\n* **Purpose**: Applies upscaling to processed images\n* **Key Parameters**:\n  * `upscale_model`: Selected model application\n  * `image`: Target for upscaling\n* **Trade-offs**: Quality improvement vs processing time\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1174/readme05.webp\" alt=\"ReActor Face Swap\" width=\"350\"/>\n\n### Video Combine Node\n* **Purpose**: Combines processed frames into final video\n* **Key Parameters**:\n  * `frame_rate`: Output video frame rate\n    * Higher: Smoother playback, more frames needed\n    * Lower: Choppier but faster processing\n  * `filename_prefix`: Custom prefix for output file\n  * `save_output`: Controls video saving to specified path\n* **Trade-offs**: Output quality vs file size\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1174/readme06.webp\" alt=\"ReActor Face Swap\" width=\"350\"/>\n\n## 5. More Information\n\nFor updates, model details, and ReActor Face Swap workflow support, please visit the official GitHub Repository https://github.com/Gourieff/comfyui-reactor-node.\n"
    },
    {
        "id": "1175",
        "readme": "The [ComfyUI-CogVideoXWrapper ](https://github.com/kijai/ComfyUI-CogVideoXWrapper)nodes and its associated workflow are fully developed by Kijai. We give all due credit to Kijai for this innovative work. On the RunComfy platform, we are simply presenting Kijai’s contributions to the community. It is important to note that there is currently no formal connection or partnership between RunComfy and Kijai. We deeply appreciate Kijai’s work!\n\n## CogVideoX Fun \n\nCogVideoX Fun is a cutting-edge pipeline for creating AI-generated images and videos with unmatched flexibility. Powered by Diffusion Transformer technology, it allows users to produce stunning visuals and achieve unique style transformations, making it an essential tool for creative innovation.\n\n\n## 1.1 How to Use CogVideoX Fun Workflow?\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1175/readme01.webp\" alt=\"CogVideoX Fun\" width=\"750\"/>\n</p>\n\nThis is the CogVideoX Fun workflow, Left Side nodes are inputs for Pose Video or Single Pose, Middle are processing CogVideoX Fun nodes, and Blue right is the outputs node.\n- Drag and drop your Video or Image in the input nodes.\n- Write your video generation prompts and set resolution and frames.\n- Click Render !!! \n\n\n## 1.2 Pose Video Input Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1175/readme02.webp\" alt=\"CogVideoX Fun\" width=\"750\"/>\n</p>\n\n- Click and Upload your Pose Reference Video.\n\nMake sure to connect the reroute to when using this group. \n\n\n## 1.3 Single Pose Image Input Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1175/readme03.webp\" alt=\"CogVideoX Fun\" width=\"750\"/>\n</p>\n\n- Upload, Drag and drop or Copy and Paste (Ctrl+V) your image in the load image node\n\nMake sure to connect the reroute to when using this group. \n\n\n## 1.4 Resizing and Frames \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1175/readme04.webp\" alt=\"CogVideoX Fun\" width=\"750\"/>\n</p>\n\n- Connect the Reroute node of the group you want to use, Video or Single Image OpenPose Image(s) into the resize node\n- Keep Resolution between 512 and 1024, Best results are seen in 768. \n\n<p align=\"center\">\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1175/readme05.webp\" alt=\"CogVideoX Fun\" width=\"500\"/>\n</p>\n\nThis Set the number of frames for rendering the video.\n- Use between 10 - 100. On higher value will require more vram also can give distorted results. \n\n## 1.5 CogVideoX Models\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1175/readme06.webp\" alt=\"CogVideoX Fun\" width=\"750\"/>\n</p>\n\nThese are the model downloader nodes, it will automatically download models in your comfyui in 2-3 mins. \n\n- CogVideoX Fun : https://github.com/aigc-apps/CogVideoX-Fun\n- Reward Lora : https://huggingface.co/alibaba-pai/CogVideoX-Fun-V1.1-Reward-LoRAs/tree/main\n\n\n## 1.6 Prompts\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1175/readme07.webp\" alt=\"CogVideoX Fun\" width=\"750\"/>\n</p>\n\n\n- `Positive`: Enter the video generation prompts for the character.\n- `Negative`: Enter what you don't want to happen (Distorted hands, blurry...etc) \n\n\n--- \n\nCogVideoX Fun is more than a tool—it’s a gateway to limitless creativity. With its advanced capabilities in video generation and custom model training, it enables users to push the boundaries of AI-powered artistry. The platform’s blend of innovation, flexibility, and precision ensures that every project stands out. From style transformations to seamless animation, CogVideoX Fun brings your creative ideas to life with unparalleled ease.\n\n\n"
    },
    {
        "id": "1176",
        "readme": "[Hunyuan Video](https://github.com/Tencent/HunyuanVideo) is an innovative open-source video foundation model that offers performance in video generation comparable to, if not better than, top closed-source models, developed by Tencent, a leading technology company. Hunyuan Video employs cutting-edge technologies for model learning, such as data curation, image-video joint model training, and an efficient infrastructure for large-scale model training and inference. Hunyuan Video boasts the largest open-source video generative model with over 13 billion parameters.\n\n## Key features of Hunyuan Video include\n\n- Hunyuan Video offers a unified architecture for generating both images and videos. It uses a special Transformer model design called \"Dual-stream to Single-stream.\" This means that the model first processes the video and text information separately, and then combines them together to create the final output. This helps the model better understand the relationship between the visuals and the text description.\n- The text encoder in Hunyuan Video is based on a Multimodal Large Language Model (MLLM). Compared to other popular text encoders like CLIP and T5-XXL, MLLM is better at aligning the text with the images. It can also provide more detailed descriptions and reasoning about the content. This helps Hunyuan Video generate videos that more accurately match the input text.\n- To efficiently handle high-resolution and high frame rate videos, Hunyuan Video uses a 3D Variational Autoencoder (VAE) with CausalConv3D. This component compresses the videos and images into a smaller representation called the latent space. By working in this compressed space, Hunyuan Video can train on and generate videos at their original resolution and frame rate without using too much computational resources.\n- Hunyuan Video includes a prompt rewrite model that can automatically adapt the user's input text to better suit the model's preferences. There are two modes available: Normal and Master. The Normal mode focuses on improving the model's understanding of the user's instructions, while the Master mode emphasizes creating videos with higher visual quality. However, the Master mode may sometimes overlook certain details in the text in favor of making the video look better.\n\n## Use Hunyuan Video in ComfyUI\n\nThis [ComfyUI-HunyuanVideoWrapper](https://github.com/kijai/ComfyUI-HunyuanVideoWrapper) nodes and related workflows was developed by Kijai. We give all due credit to Kijai for this innovative work. On the RunComfy platform, we are simply presenting his contributions to the community. \n\n1. Provide your text prompt: In the HunyuanVideoTextEncode node, enter your desired text prompt in the \"prompt\" field. [Here](https://aivideo.hunyuan.tencent.com/) are some prompt examples for your reference.\n2. Configure the output video settings in HunyuanVideoSampler node:\n    - Set the \"width\" and \"height\" to your preferred resolution\n    - Set the \"num_frames\" to the desired video length in frames\n    - \"steps\" controls the number of denoising/sampling steps (default: 30)\n    - \"embedded_guidance_scale\" determines the strength of prompt guidance (default: 6.0)\n    - \"flow_shift\" affects the video length (larger values result in shorter videos, default: 9.0)\n"
    },
    {
        "id": "1177",
        "readme": "## 1. What is the ComfyUI MV-Adapter Workflow?\nThe Multi-View Adapter (MV-Adapter) workflow is a specialized tool that enhances your existing AI image generators with multi-view capabilities. It acts as a plug-and-play addition that enables models like Stable Diffusion XL (SDXL) to understand and generate images from multiple angles while maintaining consistency in style, lighting, and details. Using the MV-Adapter ensures that multi-view image generation is seamless and efficient.\n\n## 2. Benefits of ComfyUI MV-Adapter:\n* Generate high-quality images up to 768px resolution\n* Create consistent multi-view outputs from single images or text\n* Preserve artistic style across all generated angles\n* Works with popular models (SDXL, DreamShaper, Animagine XL)\n* Supports ControlNet for precise control\n* Compatible with LoRA models for enhanced styling\n* Optional SD2.1 support for faster results\n\n## 3. How to Use the ComfyUI MV-Adapter Workflow\n\n### 3.1 Generation Methods with MV-Adapter\n#### Combined Text and Image Generation (Recommended)\n* **Inputs:** Both reference image and text description\n* **Best for:** Balanced results with specific style requirements\n* **Characteristics:**\n  - Combines semantic guidance with reference constraints\n  - Better control over final output\n  - Maintains reference style while following text instructions\n* **Example MV-Adapter workflow:**\n   1. **Prepare inputs:**\n      - Add your reference image in **Load Image** node\n      - Write descriptive text (e.g., *\"a space cat in the style of the reference image\"*) in **Text Encode** node\n      <img src=\"https://cdn.runcomfy.net/workflow_assets/1177/readme01.webp\" alt=\"mv-adapter\" width=\"350\"/>\n      <img src=\"https://cdn.runcomfy.net/workflow_assets/1177/readme02.webp\" alt=\"mv-adapter\" width=\"350\"/>\n   2. **Run workflow** (Queue Prompt) with default settings\n   3. **For further refinement (optional)**:\n      - In **MVAdapter Generator** node: Adjust `shift_scale` for wider/narrower angle range\n      - In **KSampler** node: Modify `cfg` (7–8) to balance between text and image influence\n      <img src=\"https://cdn.runcomfy.net/workflow_assets/1177/readme03.webp\" alt=\"mv-adapter\" width=\"350\"/>\n      <img src=\"https://cdn.runcomfy.net/workflow_assets/1177/readme04.webp\" alt=\"mv-adapter\" width=\"350\"/>\n\n#### Alternative Methods in MV-Adapter:\n\n##### Text-Only Generation\n* Inputs: Text prompt only via Text Encode node\n* Best for: Creative freedom and generating novel subjects\n* Characteristics:\n  - Maximum flexibility in subject creation\n  - Output quality depends on prompt engineering\n  - May have less style consistency across views\n  - Requires detailed prompts for good results\n\n##### Image-Only Generation\n* Inputs: Single reference image via Load Image node\n* Best for: Style preservation and texture consistency\n* Characteristics:\n  - Strong preservation of reference image style\n  - High texture and visual consistency\n  - Limited control over semantic details\n  - May produce abstract results in multi-view scenarios\n\n### 3.2 Parameter Reference for MV-Adapter\n* MVAdapter Generator node:\n  - ```num_views```: 6 (default) - controls number of generated angles\n  - ```shift_mode```: interpolated - controls view transition method\n  - ```shift_scale```: 8 (default) - controls angle range between views\n    \n<img src=\"https://cdn.runcomfy.net/workflow_assets/1177/readme05.webp\" alt=\"mv-adapter\" width=\"350\"/>\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1177/readme06.webp\" alt=\"mv-adapter\" width=\"350\"/>\n\n* KSampler node:\n  - ```cfg```: 7.0-8.0 recommended - balances input influences\n  - ```steps```: 40-50 for more detail (default is optimized for MV-Adapter)\n  - ```seed```: Keep same value for consistent results\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1177/readme07.webp\" alt=\"mv-adapter\" width=\"350\"/>\n \n* LoRA settings (Optional):\n  - 3D LoRA: Apply first for structural consistency\n  - Style LoRA: Add after 3D effect, start at ```0.5``` strength\n \n<img src=\"https://cdn.runcomfy.net/workflow_assets/1177/readme08.webp\" alt=\"mv-adapter\" width=\"350\"/>\n\n### 3.3. Advanced Optimization with MV-Adapter\nFor users seeking performance improvements:\n* VAE Decode node options:\n  - enable_vae_slicing: Reduces VRAM usage\n  - upcast_fp32: Affects processing speed\n\n## More Information\nFor additional details on the MV-Adapter workflow and updates, please visit [MVAdapter](https://github.com/huanngzh/ComfyUI-MVAdapter).\n"
    },
    {
        "id": "1178",
        "readme": "Hunyuan video, an open-source AI model developed by Tencent, allows you to generate stunning and dynamic visuals with ease. The Hunyuan model harnesses advanced architecture and training techniques to understand and generate content of high quality, motion diversity, and stability. \n\n## About Hunyuan Video to Video Workflow\n\nThis Hunyuan Workflow in ComfyUI utilizes the Hunyuan model to create new visual content by combining input text prompts with an existing driving video. Leveraging the capabilities of the Hunyuan model, you can generate impressive video translations that seamlessly incorporate the motion and key elements from the driving video while aligning the output with your desired text prompt. \n\n## How to Use Hunyuan Video to Video Workflow\n\n🟥 Step 1: Load Hunyuan Models\n\n- Load the Hunyuan model by selecting the \"hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors\" file in the HyVideoModelLoader node. This is the main transformer model.\n- The HunyuanVideo VAE model will be automatically downloaded in the HunyuanVideoVAELoader node. It is used for encoding/decoding video frames.\n- Load a text encoder in the DownloadAndLoadHyVideoTextEncoder node. The workflow defaults to using the \"Kijai/llava-llama-3-8b-text-encoder-tokenizer\" LLM encoder and the \"openai/clip-vit-large-patch14\" CLIP encoder, which will be auto-downloaded. You can use other CLIP or T5 encoders that have worked with previous models as well.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1178/readme01.webp\" alt=\"Hunyuan Video to Video Workflow\" width=\"350\"/>\n\n\n🟨 Step 2: Enter Prompt & Load Driving Video\n\n- Enter your text prompt describing the visual you want to generate in the HyVideoTextEncode node.\n- Load the driving video you want to use as the motion reference in the VHS_LoadVideo node.\n    - frame_load_cap: Number of frames to generate. When setting the number, you need to ensure that the number minus one is divisible by 4; otherwise, a ValueError will be triggered, indicating that the video length is invalid.\n    - skip_first_frames: Adjust this parameter to control which part of the video is used.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1178/readme02.webp\" alt=\"Hunyuan Video to Video Workflow\" width=\"350\"/>\n\n🟦 Step 3: Hunyuan Generation Settings\n\n- In the HyVideoSampler node, configure the video generation hyperparameters:\n    - Steps: Number of diffusion steps per frame, higher means better quality but slower generation. Default 30.\n    - Embedded_guidance_scale: How much to adhere to the prompt, higher values stick closer to prompt.\n    - Denoise_strength: Controls strength of using the init driving video. Lower values (e.g. 0.6) make output look more like init.\n- Pick addons and toggles in the \"Fast Groups Bypasser\" node to enable/disable extra features like the comparison video.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1178/readme03.webp\" alt=\"Hunyuan Video to Video Workflow\" width=\"350\"/>\n\n🟩 Step 4: Generate Huanyuan Video\n\n- The VideoCombine nodes will generate and save two outputs by default:\n    - The translated video result\n    - A comparison video showing the driving video and the generated result\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1178/readme04.webp\" alt=\"Hunyuan Video to Video Workflow\" width=\"700\"/>\n\n---\n\nTweaking the prompt and generation settings allows for impressive flexibility in creating new videos driven by the motion of an existing video using the Hunyuan model. Have fun exploring the creative possibilities of this Hunyuan workflow! \n\n---\nThis Hunyuan workflow was designed by Black Mixture. Please visit [Black Mixture's youtube channel](https://www.youtube.com/@BlackMixture/videos) for more information. Also Special thanks to [Kijai for the Hunyuan wrappers](https://github.com/kijai/ComfyUI-HunyuanVideoWrapper/tree/main) nodes and workflow examples.\n"
    },
    {
        "id": "1179",
        "readme": "## **What is the ComfyUI CogVideoX Workflow**\nIt turns your simple video footage into epic cinematic scenes with the ComfyUI CogVideoX Integration Workflow. This workflow leverages the power of CogVideoX, ControlNet, and Live Portrait to deliver professional-grade visuals without requiring expensive equipment. Whether you're an independent filmmaker or a creator exploring AI-driven video production, this tool empowers you to push creative boundaries.\n\n## **Key Features of the CogVideoX Workflow**\n- CogVideoX: Generate detailed cinematic scenes with AI-driven precision.\n- ControlNet Integration: Add pose, edge, and depth guidance for unparalleled control over the transformation process.\n- Live Portrait: Refine acting performances and maintain facial detail consistency.\n- Comparison Output: Analyze generated results side-by-side with original footage for refinement and sharing.\n\n\n## **How to Use the CogVideoX Workflow**\n\n### **1. Main Components of CogVideoX Workflow**\nThe **CogVideoX Workflow** features two primary components designed to deliver exceptional cinematic results:\n1. **Video Generation with CogVideoX:**\n   - This stage processes video frames and integrates **ControlNet guidance**, ensuring precise pose, edge, and depth alignment for professional-grade visuals.\n2. **Live Portrait Integration in CogVideoX:**\n   - Enhances acting performance by refining facial details and maintaining consistency across frames, perfect for scenes requiring detailed expressions.\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1179/readme01.webp\" alt=\"CogVideoX\" width=\"600\"/>\n\n#### **Tips for Using the CogVideoX Workflow Efficiently**\n- **Default Recommendation:**\n   - Begin with the **Video Generation** step to optimize processing time and resources. This is ideal for most cinematic transformations using **CogVideoX**.\n- **When to Use Live Portrait:**\n   - Enable this component if the acting details or facial expressions do not transfer accurately during the initial video generation. It ensures the final output maintains a natural and cohesive appearance.\n\n### **2. Run CogVideoX Workflow**\n1. **Load Your Video:**\n   - Use the Load Video node to upload your footage.\n   - Configure the number of frames for processing:\n     - Default: 48 frames (trained on CogVideoX).\n     - Higher frame counts (e.g., 64) are possible with suitable GPU capabilities.\n     <img src=\"https://cdn.runcomfy.net/workflow_assets/1179/readme02.webp\" alt=\"CogVideoX\" width=\"400\"/>\n2. **Adjust Resolution:**\n   - Set video resolution in the Resize Image node.\n   - Recommended: `Width: 768`, `Height: 432`.\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1179/readme03.webp\" alt=\"CogVideoX\" width=\"350\"/>\n3. **ControlNet Configuration:**\n   - Select the desired guidance mode using the Switch Node:\n     - **Input1**: Original video for image-to-image transformations.\n     - **Input2(Pose)**: Best for capturing character movement and dynamic poses.\n     - **Input3(Canny)**: Ideal for maintaining scene structure through edge detection.\n     - **Input4(Depth)**: Provides spatial consistency, particularly useful for complex backgrounds.\n   - Adjust the ControlNet strength to balance input fidelity and creative freedom.\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1179/readme04.webp\" alt=\"CogVideoX\" width=\"350\"/>\n4. **Write Prompts:**\n   - Use Positive and Negative Prompts to define the scene's aesthetics.\n   - Example Positive Prompt:\n      ```\n      A mystical forest with glowing trees, cinematic style, highly detailed.\n      ```\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1179/readme05.webp\" alt=\"CogVideoX\" width=\"400\"/>\n\n### **3. Video Processing with the CogVideoX Workflow**\n1. **Run Video Generation:**\n   - Disable Live Portrait initially to save processing time.\n   - Queue your prompts and begin processing.\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1179/readme06.webp\" alt=\"CogVideoX\" width=\"350\"/>\n2. **Enable Live Portrait (Optional):**\n   - Use Live Portrait to refine facial details and acting performance.\n   - Add specific expressions (e.g., \"smiling brightly\") to prompts for improved results.\n\n### **4. Comparing Videos with the CogVideoX Workflow**\n- Enable the built-in Comparison Output to evaluate differences between original and generated videos.\n- Use this feature to showcase results or share them on social platforms.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1179/readme07.webp\" alt=\"CogVideoX\" width=\"600\"/>\n\n\n## **Advanced Tips for the CogVideoX Workflow**\n- **Frame Optimization:**\n  - Use \"Every nth Frame\" for longer videos to reduce processing time while interpolating missing frames.\n- **Creative Prompts:**\n  - Experiment with diverse prompts for unique visual styles.\n- **ControlNet Fine-Tuning:**\n  - Adjust ControlNet strength for more creative freedom or stricter adherence to the input video.\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1179/readme08.webp\" alt=\"CogVideoX\" width=\"350\"/>\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1179/readme09.webp\" alt=\"CogVideoX\" width=\"350\"/>\n\n\n## **Example Use Case of the CogVideoX Workflow**\n1. Load a backyard video of someone walking.\n2. Apply the following prompt:\n   ```\n   Transform the backyard into a futuristic cityscape with glowing neon lights, cinematic style.\n   ```\n3. Use ControlNet (Depth) for spatial consistency.\n4. Process the video and refine acting performance with Live Portrait if needed.\n5. Use the comparison feature to showcase the transformation.\n\n\n## **Credits for the CogVideoX Workflow**\nThis remarkable workflow was crafted by Mickmumpitz, whose creativity and technical expertise made this possible. Full credit goes to Mickmumpitz! For a comprehensive tutorial, check out his YouTube channel: [Mickmumpitz](https://www.youtube.com/watch?v=gHI6PjTkBF4&t=435s).\n"
    },
    {
        "id": "1180",
        "readme": "The [ComfyUI-MMAudio](https://github.com/kijai/ComfyUI-MMAudio) nodes and its associated workflow are fully developed by Kijai. We give all due credit to Kijai for this innovative work. On the RunComfy platform, we are simply presenting Kijai’s contributions to the community. It is important to note that there is currently no formal connection or partnership between RunComfy and Kijai. We deeply appreciate Kijai’s work!\n\n## MMAudio\n\nMMAudio is a powerful tool for creating synchronized audio from video and text inputs. It utilizes multimodal joint training to learn from diverse audio-visual and audio-text datasets, ensuring exceptional adaptability. With its advanced synchronization module, it perfectly aligns audio to video frames. MMAudio revolutionizes audio generation, streamlining the process for creators and innovators alike.\n\n\n## 1.1 How to Use MMAudio Workflow?\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1180/readme01.webp\" alt=\"MMAudio\" width=\"750\"/>\n</p>\n\nThis is the MMAudio  workflow, Left Side nodes are inputs for uploading video, Middle is processing MMAudio nodes, and right is the outputs node.\n- Upload your Video in input nodes.\n- Write your audio generation prompts.\n- Click Render !!! \n\n\n## 1.2 Video Input\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1180/readme02.webp\" alt=\"MMAudio\" width=\"750\"/>\n</p>\n\n- Click and Upload your Reference Video.\n\nThe video is set to downscale the video to ?*512 resolution as processing HD Video or longer video may run of out memory.\n\n## 1.3 MMAudio Processing\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1180/readme03.webp\" alt=\"MMAudio\" width=\"750\"/>\n</p>\n\n- `Positive`: Enter the video generation prompts for the audio.\n- `Negative`: Enter what you don't want to hear.\n- `Steps` : More steps may improve audio quality.\n\n\n## 1.4 MMAudio Models\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1180/readme04.webp\" alt=\"MMAudio\" width=\"750\"/>\n</p>\n\nThese are the model downloader nodes, it will automatically download models in your comfyui in 2-3 mins. \n\n- MMAudio Models : https://github.com/hkchengrex/MMAudio\n\n\n\n--- \n\n\nWith its innovative multimodal training and precise synchronization, MMAudio sets a new standard in audio generation. Whether you're crafting videos, animations, or immersive experiences, MMAudio empowers creators with seamless, high-quality audio. Elevate your projects and bring your ideas to life with MMAudio.\n\n"
    },
    {
        "id": "1181",
        "readme": "## What Dance Video Transform ComfyUI Workflow Does\n\nDance Video Transform ComfyUI Workflow transforms dance videos into stunning new scenes with professional face swapping while preserving the original choreography and ensuring high-quality output. The process happens in stages, from motion analysis to face replacement, allowing quality checks at each step.\n\n### How Dance Video Transform ComfyUI Workflow Works\n\nThe workflow transforms your dance video by automating these complex transformations through several stages, requiring only your video, a face image, and scene description:\nMotion Analysis → Style Transfer → Face Replacement\n- Analyzes dance movements and spatial information\n- Transforms the scene according to your description\n- Integrates new face while maintaining expressions\n\n### Key Features of Dance Video Transform ComfyUI Workflow\n- Optimized for vertical format (9:16 aspect ratio)\n- Triple ControlNet system for stable transformations\n- Professional face swapping with natural blending\n- Fast testing mode (process 50 frames in minutes)\n- Support for high-resolution output (up to 896px height)\n- Advanced motion preservation using AnimateDiff\n- Dual output system for quality verification\n\n## Quick Start Guide\n\n### Step 1: Initial Setup\nIn respective nodes:\n- **Load Video (Upload):**\n  * Upload 10-15 second dance video with a 9:16 aspect ratio\n  * If your video is not in 9:16, you will need to adjust parameters Width and Height to match your video.\n  * Frame Load Cap: 50 (render only the first 50 frames for quick test)\n- **Load Image:**\n  * Upload clear, front-facing face photo\n- **Batch Prompt Schedule:**\n  * Briefly describe the scene and any other aspects you want to transform\n  ```\n  \"0\": \"[person] in KC Chiefs jersey wearing bluejeans and a baseball cap dancing in the locker room\"\n  ```\n  * Set negative prompt as needed\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1181/readme01.webp\" alt=\"dance video transform\" width=\"450\"/>\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1181/readme02.webp\" alt=\"dance video transform\" width=\"450\"/>\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1181/readme03.webp\" alt=\"dance video transform\" width=\"450\"/>\n\n### Step 2: Quick Test Run\n1. Click \"Queue Prompt\"\n2. This processes ~2 seconds of video\n3. You'll see two outputs:\n   - First output: Scene transformation only\n   - Second output: With face swap applied\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1181/readme04.webp\" alt=\"dance video transform\" width=\"450\"/>\n\n### Step 3: Full Video Processing\nOnly after quick test looks good:\n1. Return to \"Load Video\" node\n2. Change Frame Load Cap to 0 for full video\n3. Click \"Queue Prompt\" for complete processing\n   (This will take significantly longer)\n\n### [Author's](https://www.instagram.com/junkboxai/) Tips for Beginners\n- Follow the Notes: Look for any notes in the interface—they will guide you step-by-step\n- Don't Worry About Advanced Settings: Most of the time, you don't need to adjust anything beyond what is mentioned here\n- Aspect Ratio Importance: Ensure the aspect ratio is correct, otherwise the video may look stretched or cropped\n\n## Key Nodes Reference\n### AnimateDiff Settings\nNodes here create smooth motion preservation throughout the video transformation.\nContext Options defines how frames should be grouped and processed, feeding these settings to AnimateDiff Loader, which then applies the actual motion preservation. Context length and overlap settings directly affect how AnimateDiff Loader maintains movement consistency.\n\n1. **Context Options Node (#94):** Achieves frame grouping and temporal processing control for consistent motion.\n   * **context_length:**\n     * Controls how many frames are processed together\n     * Higher = smoother but more VRAM usage\n     * Lower = faster but may lose motion coherence\n   * **context_overlap:**\n     * Handles frame transition smoothness\n     * Higher = better blending but slower processing\n     * Lower = faster but may show transition gaps\n   * **context_schedule:**\n     * Controls frame distribution\n     * \"uniform\" best for dance motion\n     * Don't change unless specific needs\n   * **closed_loop:**\n     * Controls video loop behavior\n     * True only for perfectly looping videos\n2. **AnimateDiff Loader Node (#93):** Implements the motion preservation using AnimateDiff model and applies temporal consistency.\n   * **motion_scale:**\n     * Controls motion strength\n     * Higher: Exaggerated movement\n     * Lower: Subdued movement\n   * **beta_schedule:** lcm >> sqrt_linear\n     * Controls sampling behavior\n     * Optimized for this workflow\n     * Don't modify unless necessary\n     <img src=\"https://cdn.runcomfy.net/workflow_assets/1181/readme05.webp\" alt=\"dance video transform\" width=\"350\"/>\n\n### ControlNet Stack\nNodes here maintain video integrity through three-layer guidance system.\nThe three ControlNets process input frames simultaneously, each focusing on different aspects. Soft Edge provides basic structure, Depth adds spatial understanding, and OpenPose ensures movement accuracy. Results combine through stackers with total strength not exceeding 1.4 for stability.\n\n1. **Soft Edge ControlNet:** Extracts and preserves structural elements and shapes from original frames.\n   * **Strength:**\n     * Controls structural preservation\n     * Higher = stronger adherence to original shapes\n     * Lower = more creative freedom in shape modification\n   * **End percent:**\n     * When control influence stops\n     * Higher = longer influence throughout process\n     * Lower = allows more deviation in later steps\n2. **Depth ControlNet:** Processes spatial relationships and maintains 3D consistency.\n   * **Strength:**\n     * Controls spatial awareness\n     * Higher = stronger 3D consistency\n     * Lower = more artistic freedom with space\n   * **End percent:**\n     * Maintains depth influence duration\n     * Should match Soft Edge for consistency\n3. **OpenPose ControlNet:** Captures and transfers pose information for accurate movement.\n   * **Strength:**\n     * Controls pose preservation\n     * Higher = stricter pose following\n     * Lower = more flexible pose interpretation\n   * **End percent:**\n     * Maintains pose influence\n     * Keeps movement natural throughout process\n     <img src=\"https://cdn.runcomfy.net/workflow_assets/1181/readme06.webp\" alt=\"dance video transform\" width=\"750\"/>\n\n### Face Processing\nNodes here handle face replacement and enhancement for natural results.\nThe process works in two stages: FaceRestore first enhances the original face quality, then ReActor performs the swap using the enhanced face as reference. This two-stage process ensures natural integration while preserving expressions.\n\n1. **FaceRestore System:** Enhances facial details and prepares for swapping.\n   * **Fidelity:**\n     * Controls detail preservation in restoration\n     * Higher = more detailed but potential artifacts\n     * Lower = smoother but may lose details\n   * **Detection:**\n     * Face detection model choice\n     * Reliable for most scenarios\n     * Don't change unless faces aren't detected\n2. **ReActor Face Swap:** Performs face replacement and blending with preserved expressions.\n   * **Visibility:**\n     * Controls swap visibility\n     * Higher = stronger face swap effect\n     * Lower = more subtle blending\n   * **Weight:**\n     * Face feature preservation balance\n     * Higher = stronger source face features\n     * Lower = better blending with target\n   * **Console log level:**\n     * Controls debugging information\n     * Higher = more detailed logs\n     <img src=\"https://cdn.runcomfy.net/workflow_assets/1181/readme07.webp\" alt=\"dance video transform\" width=\"350\"/>\n     <img src=\"https://cdn.runcomfy.net/workflow_assets/1181/readme08.webp\" alt=\"dance video transform\" width=\"350\"/>\n\n## Additional Node Details\n\n### Input & Preprocessing\nPurpose: Loads video, adjusts dimensions, and prepares VAE model for processing.\n1. **Load Video:**\n   * **Frame Load Cap:**\n     * Controls number of frames to process\n     * 50 = quick test (processes ~2 seconds)\n     * 0 = process entire video\n     * Affects total processing time\n   * **Skip First Frames:**\n     * Defines starting point in video\n     * Higher = starts later in video\n     * Useful for skipping intros\n   * **Select Every Nth:**\n     * Controls frame sampling rate\n     * Higher numbers skip frames\n     * 1 = use every frame\n     * 2 = use every second frame, etc.\n2. **Image Scale:**\n   * **Width:** 512\n     * Controls output frame width\n     * Must maintain 9:16 ratio with height\n   * **Height:** 896\n     * Controls output frame height\n     * Must maintain 9:16 ratio with width\n   * **Method:** nearest-exact\n     * Best for maintaining sharpness\n     * Alternatives may blur content\n     * Recommended for dance videos\n     * Don't change unless specific needs\n3. **VAE Loader:**\n   * **Model:** vae-ft-mse-840000-ema-pruned\n     * Optimized for stability and quality\n     * Handles image encoding/decoding\n     * Balanced compression ratio\n     * Don't change unless specific needs\n   * **VAE Mode:** Don't change\n     * Optimized for current workflow\n     * Affects encoding quality\n     <img src=\"https://cdn.runcomfy.net/workflow_assets/1181/readme09.webp\" alt=\"dance video transform\" width=\"350\"/>\n     <img src=\"https://cdn.runcomfy.net/workflow_assets/1181/readme10.webp\" alt=\"dance video transform\" width=\"500\"/>\n\n### Latent Processing\nPurpose: Handles all latent space operations and transformations.\n1. **Empty Latent Image:**\n   * **Width/Height:** matches input\n     * Must match Image Scale dimensions\n     * Affects memory usage directly\n     * Larger sizes need more VRAM\n     * Can't be smaller than input\n   * **Batch Size:** from video frames\n     * Set automatically from frame count\n     * Affects processing speed and VRAM\n     * Higher = more memory needed\n2. **VAE Encode:**\n   * **VAE Model:** from VAE Loader\n     * Uses settings from VAE Loader\n     * Maintains consistency\n   * **Decode:** enabled\n     * Controls decoding quality\n     * Disable only if VRAM limited\n     * Affects output quality\n3. **Latent Blend:**\n   * **Blend Factor:**\n     * Controls mixing of latent spaces\n     * 0 = full source content\n     * Higher = more empty latent influence\n     * Affects style transfer strength\n4. **Latent Upscale By:**\n   * **Method:** nearest-exact\n     * Best for maintaining sharpness\n     * Alternative methods may blur\n     * Preserves motion details\n   * **Scale:**\n     * Controls size increase\n     * Higher = better detail but more VRAM\n     * Lower = faster processing\n     * 1.6 optimal for most cases\n     <img src=\"https://cdn.runcomfy.net/workflow_assets/1181/readme11.webp\" alt=\"dance video transform\" width=\"350\"/>\n     <img src=\"https://cdn.runcomfy.net/workflow_assets/1181/readme12.webp\" alt=\"dance video transform\" width=\"350\"/>\n\n### Sampling & Refinement\nPurpose: Two-stage sampling process for quality transformation.\n1. **KSampler (First Pass):**\n   * **Steps:**\n     * Number of denoising steps\n     * Higher = better quality but slower\n     * 6 optimal for lcm sampler\n   * **CFG:**\n     * Controls prompt influence\n     * Higher = stronger style adherence\n     * Lower = more freedom\n   * **Sampler:** lcm\n     * Optimized for speed\n     * Good quality/speed balance\n   * **Scheduler:** sgm_uniform\n     * Works best with lcm\n     * Maintains temporal consistency\n   * **Denoise:**\n     * Full strength for first pass\n     * Controls transformation intensity\n2. **KSampler (Hires Pass):**\n   * **Steps:**\n     * Matches first pass for consistency\n     * Higher not needed for refinement\n   * **CFG:**\n     * Maintains style consistency\n     * Balanced detail preservation\n   * **Sampler:** lcm\n     * Same as first pass\n     * Maintains consistency\n   * **Scheduler:** sgm_uniform\n     * Maintains consistency with first pass\n     * Good for detail refinement\n   * **Denoise:**\n     * Lower than first pass\n     * Preserves more original detail\n     * Good balance for refinement\n\n### Output Processing\nPurpose: Creates final video outputs with and without face swap.\n1. **Video Combine (Raw):**\n   * **Frame Rate:**\n     * Standard video frame rate\n     * Controls playback speed\n     * Lower = smaller file size\n     * Higher = smoother motion\n   * **Format:** video/h264-mp4\n     * Standard format for compatibility\n     * Good balance of quality/size\n     * Widely supported\n   * **CRF:**\n     * Controls compression quality\n     * Lower = better quality but larger file\n     * Higher = smaller file but lower quality\n     * 19 is high quality setting\n   * **Pixel Format:** yuv420p\n     * Standard format for compatibility\n     * Don't change unless needed\n     * Ensures wide playback support\n2. **Video Combine (Face Swap):**\n   * Same parameters as raw output\n   * Uses identical settings for consistency\n   * Adds face swap integration\n   * Maintains video quality settings\n\n## Optimization Tips\n\n### Quality vs Speed Trade-offs\n1. **Resolution Balance:**\n   - Standard: 512x896\n     * Faster processing\n     * Good for most uses\n   - High Quality: 768x1344\n     * Better detail\n     * 2-3x longer processing time\n2. **Face Swap Quality:**\n   - Standard: Default settings\n     * Natural integration\n     * Balanced processing time\n   - Maximum Quality:\n     * Increase codeformer_fidelity to 0.9\n     * Slower but more detailed faces\n3. **Motion Smoothness:**\n   - Faster Processing:\n     * Reduce context_overlap to 2\n     * Slightly less smooth transitions\n   - Better Motion:\n     * Increase overlap to 6\n     * Uses more VRAM, slower processing\n\n### Common Issues & Solutions\n1. **Face Blending:**\n   - Issue: Unnatural face transition\n   - Solution: Adjust codeformer_weight\n     * Try range 0.4-0.7\n     * Lower = better blending\n     * Higher = more facial details\n2. **Style Strength:**\n   - Issue: Weak style transfer\n   - Solution: Increase cfg\n     * Try range 7-8\n     * Higher = stronger style\n     * May affect motion quality\n3. **Memory Management:**\n   - Issue: VRAM limitations\n   - Solutions:\n     * Enable VAE slicing\n     * Reduce resolution\n     * Process shorter segments\n\n## More Information\nFor additional details and amazing creations, please visit [junkboxai's Instagram](https://www.instagram.com/junkboxai/).\n\n"
    },
    {
        "id": "1182",
        "readme": "LatentSync is a state-of-the-art end-to-end lip sync framework that harnesses the power of audio-conditioned latent diffusion models for realistic lip sync generation. What sets LatentSync apart is its ability to directly model the intricate correlations between audio and visual components without relying on any intermediate motion representation, revolutionizing the approach to lip sync synthesis.\n\nAt the core of LatentSync's pipeline is the integration of Stable Diffusion, a powerful generative model renowned for its exceptional ability to capture and generate high-quality images. By leveraging Stable Diffusion's capabilities, LatentSync can effectively learn and reproduce the complex dynamics between speech audio and corresponding lip movements, resulting in highly accurate and convincing lip sync animations.\n\nOne of the key challenges in diffusion-based lip sync methods is maintaining temporal consistency across generated frames, which is crucial for realistic results. LatentSync tackles this issue head-on with its groundbreaking Temporal REPresentation Alignment (TREPA) module, specifically designed to enhance the temporal coherence of lip sync animations. TREPA employs advanced techniques to extract temporal representations from the generated frames using large-scale self-supervised video models. By aligning these representations with the ground truth frames, LatentSync's framework ensures a high degree of temporal coherence, resulting in remarkably smooth and convincing lip sync animations that closely match the audio input.\n\n## 1.1 How to Use LatentSync Workflow?\n> **Note:** The LatentSync node has been updated to version 1.6 (latest version).\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1182/readme01.webp\" alt=\"LatentSync\" width=\"750\"/>\n</p>\n\nThis is the LatentSync workflow, Left Side nodes are inputs for uploading video, Middle is processing LatentSync nodes, and right is the outputs node.\n- Upload your Video in input nodes.\n- Upload your Audio input of dialouges. \n- Click Render !!! \n\n\n## 1.2 Video Input\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1182/readme02.webp\" alt=\"LatentSync\" width=\"750\"/>\n</p>\n\n- Click and Upload your Reference Video which has face in it.\n\nThe video is adjusted to 25 FPS to sync properly with the Audio model\n\n## 1.3 Audio Input\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1182/readme03.webp\" alt=\"LatentSync\" width=\"750\"/>\n</p>\n\n- Click and Upload your audio here.\n\n\n--- \n\n\nLatentSync sets a new benchmark for lip sync with its innovative approach to audio-visual generation. By combining precision, temporal consistency, and the power of Stable Diffusion, LatentSync transforms the way we create synchronized content. Redefine what's possible in lip sync with LatentSync.\n\n"
    },
    {
        "id": "1183",
        "readme": "TRELLIS revolutionizes 3D asset creation by introducing a groundbreaking approach that seamlessly integrates sparse 3D structures with rich visual features. At the heart of TRELLIS lies the innovative Structured LATent (SLAT) representation, which allows you to effortlessly generate stunning 3D objects with intricate geometries and vivid textures, surpassing the capabilities of existing methods. What sets TRELLIS apart is its unparalleled versatility, as it dynamically supports a wide range of output formats, including Radiance Fields, 3D Gaussians, and meshes. By leveraging the power of Rectified Flow Transformers, TRELLIS achieves exceptional precision and delivers unmatched creative freedom for 3D content generation.\n\n## 1 How to Use TRELLIS Workflow?\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1183/readme01.webp\" alt=\"Trellis\" width=\"750\"/>\n</p>\n\nThis is the TRELLIS workflow, Left Side node is input for uploading Image, Middle is processing TRELLIS nodes, and right is the output node.\n- Upload your Image in input nodes.\n- Click Render !!! \n\nAll Outputs (GLB, Texutures and Previews will stored in the Ouput folder of Comfyui Inside \"Output_3D_Trellis\" folder.\n\n---\n\n\n### 1.1 Image Input\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1183/readme02.webp\" alt=\"Trellis\" width=\"750\"/>\n</p>\n\n- Click and Upload your Reference image for generating 3D model.\n\n---\n\n### 1.2 Trellis Image to 3D Node\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1183/readme03.webp\" alt=\"Trellis\" width=\"550\"/>\n</p>\n\n### Input Options\n- `Model`: Specifies the model file used for 3D asset generation.\n- `Images`: Input images for processing.\n- `Video Path`: Option to input a video path for 3D generation.\n- `Masks`: Input for texture masking, labeled as texture_image.\n\n### Configuration Settings\n- `Mode`: Selection between different modes (e.g., single).\n- `Seed`: Randomization seed for deterministic outputs (set to 7777).\n- `Control After Generate`: Determines when control actions occur (fixed,increment or random).\n\n### Guidance and Sampling\n- `SS Guidance Strength`: Controls the strength of stable structure guidance (set to 7.5).\n- `SS Sampling Steps`: Number of sampling steps for stable structure (set to 12).\n- `SLAT Guidance Strength`: Controls SLAT representation guidance strength (3.0).\n- `SLAT Sampling Steps`: Number of sampling steps for SLAT (12).\n\n### Mesh and Texture Options\n- `Mesh Simplify`: Level of mesh simplification (set to 0.95).\n- `Texture Size`: Texture resolution size (1024).\n- `Texture Mode`: Mode for texture processing (fast).\n- `FPS`: Frames per second for rendering (15).\n\n### Output Options\n- `Multimode`: Output generation mode (stochastic).\n- `Project Name`: Name for the generated project (Output_3D_Trellis).\n\n### Saving Options\n- `Save GLB`: Enabled (true) for saving in GLB format.\n- `Render Video`: Enabled (true) for video output rendering.\n- `Save Gaussian`: Enabled (true) for saving Gaussian format.\n- `Save Texture`: Enabled (true) for saving textures.\n- `Save Wireframe`: Enabled (true) for saving wireframe views.\n\n\n### 1.3 Trellis 3D Model\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1183/readme04.webp\" alt=\"Trellis\" width=\"550\"/>\n</p>\n\nModels will be autodownloaded on Runcomfy, wait 2-3 mins when downloading them for the first time.\n\nModels Download Location:\n \n<pre>\n1) ComfyUI/\n    └── models/\n      └── checkpoints/\n          └── TRELLIS-image-large/\n              ├── .gitattributes\n              ├── pipeline.json\n              ├── README.md\n              └── ckpts/\n                  ├── slat_dec_gs_swin8_B_64l8gs32_fp16.json\n                  ├── slat_dec_gs_swin8_B_64l8gs32_fp16.safetensors\n                  ├── slat_dec_mesh_swin8_B_64l8m256c_fp16.json\n                  ├── slat_dec_mesh_swin8_B_64l8m256c_fp16.safetensors\n                  ├── slat_dec_rf_swin8_B_64l8r16_fp16.json\n                  ├── slat_dec_rf_swin8_B_64l8r16_fp16.safetensors\n                  ├── slat_enc_swin8_B_64l8_fp16.json\n                  ├── slat_enc_swin8_B_64l8_fp16.safetensors\n                  ├── slat_flow_img_dit_L_64l8p2_fp16.json\n                  ├── slat_flow_img_dit_L_64l8p2_fp16.safetensors\n                  ├── ss_dec_conv3d_16l8_fp16.json\n                  ├── ss_dec_conv3d_16l8_fp16.safetensors\n                  ├── ss_enc_conv3d_16l8_fp16.json\n                  ├── ss_enc_conv3d_16l8_fp16.safetensors\n                  └── ss_flow_img_dit_L_16l8_fp16.json\n                  └── ss_flow_img_dit_L_16l8_fp16.safetensors\n\n2) ComfyUI\\models\\classifiers/\n                   └── dinov2_vitl14_reg.pth\n</pre>\n\n\n    \nModels Download Link -\n\n  - Trellis - https://huggingface.co/JeffreyXiang/TRELLIS-image-large/tree/main\n  - DinoV2 - https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_pretrain.pth\n\n--- \n\n\nTrellis pushes the boundaries of 3d asset generation with its innovative approach and flexible output capabilities. by combining the structured latent representation with rectified flow transformers, it empowers creators to produce high-quality, versatile 3d content effortlessly. unlock new creative possibilities with trellis.\n\n"
    },
    {
        "id": "1184",
        "readme": "# ComfyUI Nvidia Cosmos Text & Image to Video Workflow\n\n## What is the Nvidia Cosmos Workflow\nTurn your imagination into fluid videos using the newly released Nvidia Cosmos models in ComfyUI. This workflow demonstrates the strong AI capabilities of Nvidia Cosmos with its text-to-video and image-to-video generation features. Powered by Nvidia Cosmos's state-of-the-art 7B and 14B models, you can create high-quality videos from either textual descriptions or still images. The Nvidia Cosmos engine gives stellar results thanks to its ultra-efficient video processing capabilities.\n\n---\n\n## Key Features of Nvidia Cosmos\n- **Dual Generation Modes**: Nvidia Cosmos offers both text-to-video and image-to-video generation\n- **Guaranteed Motion**: Always generates videos with movement when using 121 frames\n- **Effective Negative Prompts**: Non-distilled model ensures better control through negative prompts\n- **Flexible Image Control**: Generate from the last frame or create transitions between images\n- **Ultra-Efficient VAE**: Nvidia Cosmos employs a refined VAE system for smooth, high-quality video generation\n- **High Resolution Support**: Create videos at resolutions of 704x704 and above\n- **Precise Frame Control**: Optimized for 121-frame sequences\n- **Smart Image Interpolation**: Generate smooth transitions between reference images\n\n---\n\n## How to Use the Nvidia Cosmos Workflow\n\nNvidia Cosmos workflow contains two main parts: _text-to-video_ and _image-to-video_ generation. By default, the _image-to-video_ group is bypassed. To switch between the two modes:\n\n- For _text-to-video_: Keep the _image-to-video_ group bypassed (default setting)\n- For _image-to-video_: Right-click the _image-to-video_ group and select `Set Group Nodes to Always`\n\n### 1. Text to Video Generation with Nvidia Cosmos\n\n#### Setup and Requirements\n- Choose your preferred Nvidia Cosmos model size (7B recommended for starting)\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1184/readme01.webp\" alt=\"Nvidia Cosmos\" width=\"350\"/>\n- Set resolution (Default 1280x704; minimum 704x704)\n- Frame settings:\n  - Length: 121 frames (The model performs optimally with a length of 121; deviating too much from this can result in subpar video quality.)\n  - Frame rate: 24.00 (default rate for optimal quality)\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1184/readme02.webp\" alt=\"Nvidia Cosmos\" width=\"350\"/>\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1184/readme03.webp\" alt=\"Nvidia Cosmos\" width=\"350\"/>\n  \n\n#### Sampling Parameters for Nvidia Cosmos\n- Sampler: `res_multistep` (Nvidia's recommended sampler for Cosmos)\n- Scheduler: `karras` (default for stability)\n- Steps: `20` (higher = better quality but slower; lower = faster but less detailed)\n- CFG: `6.5` (prompt guidance strength)\n- Denoise: `1.00` (1.00 = complete transformation; lower values keep more original content)\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1184/readme04.webp\" alt=\"Nvidia Cosmos\" width=\"350\"/>\n\n#### Prompting Tips for Nvidia Cosmos\n- Use detailed, multi-sentence prompts for better results\n- Include comprehensive negative prompts\n- Short prompts may generate coherent videos but might not strictly follow instructions\n\n### 2. Image to Video Generation with Nvidia Cosmos\n\n#### Setup and Requirements\n- Same base requirements as Nvidia Cosmos text-to-video\n- Supports `start_image` and `end_image` inputs\n\n#### Reference Image Options\n- Set a `start_image` or `end_image`, or both at the same time\n- Images work best when similar in style and content (for smooth transitions)\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1184/readme05.webp\" alt=\"Nvidia Cosmos\" width=\"350\"/>\n\n#### Key Parameters\n- Identical sampling settings to text-to-video mode\n- Maintains same video quality standards\n\n## Advanced Tips for Nvidia Cosmos\n- For higher quality results with more VRAM, try the Nvidia Cosmos 14B model\n- Ensure prompts are descriptive and detailed for best results\n- Experiment with different image pairs for unique transitions\n\n---\n\n## More Information about Nvidia Cosmos\nFor more details and updates about Nvidia Cosmos, visit [Nvidia Cosmos Official Page](https://www.nvidia.com/en-us/ai/cosmos/).\n"
    },
    {
        "id": "1190",
        "readme": "Janus-Pro is a cutting-edge autoregressive framework that unifies multimodal understanding and generation, addressing key limitations of previous approaches. By decoupling visual encoding into separate pathways while maintaining a single transformer architecture, Janus-Pro eliminates conflicts between perception and synthesis, enhancing both flexibility and performance in multimodal AI. With Janus-Pro, users can achieve a more refined balance between visual comprehension and content generation, making Janus-Pro a superior choice for next-generation AI solutions.\n\nAt the core of Janus-Pro’s design is its innovative dual-pathway visual encoding strategy, which allows Janus-Pro to process visual inputs more effectively without sacrificing its generative capabilities. Unlike traditional unified models that struggle with balancing understanding and generation, Janus-Pro optimizes both tasks by assigning them dedicated encoding pathways while still leveraging a single, powerful transformer for processing. This approach enables Janus-Pro to seamlessly adapt across diverse multimodal tasks, from image synthesis to text-guided generation, reinforcing Janus-Pro’s ability to outperform existing AI frameworks.\n\nA major challenge in unified multimodal models is maintaining high performance across a wide range of tasks without requiring task-specific architectures. Janus-Pro overcomes this with its streamlined yet highly adaptable framework, surpassing previous unified models and even matching or exceeding the performance of specialized task-specific solutions. With its simplicity, flexibility, and superior effectiveness, Janus-Pro represents a significant step forward in multimodal AI. Janus-Pro is setting a new benchmark for next-generation unified models, proving that Janus-Pro is the future of multimodal AI technology.\n\n## 1.1 How to Use Janus-Pro Workflow?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1190/readme01.webp\" alt=\"Janus-Pro\" width=\"750\"/>\n</p>\n\nYou can use Janus-Pro workflow in 2 ways\n\n1) Janus-Pro Image generation \n2) Janus-Pro Image Description (OCR, Captions, Describe...etc) \n\n## 1.2 Janus-Pro Image Generation\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1190/readme02.webp\" alt=\"Janus-Pro\" width=\"750\"/>\n</p>\n\n- The Janus Image Generation Sampler lets you enter prompts.\n- You can use Janus-Pro-1B or Janus-Pro-7B model.\n- Janus-Pro Image generation is currently restricted to a 1:1 Square (384*384 px) ratio.\n\nThe Janus-Pro models will be auto-downloaded in your cloud runcomfy machine upon running for the first time. This may take 2-5 minutes when queuing for the first time.\nModels Link -\n\n  - Janus-Pro-1B - https://huggingface.co/deepseek-ai/Janus-Pro-1B\n  - Janus-Pro-7B - https://huggingface.co/deepseek-ai/Janus-Pro-7B\n\nThe models will be downloaded in : `Comfyui/models/Janus-Pro`\n\n## 1.3 Janus-Pro Image Description\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1190/readme03.webp\" alt=\"Janus-Pro\" width=\"750\"/>\n</p>\n\n- Click and Upload an Image in the Load Image Node for Janus-Pro processing.\n- You can perform : **OCR, Captions, Detailed Description** using the Janus-Pro Image Understanding Node. Simply type your request in the Type Box provided in the node.\n\nExample Question:\n“Describe this image in detail, where is this located, what is written in it… etc.”\n\n\n\n--- \n\nJanus-Pro sets a new standard for multimodal AI by seamlessly integrating understanding and generation within a unified framework. Janus-Pro’s innovative dual-pathway encoding enhances flexibility, resolving conflicts that hinder traditional models. By surpassing previous unified architectures and rivaling task-specific solutions, Janus-Pro paves the way for more efficient and versatile AI systems. As a powerful and adaptable framework, Janus-Pro stands at the forefront of next-generation multimodal intelligence, proving that Janus-Pro is the future of multimodal AI.\n"
    },
    {
        "id": "1191",
        "readme": "ComfyUI Sonic redefines portrait animation by harnessing global audio perception for ultra-realistic facial movements and expressions. Unlike traditional methods, it captures the full context of speech—beyond phonemes—to generate fluid, emotionally rich animations. With cutting-edge AI technology, Sonic ensures seamless sync between voice and visuals, bringing characters to life with unmatched realism. Elevate your animations with Sonic and make every expression feel truly alive.\n\n[The ComfyUI Sonic nodes and related workflow were developed by smthemex. For more information, please visit smthemex's GitHub.](https://github.com/smthemex/ComfyUI_Sonic)\n\n## 1.1 How to Use Sonic Workflow?\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1191/readme01.webp\" alt=\"Sonic\" width=\"750\"/>\n</p>\n\nLeft nodes are your inputs for Audio and Avatar Image. Middle one is the Sonic Processing Node. Right side is the video combine node for outputting video.\n\nFollow these Steps:\n1) Input your Avatar Image which will be used to visualize the dialogues from the audio.\n2) Input your Audio for generating an audio-driven voice-over of the inserted image.\n3) Click Queue Prompt!!\n\nDone! Your rendered video will be stored in the Outputs folder.\n\n### Strengths and Weaknesses of Sonic:\n\nStrengths:\n- Sonic generates highly realistic and expressive portrait animations driven by audio.\n- Sonic uses SVD, so there is no flickering between frames.\n- Consistency is better than previously released audio2video models.\n\nWeaknesses:\n- As Sonic uses SVD, far or full body shots may struggle with projecting vocals on the face properly.\n- Side view faces, or faces at complex angles might give distorted results.\n\n## 1.2 Sonic Audio and Video Input\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1191/readme02.webp\" alt=\"Sonic\" width=\"750\"/>\n</p>\n\n- Upload your Audio in the load audio node (Dialogues or Vocals)\n- Upload your image in the Load image node (A close-up or medium shot of a person)\n  \n## 1.3 Sonic Processing Node\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1191/readme03.webp\" alt=\"Sonic\" width=\"750\"/>\n</p>\n\nComfyUI Sonic uses **SVD Model** under the hood for processing, so the results and settings are according to the SVD model.\nThese settings are set to optimum; there's no necessity to change them.\n\n- Keep `min resolution` near 768 or under if there are artifacts like morphing or distorted hands.\n--- \nSonic transforms portrait animation by focusing on global audio perception for seamless, lifelike expressions. By capturing the full depth of speech, it creates animations that feel natural, emotive, and engaging. Whether for storytelling, virtual avatars, or content creation, Sonic delivers unmatched realism. Step into the future of animation with Sonic—where every word comes to life.\n"
    },
    {
        "id": "1192",
        "readme": "The **Consistent Character** ComfyUI workflow by **Mickmumpitz** is a powerful tool designed to help you generate consistent characters with AI, maintaining uniform appearance across multiple angles, expressions, and environments. This consistent character generator ensures that your character remain visually identical whether you're working on **animated films, illustrated storybooks, comics, or AI-driven content**. The workflow delivers perfect consistent character creation without requiring complex prompts or manual adjustments.\n\nOriginally developed by **Mickmumpitz** on his **YouTube channel**, we highly recommend checking out his in-depth tutorial to master consistent character generation. While we've made the **Consistent Character** available and set up the environment for your convenience, all credit goes to **Mickmumpitz** for creating this outstanding consistent character solution.\n\n\n## How to Use Consistent Character Workflow?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1192/readme01.webp\" alt=\"Consistent Character\" width=\"750\"/>\n</p>\n\nAll processes are grouped for clarity and convenience. You can enable them individually for gradual progress or activate all at once to utilize the entire consistent character workflow.\n\n1) Multiview – Generates multiple views of your consistent character from the input image.\n   - Upload your character image in this group.\n \n2) Upscale 1st Pass – Upscales the multiple views of your consistent character.\n3) Upscale 2nd Pass – Enhances the face and details of your consistent character.\n4) Emotions – Adds facial expressions to your consistent character using advanced Live Portrait Nodes.\n5) Lighting – Places your consistent character under various lighting conditions with the IC Light Changer model.\n6) Character Sheet – Compiles all the above results into a single consistent character sheet for future reference.\n\nDone! Your consistent character images will be saved in the Outputs folder.\n\n### Optional - Character Generation\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1192/readme02.webp\" alt=\"Consistent Character\" width=\"450\"/>\n</p>\n\nNote: If you don't have an input image, you can create a consistent character in the \"00_Character Generation\" workflow group. \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1192/readme04.webp\" alt=\"Consistent Character\" width=\"750\"/>\n</p>\n\nSimply enable the Group, enter your consistent character prompts, and set the value to `1` in the switch node within the Multiview Group and you are good to go.\n\n\n### 1 - MultiView\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1192/readme03.webp\" alt=\"Consistent Character\" width=\"750\"/>\n</p>\n\nUpload your character image in the Load Image node. A standing A-Pose or T-Pose yield the best results. You may also upload non-living objects.\n\n  \n### 2 - Upscaler 1st Pass\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1192/readme05.webp\" alt=\"Consistent Character\" width=\"750\"/>\n</p>\n\nIn this Group, the face will be enhanced.\n\n### 3 - Upscaler 2nd Pass\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1192/readme06.webp\" alt=\"Consistent Character\" width=\"750\"/>\n</p>\n\nThe overall features will be enhanced in this group.\n\n### 4 - Emotions\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1192/readme07.webp\" alt=\"Consistent Character\" width=\"750\"/>\n</p>\n\nIn this group, you can modify facial expressions of the consistent character. \n\n### 5 - Lighting \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1192/readme08.webp\" alt=\"Consistent Character\" width=\"750\"/>\n</p>\n\nIn this group, you can add various lighting conditions.\n\n### 6 - Character Sheet\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1192/readme09.webp\" alt=\"Consistent Character\" width=\"750\"/>\n</p>\n\nThis Group will compile all the consistent character images from the above group and save the final result in the ComfyUI > output folder.\n\n\n--- \n\nWith the **Consistent Character Workflow**, you can create **high-quality, repeatable consistent character designs** that seamlessly fit into your creative projects. Whether you're crafting a **consistent character cast for AI movies, developing unique consistent characters for comics, or designing consistent character assets for social media content**, this workflow provides the control and flexibility needed for professional results. By leveraging **Mickmumpitz's** innovative approach to consistent character creation, you can focus more on storytelling and artistic expression while ensuring that your consistent characters maintain their distinct and recognizable appearance across all your work.\n"
    },
    {
        "id": "1193",
        "readme": "# Hunyuan3D-2: Transform 2D Images into 3D Models\n\n## Introduction to Hunyuan3D-2 Workflow\n> 🎉 7/11/25 Update: Hunyuan3D 2.1 is here! Major upgrade from v2.0 featuring enhanced textures, richer details, and improved lighting effects. [Try the new workflow now](https://www.runcomfy.com/comfyui-workflows/hunyuan3d-2-1-comfyui-workflow-professional-3d-asset-creation)!\n\nWelcome to Hunyuan3D-2, a cutting-edge ComfyUI workflow designed to effortlessly convert 2D images into stunning 3D models. Leveraging the latest advancements in AI segmentation, neural rendering, and computer vision, Hunyuan3D-2 simplifies the traditionally intricate process of 3D modeling. Whether you're a game developer, 3D artist, or content creator, this workflow allows you to generate high-quality 3D assets complete with textures in just a few clicks.\n\n### How Hunyuan3D-2 Works\n\n**Hunyuan3D-2 ComfyUI Workflow operates through four main stages:**\n- **Image Preparation:** Resizes and removes the background from your input image.\n- **3D Mesh Generation:** Creates a 3D mesh from the isolated subject.\n- **Mesh Optimization:** Cleans up geometry and reduces polygon count for better performance.\n- **Texture Application:** Renders the model from multiple angles and applies realistic textures using AI-powered texture synthesis.\n\nThe final output is a production-ready 3D model in GLB format, suitable for games, AR/VR applications, or further refinement in 3D software.\n\n## Using Hunyuan3D-2: A Step-by-Step Guide\n\n### Step 1: Load Your Image\nUpload your image to the \"Load Image\" node. For optimal results, use a clear photo of a single subject against a simple background.\n\n### Step 2: Execute the Workflow\nClick \"Queue Execution\" to process the entire workflow.\n\n### Step 3: Download Your 3D Model\nOnce processing is complete, find your glb format 3D model in the output folder with the filename \"xxx.glb\".\n\n\n## Key Features of Hunyuan3D-2\n- **One-Click 3D Generation:** Convert any 2D image into a 3D model with a single execution.\n- **Automatic Background Removal:** Utilizes IoSPyReNet segmentation for clean subject isolation.\n- **High-Quality Mesh Generation:** Advanced VAE decoding for detailed geometry reconstruction.\n- **Optimized Output:** Automatic mesh cleanup and polygon reduction for real-time applications.\n- **PBR Material Support:** Compatible with physically-based rendering pipelines.\n- **GLB Export Format:** Universal format compatible with most 3D applications.\n- **JIT Acceleration:** Just-In-Time compilation for faster processing.\n- **Reproducible Results:** Seed control ensures consistent outputs for production workflows.\n\n\n## Key Nodes and Parameters in Hunyuan3D-2 Workflow\n       \n### Mesh Generation Nodes\n\n- **Hy3D VAE Decode:**\n    - `octree_resolution`: 384 (higher resolution yields more details)\n    - `num_chunks`: 32000 (controls mesh detail level)     \n- **Hy3D Postprocess Mesh:**  \n    - `remove_floaters`: true (eliminates disconnected geometry)\n    - `smooth_normals`: false (softens edges when true)\n    <img src=\"https://cdn.runcomfy.net/workflow_assets/1193/readme01.webp\" alt=\"Hunyuan3D-2\" width=\"450\"/>\n\n### Texturing Nodes\n        \n- **Hy3D Sample MultiView:**\n    - `seed`: 1024 (controls randomization)\n    - `denoise_strength`: 1.0 (0.7-1.0 recommended for texture clarity)\n- **CV2 Inpaint Texture:**\n    - `expand_radius`: 3 (fills texture gaps; 2-5px recommended)\n    <img src=\"https://cdn.runcomfy.net/workflow_assets/1193/readme02.webp\" alt=\"Hunyuan3D-2\" width=\"350\"/>\n    <img src=\"https://cdn.runcomfy.net/workflow_assets/1193/readme03.webp\" alt=\"Hunyuan3D-2\" width=\"350\"/>\n\n### Export Nodes\n\n- **Hy3DExportMesh:**\n    - `format`: GLB (for universal compatibility)\n    <img src=\"https://cdn.runcomfy.net/workflow_assets/1193/readme04.webp\" alt=\"Hunyuan3D-2\" width=\"350\"/>\n        \n\n## Tips for Optimal Results with Hunyuan3D-2\n- Use images with clear subjects against simple backgrounds.\n- For higher-quality textures, increase the reference image size (requires more VRAM).\n- Change the `seed` value in \"Hy3D Sample MultiView\" for different texture variations.\n"
    },
    {
        "id": "1194",
        "readme": "# ACE++ Character Consistency Workflow\n\nThe **ACE++ Character Consistency** workflow is a powerful tool that enables you to generate **consistent characters** from a single reference image—**no training required**. Using **ACE++**, you can maintain a **uniform appearance across multiple angles and poses**, making it ideal for **animations, comics, illustrated stories, and AI-driven content**. This workflow seamlessly integrates with **Flux Fill** to enhance consistency, ensuring that your character retains the original style throughout different renderings.\n\n## Why Choose ACE++?\n- **No Training Required:** Generate **consistent character images** instantly.\n- **Highly Reliable Character Retention:** Works with **multiple angles and poses** while keeping character features intact.\n- **Perfect for Creative Applications:** Ideal for **AI-generated animations, comics, and digital storytelling**.\n- **Customizable Output:** Fine-tune style with **prompts or Loras**, though optimal results align with the original image style.\n\nOriginally developed by **Sebastian Kamph**, **ACE++ Character Consistency** leverages **instruction-based image creation and editing via context-aware content filling**. While this workflow is easily accessible, full credit goes to **Sebastian Kamph** for his innovative approach. Learn more through his tutorial to master **ACE++**!\n\nFor further details, visit: [ACE++ Official Page](https://ali-vilab.github.io/ACE_plus_page/)\n\n## How to Use ACE++ Character Consistency Workflow?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1194/readme01.webp\" alt=\"ACE++ Character Consistency\" width=\"750\"/>\n</p>\n\n### Steps to Generate Consistent Character Images with ACE++:\n1) **Upload Your Character Image** in Load Image Group.\n2) **Enter Your Prompts** to customize the output.\n3) **Click Queue Prompt** to render the image.\n\nYour generated character images will be saved in the **Outputs > ACE folder**.\n\n### Image Loading and Processing\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1194/readme03.webp\" alt=\"ACE++ Image Processing\" width=\"450\"/>\n</p>\n\n- **Upload a face image**, which will be **automatically resized to 1024x1024**.\n- While other sizes are possible, **1024x1024 ensures optimal results**.\n- This resolution maintains image **manageability** and ensures **proper padding alignment**.\n- Fully compatible with **Flux generation size standards** for the best performance.\n\n### Image Generation with ACE++\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1194/readme04.webp\" alt=\"ACE++ Image Generation\" width=\"750\"/>\n</p>\n\n- **Write a descriptive prompt and generate your image.**\n- **Flux guidance should be set high**, with a default value of **50**.\n- Use **Euler with 20 steps** and **CFG set to 1**.\n- Experiment with settings to find the best results.\n- The **uploaded image dictates the output style**.\n- Using **additional prompts or Loras** can modify the style, but may **affect image quality**.\n\n### Saving Images\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1194/readme05.webp\" alt=\"ACE++ Output\" width=\"750\"/>\n</p>\n\nYour generated images will be saved in **ComfyUI > Outputs** folder.\n\n## ACE++ Models and Setup\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1194/readme02.webp\" alt=\"ACE++ Models\" width=\"750\"/>\n</p>\n\nWhen running **ACE++ Character Consistency** for the first time, necessary models will be downloaded automatically. Please allow **2-3 minutes** for the setup on your **RunComfy machine**.\n\n### Required Model Downloads:\n1) **Portrait Lora Model**\n   - Download: [comfyui_portrait_lora64.safetensors](https://huggingface.co/ali-vilab/ACE_Plus/tree/main/portrait)\n   - Place in: `/models/loras/`\n2) **Flux Fill FP8 Model**\n   - Download: [Flux Fill FP8](https://civitai.com/models/969431/flux-fill-fp8)\n   - Place in: `/models/diffusion_models/`\n\n## Conclusion\n\nWith **ACE++ Character Consistency**, you can effortlessly generate **high-quality, consistent characters** for animations, illustrations, and AI-powered content creation. This workflow eliminates the need for training, streamlining character generation with **precision and ease**.\n\n**Unlock the full potential of ACE++ and bring your characters to life with unmatched consistency!**\n\n"
    },
    {
        "id": "1195",
        "readme": "# ACE++ Face Swap Workflow\n\nThe **ACE++ Face Swap** workflow for ComfyUI allows you to seamlessly replace faces in images using natural language instructions, eliminating the need for manual editing or complex training. Built on **instruction-based image creation** and **context-aware content filling**, **ACE++ Face Swap** ensures that swapped faces blend naturally while maintaining the **original style and details**. Whether for **AI art, digital storytelling, or creative experimentation**, **ACE++ Face Swap** provides a powerful and flexible face-swapping solution.\n\nOriginally developed by **Sebastian Kamph**, **ACE++ Face Swap Workflow** utilizes advanced AI techniques to achieve high-quality results. While this ComfyUI workflow is available for easy use, full credit goes to **Sebastian Kamph** for developing this innovative approach.\n\nFor more details, visit: [ACE++ Official Page](https://ali-vilab.github.io/ACE_plus_page/)\n\n---\n\n## How to Use ACE++ Face Swap?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1195/readme01.webp\" alt=\"ACE++ Face Swap\" width=\"750\"/>\n</p>\n\n**Basic steps to use ACE++ Face Swap:**\n1. Upload your Face Swapping and Character images in the **Load Image Group**.\n2. Enter prompts to customize your output.\n3. Click Queue Prompt to generate.\n\nYour generation results will be saved in **Output > AceFaceSwap** folder.\n\n### Image Uploading\n\n  <p align=\"center\">\n     <img src=\"https://cdn.runcomfy.net/workflow_assets/1195/readme02.webp\" alt=\"ACE++ Face Swap\" width=\"750\"/>\n  </p>\n- Upload the image where you want to swap a new face.\n  <p align=\"center\">\n     <img src=\"https://cdn.runcomfy.net/workflow_assets/1195/readme03.webp\" alt=\"ACE++ Face Swap\" width=\"450\"/>\n  </p>\n- Right-click on the image, select **Open in Mask Editor**, draw a mask around the face and the area to be edited, then save the mask.  \n- In the second **Load Image** node, upload the face you want to use as the replacement.\n\n### Image Generation\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1195/readme04.webp\" alt=\"ACE++ Face Swap\" width=\"750\"/>\n</p>\n\n- **Write your prompt**  \n- **Flux guidance should be set high**, with the default set to **50**  \n- Use **Euler with 10-12 steps** and **CFG set to 1**  \n- Feel free to **experiment with settings** to find improvements  \n- While you can **force other styles using prompts or Loras**, this may **degrade image quality**  \n\n### Image Processing\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1195/readme05.webp\" alt=\"ACE++ Face Swap\" width=\"750\"/>\n</p>\n\nA Crop and Stitch Group to restore your face image onto the original image.\n\n### Models in ACE++ Face Swap\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1195/readme06.webp\" alt=\"ACE++ Face Swap\" width=\"750\"/>\n</p>\n\nModels will download automatically when running **ACE++ Face Swap Workflow** for the first time. Please wait **2-3 minutes** for setup on your **RunComfy machine**.\n\n1. **Portrait LoRA Model**\n   - Download: [comfyui_portrait_lora64.safetensors](https://huggingface.co/ali-vilab/ACE_Plus/tree/main/portrait)\n   - Place in: `/models/loras/`\n2. **Flux Fill FP8 Model**\n   - Download: [Flux Fill FP8](https://civitai.com/models/969431/flux-fill-fp8)\n   - Place in: `/models/diffusion_models/`\n3. **Flux Turbo LoRA**\n   - Download: [Flux Turbo Alpha](https://civitai.com/models/876388/flux1-turbo-alpha)\n   - Place in: `/models/loras/`\n\n### Saving Results\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1195/readme07.webp\" alt=\"ACE++ Face Swap\" width=\"750\"/>\n</p>\n- **Rendered images will be saved in:** `ComfyUI > Output > AceFaceSwap`\n\n---\n\n## Why ACE++ Face Swap?\n\nWith **ACE++ Face Swap**, you can seamlessly swap faces in images with **precision and ease**. Whether you're editing portraits, enhancing AI art, or experimenting with creative transformations, **ACE++ Face Swap** enables **high-quality results without complex editing**.\n\n**Try ACE++ Face Swap today and bring your creative face-swapping ideas to life effortlessly!** 🚀\n\n"
    },
    {
        "id": "1196",
        "readme": "Hunyuan Video is an open-source foundation model that challenges the dominance of closed-source systems by delivering cutting-edge text-to-video generation capabilities. Built on innovations in large-scale data curation, adaptive architectural design, and optimized infrastructure, Hunyuan Video sets new benchmarks in visual quality.\n\nWhile Hunyuan Video primarily focuses on text-to-video generation, Hunyuan IP2V workflow extends this capability by converting image and text prompt into dynamic video through the same model. This approach enables users to steer content creation using visual references, offering an alternative method for AI-driven content production.\n\nBy combining an image with a prompt, **Hunyuan IP2V** generates motion while preserving the input's key characteristics, making it a useful tool for AI animation, concept visualization, and artistic storytelling. Whether crafting dynamic scenes, stylized movement, or extending static visuals into animated sequences, Hunyuan Video's framework delivers efficient pathways to high-quality results.\n\n\n## How to Use Hunyuan Video - IP2V Workflow?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1196/readme01.webp\" alt=\"Hunyuan IP2V\" width=\"750\"/>\n</p>\nGroups are color-coded for clarity:\n\n1) Green - Inputs  \n2) Purple - Models  \n3) Pink - Hunyuan Sampler  \n4) Red - VAE + Decoding\n5) Grey - Output\n\nUpload your inputs (image and text) in the green nodes and adjust video settings, such as duration and resolution, in the pink sampler node.\n\n\n### Input 1 - Image\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1196/readme02.webp\" alt=\"Hunyuan IP2V\" width=\"450\"/>\n</p>\n\nUpload an image for reference of the place, person, or object you seek similar results for.\n\n### Input 2 - Text \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1196/readme04.webp\" alt=\"Hunyuan IP2V\" width=\"750\"/>\n</p>\n\nIn the first text box, enter your prompts and include the image using the keyword `\"<image>\"`. \n\nFor example, if your input is \"empty street\" and you want to add a woman, the prompt would be: \"A portrait of a woman, background is `<image>`.\"\n\n\n### Sampler\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1196/readme03.webp\" alt=\"Hunyuan IP2V\" width=\"750\"/>\n</p>\n\nYou can adjust the following:\n\n* Image Resolution - Maximum is 1280px 720px , requiring more VRAM.\n* Frames - This sets the number of frames (24 frames = 1 second).\n  \n### Models\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1196/readme05.webp\" alt=\"Hunyuan IP2V\" width=\"750\"/>\n</p>\nIn this Group, the models will auto-download on first run. Please allow 3-5 minutes for the download to complete in your temporary storage.\n\nLinks: \n1) Diffusion: https://huggingface.co/Kijai/HunyuanVideo_comfy/blob/main/hunyuan_video_FastVideo_720_fp8_e4m3fn.safetensors\n- ComfyUI > models > diffusion_models\n\n3) Vae: https://huggingface.co/Kijai/HunyuanVideo_comfy/blob/main/hunyuan_video_vae_bf16.safetensors\n  - ComfyUI > models > vae\n   \n### Outputs \n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1196/readme06.webp\" alt=\"Hunyuan IP2V\" width=\"750\"/>\n</p>\n\nThe rendered video will be saved in the Outputs folder in Comfyui.\n\n\n--- \n\nWith **Hunyuan IP2V** workflow, you’re not limited to text-based video generation now, you can **bring your images to life with motion and style**. Whether for **AI filmmaking, digital art, or creative storytelling**, this workflow gives you the power to **shape your vision with more control than ever before**.\n"
    },
    {
        "id": "1197",
        "readme": "# Hunyuan Lora: Advanced Video Generation with Custom Styles\n\nHunyuan Video is an open-source video foundation model that pushes the boundaries of AI-driven video generation, offering state-of-the-art text-to-video capabilities. With advancements in large-scale data processing, adaptive model architecture, and high-efficiency rendering, Hunyuan Video delivers **unmatched visual quality and fluid motion generation**.  \n\nThe **Hunyuan Lora workflow** enhances this capability by allowing users to **apply pre-trained Loras** to their video generations. Instead of relying solely on text prompts, users can **load existing Hunyuan Lora models to refine style, maintain character consistency, or introduce specific artistic effects**. This workflow simplifies the process of customizing video outputs, making it easier to achieve a cohesive and controlled look.  \n\nBy leveraging **Hunyuan Lora**, creators can **apply unique aesthetics, preserve facial identities, or replicate artistic styles** across multiple videos, providing a powerful tool for AI-powered animation, cinematic sequences, and personalized video content creation. Whether you're working on **character consistency, scene stylization, or detailed video refinement**, the Hunyuan Lora workflow offers a seamless way to elevate your AI-generated videos. 🚀\n\n\n## How to Use Hunyuan Lora Workflow?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1197/readme01.webp\" alt=\"Hunyuan Lora\" width=\"650\"/>\n</p>\nGroups are color-coded for clarity:\n\n1) Green - Inputs for **Hunyuan Lora** implementation \n2) Purple - Models  \n3) Pink - Hunyuan Sampler  \n4) Red - VAE + Decoding\n5) Grey - Output Video\n\nEnter your Positive text and Load any **Hunyuan Lora** you Desire in the green nodes and adjust video settings, such as duration and resolution, in the pink sampler node.\n\n\n### Input (Hunyuan Lora and Text)\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1197/readme02.webp\" alt=\"Hunyuan Lora\" width=\"450\"/>\n</p>\n\n1) Download your **Hunyuan Lora models** from the Civit.ai website and place the models in Comfyui > models > lora > Hunyuan for organization.\nRefresh the browswer to see them in the HunyuanVideo Lora Select list.\n2) Enter your desired prompts and keywords in the text box to trigger the **Hunyuan Lora**.\n\nYou can chain multiple video LORAs by duplicating the HunyuanVideo Lora node and connecting them as you do with standard SD models.\n\n### Sampler\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1197/readme04.webp\" alt=\"Hunyuan Lora\" width=\"450\"/>\n</p>\n\nYou can adjust the following parameters for your **Hunyuan Lora** generation:\n\n* Image Resolution - Maximum is 1280px 720px, requiring more VRAM.\n* Frames - This sets the number of frames (24 frames = 1 second, Max 125 frames).\n  \n### Models\n\nIn this Group, the models will auto-download on first run. Please allow 3-5 minutes for the download to complete in your temporary storage.\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1197/readme03.webp\" alt=\"Hunyuan Lora\" width=\"350\"/>\n</p>\n\nLinks for **Hunyuan Lora** implementation:\n1. [Diffusion](https://huggingface.co/Kijai/HunyuanVideo_comfy/blob/main/hunyuan_video_FastVideo_720_fp8_e4m3fn.safetensors) - ComfyUI > models > diffusion_models\n     <p align=\"center\">\n       <img src=\"https://cdn.runcomfy.net/workflow_assets/1197/readme05.webp\" alt=\"Hunyuan Lora\" width=\"450\"/>\n      </p>\n3. [VAE](https://huggingface.co/Kijai/HunyuanVideo_comfy/blob/main/hunyuan_video_vae_bf16.safetensors) - ComfyUI > models > vae\n   \n### Outputs \n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1197/readme06.webp\" alt=\"Hunyuan Lora\" width=\"450\"/>\n</p>\n\nThe rendered **Hunyuan Lora video** will be saved in the Outputs folder in Comfyui.\n\n\n--- \n\nWith the **Hunyuan Lora**, you can go beyond basic text-to-video generation and enhance your videos with custom styles and character consistency. Whether you're creating AI-driven animations, cinematic sequences, or artistic projects, applying pre-trained **Hunyuan Lora models** gives you **greater control over your visual output**. Try **Hunyuan Lora** today and take your Hunyuan Video creations to the next level! 🚀\n"
    },
    {
        "id": "1198",
        "readme": "# AP Workflow (APW) for ComfyUI - All-in-One Media Generation Solution\n\n> **No model hunting, no downloads, no configuration hassles - APW comes with everything pre-configured and ready to use.**\n\n## What is AP Workflow (APW) for ComfyUI?\n\nAP Workflow is a comprehensive, enterprise-grade, all-in-one solution that enables you to build your own private, Midjourney-like image and video generation system. Created by Alessandro Perilli, APW serves as a blueprint for automated AI media production, providing carefully curated nodes, pre-set models, and optimized workflows that transform ComfyUI into a powerful production system without requiring technical expertise. With APW, there's no need to search for, download, or configure dozens of models - a process that typically takes a week or more. Everything is pre-configured and ready to use immediately.\n\n## Professional and Industrial Use Cases for AP Workflow (APW)\n\nAP Workflows are used by professional studios and industrial-scale organizations worldwide for production-ready applications:\n\n- **Creative Design:** Fast idea prototyping for interior, industrial, and game design\n- **Film Production:** AI Cinema image generation for storyboarding and location scouting \n- **Fashion Industry:** Creative upscaling of images and remote model casting for collections and advertisements\n- **Visual Effects:** Face swap for stunt scenes and vintage film restoration\n- **E-commerce:** Virtual try-on for online shopping and retail point of sale\n- **Art Generation:** Complex image creation with consistent styles and themes\n\nWhether you're a fashion brand, e-commerce company, photography studio, ad agency, design studio, gaming company, AI-first startup, or innovative artist, AP Workflow (APW) helps incorporate AI into your professional creative workflow at industrial scale—turning experimental AI tools into production-ready systems.\n\n## Benefits of AP Workflow (APW)\n\n* **Complete All-in-One Solution:** A carefully design of curated sets of the best nodes from the AI community\n* **Industrial Scale Production:** Generate and manipulate media at scale for commercial applications\n* **Unified Generation System:** Integrated pipelines for images (FLUX, SD3, SD1.5/SDXL) and videos (Hunyuan, CogVideoX)\n* **Advanced Enhancement Tools:** Face and hand detailing, object swapping, face swapping, and high-quality upscaling\n* **Prompt Intelligence:** LLM and VLM integration to improve prompts and generate sophisticated captions\n* **Ready-to-Use Cloud Environment:** Easily run the workflow in a cloud environment without worrying about setup or installation (for local installation, please refer to [APW installation documentation](https://perilli.com/ai/comfyui-ap-workflow/#requirednodes))\n\n## AP Workflow (APW) Overview\n\nAP Workflow (APW) is a large, sophisticated media studio designed to give digital artists, filmmakers, and advertisers unprecedented control over AI generation. While the size may initially seem daunting, it is organized into logical functional groups with a centralized Controller system.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1198/readme01.webp\" alt=\"AP Workflow\" width=\"750\"/>\n\n### Main Components:\n\n1. **Central Controller:** Single hub to enable/disable all workflow functions\n2. **Prompt Enhancement:** LLM-powered tools to generate better prompts and captions\n3. **Unified Generation Pipelines:** Integrated systems for both image (FLUX, SD3, SD1.5/SDXL) and video content (Hunyuan, CogVideoX)\n4. **Comprehensive Manipulation:** Tools for enhancing both images and videos, including face detailing, hand fixing, and object swapping\n5. **Creative Upscaling:** Professional-grade upscalers comparable to commercial solutions\n6. **Image Finishing:** Color correction, LUT application, grain effects, and watermarking\n7. **Evaluation Tools:** Compare results and score media on aesthetic quality\n\nAP Workflow (APW) is comprehensively documented and features an intelligent navigation system with bookmarks for easily moving between sections. For navigation shortcuts and other interface details, see the [APW navigation documentation](https://perilli.com/ai/comfyui-ap-workflow/#navigation).\n\n## Getting Started with AP Workflow (APW)\n\nAP Workflow (APW) is pre-configured to generate images with the FLUX 1 model. After opening in the cloud, you can immediately queue a generation with the prompt already set in the Prompt Builder function to have a quick test.\n\nCheck the outcome of your image generation/manipulation in the Final Image Saver section at the bottom-left of the workflow using the Navigation tool.\n\n## Key Nodes in AP Workflow (APW)\n\n### Central Controller\nA unified control hub that allows you to enable/disable all workflow nodes from a single location. This centralized approach makes managing the complex workflow simple and intuitive.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1198/readme02.webp\" alt=\"AP Workflow\" width=\"550\"/>\n\n### Prompt Enhancement System\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1198/readme03.webp\" alt=\"AP Workflow\" width=\"200\"/>\n\n#### Prompt Builder\nA visual and flexible prompt builder that allows quick switching between frequently used types and styles for positive prompts, and frequently used negative prompts.\n\n#### Prompt Enricher\nEnriches your positive prompt with additional text generated by a large language model (LLM), supporting both centralized proprietary models and open access models installed locally.\n\n#### Caption Generator\nAutomatically captions uploaded images using visual language models like Florence 2, OpenAI GPT-4V, or locally installed VLMs.\n\n### Unified Media Generation System\n\nThe integrated generation system of AP Workflow (APW) handles both images and videos within a cohesive framework:\n<p>\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1198/readme04.webp\" alt=\"AP Workflow\" width=\"450\" style={{marginRight: 20 + 'px'}} />\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1198/readme05.webp\" alt=\"AP Workflow\" width=\"200\" />\n</p>\n\n#### Comprehensive Generation Capabilities\n- **Image Generation:** Three pipelines (FLUX.1, SD3, SD1.5/SDXL) that can work individually or in combination\n- **Video Creation:** Professional-quality video generation with Hunyuan (T2V/V2V up to 1280x720px/129 frames) and CogVideoX (T2V/I2V up to 1360x768px/81 frames)\n- **Cross-Modal Generation:** Seamless connections between image and video, allowing for image-to-video and video-to-video transformations\n\n### Enhancement Tools\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1198/readme06.webp\" alt=\"AP Workflow\" width=\"550\"/>\n\n#### Image and Video Painters\n- **Repainter:** Full image regeneration with controllable fidelity\n- **Inpainter:** Targeted editing of specific image areas\n- **Image Expander:** Extend image boundaries in any direction\n\n#### Comprehensive Manipulation\n- **Object Swapper:** Identify and replace specific objects within images\n- **Face Swapper:** Replace faces while maintaining expression and lighting\n- **Face Detailer:** Enhance facial features with separate treatment for small and large faces\n- **Hand Detailer:** Fix and improve hand anatomy in generated images\n\n#### Creative Upscaling\n- **CCSR Upscaler:** SD 2.1-based upscaler for intricate pattern preservation\n- **SUPIR Upscaler:** SDXL-based upscaler with creative options and LoRA support\n\n#### Image Finishing\nFinal enhancement tools including colorization, color correction, LUT application, film grain effects, and watermarking.\n\n### Advanced Conditioning\n\n#### LoRAs\nApply style and content modifications through LoRAs for all supported models, organized by categories for easier selection.\n\n#### ControlNet\nUse up to four concurrent ControlNet models for precise control over generations, with support for various preprocessors.\n\n#### IPAdapter & Redux\nGenerate variants and compositions from reference images, allowing for consistent character generation and style transfer.\n\n### Evaluation Tools\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1198/readme07.webp\" alt=\"AP Workflow\" width=\"550\"/>\n\n- **Image Comparer:** Compare multiple generation versions side-by-side\n- **Face Analyzer:** Automatically select images with facial landmarks similar to references\n- **Aesthetic Score Predictor:** Rank images based on aesthetic quality\n- **Debugging Tools:** Extensive logging and preview functions for workflow understanding\n\n## Latest Features\n\nAP Workflow (APW) is continuously updated with new capabilities. For details on the latest features in APW 12.0, including video generation enhancements, SD3.5 support, and workflow improvements, see the [official change log](https://perilli.com/ai/comfyui-ap-workflow/#apw_ga_newfeatures).\n\n## Credits and License\n\nAP Workflows are designed by Alessandro Perilli. For more information, updates, and installation instructions, visit [Alessandro's official website](https://perilli.com/ai/comfyui-ap-workflow/).\n\nSpecial thanks to all the custom node creators mentioned in AP Workflow (APW) documentation, including:\n@rgthree\n@kijai\n@Kosinkadink\n@matan1905\n@cubiq\n@ltdrdata\n@glibsonoran\n@crystool\n@huchenlei\n"
    },
    {
        "id": "1199",
        "readme": "## ComfyUI Wan 2.1 Workflow Description\n\n### 1. What is Wan 2.1?\nThe **ComfyUI Wan 2.1** workflow is a cutting-edge video generation pipeline that leverages the latest Wan 2.1 models to create high-quality videos from text prompts or/and base images. Wan 2.1 supports Text-to-Video (T2V) and Image-to-Video (I2V) generation, producing 5-second videos with natural motion and professional-grade quality. Wan 2.1 sets a new benchmark for AI video generation, outperforming open-source and commercial alternatives. The Wan 2.1 14B model pushes the limits further, delivering exceptional results up to 720P.\n\n### 2. Benefits and Capabilities of Wan 2.1\n- **High-quality output:** Generates 480P to 720P videos with realistic motion and high-fidelity textures.\n- **Hardware accessibility:** The lightweight Wan 2.1 1.3B model requires only 8.19GB VRAM, making it compatible with most modern GPUs (which are provided by RunComfy here!).\n- **Versatile generation:** Wan 2.1 Supports both Text-to-Video (T2V) and Image-to-Video (I2V) workflows.\n- **Multilingual support:** Wan 2.1 is the **first** video model capable of generating both Chinese and English text within videos.\n- **VAE efficiency:** The Wan-VAE backend efficiently handles 1080P videos while preserving temporal consistency.\n- **Fast processing:** The Wan 2.1 1.3B model delivers quick results while maintaining quality.\n\n### 3. How to Use Wan 2.1\n\n#### 3.1 Wan 2.1 Generation Methods\n\n<p align=\"center\">\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1199/readme01.webp\" alt=\"Wan 2.1\" width=\"750\"/>\n</p>\n\n##### Primary Wan 2.1 Generation Method (disabled by default): Text-to-Video\n- **Inputs:** Text prompt\n- **Best for:** Creating videos from scratch using textual descriptions\n- **Characteristics:**\n  - Uses the Wan 2.1 1.3B model for faster generation\n  - Creates 33-frame (5-second) videos at 480P resolution\n  - Optimized for smooth motion in short clips\n\n<p align=\"center\">\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1199/readme02.webp\" alt=\"Wan 2.1\" width=\"750\"/>\n</p>\n\n##### Advanced Wan 2.1 Method (enabled by default): Image-to-Video with Text Prompt\n- **Inputs:** Base image + text prompt\n- **Best for:** Animating still images while guiding motion with a prompt\n- **Characteristics:**\n  - Preserves visual elements of the input image\n  - Allows text control over motion direction\n  - Uses the Wan 2.1 14B model for higher fidelity\n  - Creates 33-frame videos at 512x512 resolution\n\n###### Example Workflow:\n1. In **CLIPTextEncode (Positive Prompt / Negative Prompt):** Enter your scene description (e.g., \"a fox moving quickly in a beautiful winter landscape with trees and mountains during daytime, tracking camera\").\n2. In **Load Image:** Upload your base image.\n3. For further refinement (optional):\n   - In **KSampler:** Adjust `steps` (default: 30) for a quality vs. speed balance.\n   - In **ModelSamplingSD3:** Modify `scale` value (default: 8) for prompt adherence.\n4. Click **Queue Prompt** to start the generation.\n5. In **SaveAnimatedWEBP** find your output preview (also saved in ComfyUI > Output folder).\n\n#### 3.2 Parameter Reference for Wan 2.1\n- **KSampler:**\n  - `steps`: 20-30 (higher values improve quality but increase time)\n  - `cfg`: 6.0 (controls prompt adherence strength)\n  - `scheduler`: \"simple\" (determines noise scheduling approach)\n  - `sampler_name`: \"uni_pc\" (recommended sampler for Wan 2.1)\n  <p align=\"center\">\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1199/readme03.webp\" alt=\"Wan 2.1\" width=\"350\"/>\n  </p>\n- **WanImageToVideo:**\n  - `width/height`: 512 (output resolution)\n  - `length`: 33 (frames per video)\n  - `batch_size`: 1 (number of videos per run)\n- **ModelSamplingSD3:**\n  - `scale`: 8 (controls guidance adherence)\n- **EmptyHunyuanLatentVideo:**\n  - `width/height`: 832/480 (T2V output resolution)\n  - `length`: 33 (frames per video)\n  - `batch_size`: 1 (number of videos per run)\n  <p align=\"center\">\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1199/readme04.webp\" alt=\"Wan 2.1\" width=\"350\"/>\n  </p>\n\n#### 3.3 Advanced Optimization with Wan 2.1\n- **Memory Optimization:**\n  - Use the Wan 2.1 1.3B model for faster generation with lower VRAM requirements.\n  - Reduce resolution (e.g., 512x320) for quicker processing.\n  - Decrease frame count for shorter and faster renders.\n- **Quality Optimization:**\n  - Use the Wan 2.1 14B model for higher-quality output.\n  - Increase KSampler steps to 30-40 for more refined results.\n  - Utilize Image-to-Video with a high-quality base image for the best fidelity.\n\n## More Information\nFor additional details on Wan 2.1, visit the [Wan-Video GitHub repository](https://github.com/Wan-Video/Wan2.1).\n\n### Credits\nThe Wan 2.1 model was developed by the Wan Team, and the ComfyUI integration was created by the original developers. Full credit goes to these innovators for advancing AI-powered video generation.\n"
    },
    {
        "id": "1200",
        "readme": "## ComfyUI DiffuEraser Video Inpainting Workflow Description\n\n### What is the ComfyUI DiffuEraser Workflow?\n**DiffuEraser** is a cutting-edge video inpainting solution that seamlessly removes unwanted objects from videos while preserving temporal consistency. Using a powerful diffusion-based inpainting model, DiffuEraser reconstructs missing areas with contextually accurate content. This workflow integrates with **Segment Anything 2 (SAM2)** for automatic mask generation, eliminating the need for manually created masks.\n\nDiffuEraser utilizes a denoising UNet alongside an auxiliary BrushNet branch, integrating temporal attention to maintain frame consistency. By leveraging prior information, it reduces hallucinations and artifacts, ensuring flawless object removal.\n\nThe [**SAM2** integration](https://www.runcomfy.com/comfyui-workflows/comfyui-segment-anything-v2-SAM2-image-and-video-segmentation) by *Runcomfy* Crew automates mask creation using a point-selection interface, enabling users to mark objects for removal without manually creating masks. This significantly streamlines the inpainting workflow.\n\n### Benefits of DiffuEraser Workflow\n- High-quality reconstruction with natural scene blending.\n- Automatic mask generation via SAM2, reducing manual effort.\n- Temporal consistency for seamless inpainting across frames.\n- Flexible object selection with a point-based interface.\n- Professional-grade results with minimal user input.\n- Suppressed hallucinations by leveraging prior information.\n- Compatible with standard video formats for effortless integration.\n\n### How to Use DiffuEraser Workflow\n\n#### Object Removal with DiffuEraser\n##### Primary Generation Method: SAM2 + DiffuEraser\n\n<p align=\"center\">\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1200/readme01.webp\" alt=\"DiffuEraser\" width=\"750\"/>\n</p>\n\n- **Inputs:** Original video, frames for object selection via point coordinates\n- **Best for:** Removing objects, people, watermarks, or other unwanted elements\n- **Characteristics:**\n  - Uses SAM2 for automatic mask generation\n  - Produces natural inpainting with high visual fidelity\n  - Ensures temporal consistency across all frames\n\n\n##### Example Workflow\n1. **Prepare inputs**\n   - In **Load Video Node:** Upload your source video\n   - In **Points Editor:** Load first frame to add positive points (green) to mark objects for removal\n2. **Refinement (Optional)**\n   - In **DiffuEraserSampler** adjust `mask_dilation_iter` for precise masking\n   - Modify `crf` in **Video Combine** for higher output quality\n3. **Output**\n   - In **Video Combine:** find the preview and save it to your local machine\n\n\n##### Alternative Method: Manual Mask Creation\n\n<p align=\"center\">\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1200/readme02.webp\" alt=\"DiffuEraser\" width=\"750\"/>\n</p>\n\n- **Inputs:** Pre-created mask video.\n- **Best for:** Users needing precise control over masked regions.\n- **Characteristics:**\n  - Requires manual mask creation.\n  - Offers full control over object selection.\n  - Ideal for complex scenes or artistic workflows.\n\n#### Parameter Reference for DiffuEraser\n\n<p align=\"center\">\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1200/readme04.webp\" alt=\"DiffuEraser\" width=\"750\"/>\n</p>\n\n- **DiffuEraserLoader:**\n  - `checkpoint`: [SD1.5/v1-5-pruned-emaonly.ckpt] - Stable Diffusion base model.\n  - `lora`: [flux/flux.1-turbo-alpha/diffusion_pytorch_model.safetensors] - LoRA for enhanced inpainting.\n- **DiffuEraserSampler:**\n  - `seed`: [random] - Controls generation variability.\n  - `num_inference_steps`: [2] - Higher values improve quality.\n  - `guidance_scale`: [0] - Controls adherence to prior information.\n  - `video_length`: [10] - Defines processed frames.\n  - `mask_dilation_iter`: [8] - Expands mask coverage.\n  - `ref_stride`: [10] - Reference frame stride for temporal consistency.\n  - `neighbor_length`: [10] - Defines frames used for reference.\n  - `subvideo_length`: [50] - Max frames processed in a batch.\n  - `seg_repo`: [briaai/RMBG-2.0] - Background removal model.\n- **Video Combine:**\n  - `frame_rate`: [1] - Matches source frame rate.\n  - `format`: [video/h264-mp4] - Output format.\n  - `crf`: [19] - Controls video compression quality.\n\n#### Advanced Optimization with DiffuEraser\n- **Performance Optimization:**\n  - Reduce `subvideo_length` for faster processing.\n  - Lower `num_inference_steps` to speed up generation.\n- **Quality Enhancements:**\n  - Increase `mask_dilation_iter` to improve mask coverage.\n  - Adjust `neighbor_length` for moving object refinements.\n\n### Usage Tips\n1. Use **Points Editor** to mark multiple points on the target object.\n2. Add **negative points (red)** if SAM2 includes unwanted areas.\n3. For **moving objects,** mark points across several frames.\n4. Simpler backgrounds yield better inpainting results.\n5. Lower `video_length` and `subvideo_length` for longer videos to avoid memory issues.\n\n## More Information\n- For detailed guides and updates on DiffuEraser, visit [DiffuEraser](https://lixiaowen-xw.github.io/DiffuEraser-page/)\n- For ComfyUI integration of DiffuEraser, visit [ComfyUI DiffuEraser](https://github.com/smthemex/ComfyUI_DiffuEraser)\n- For detailed guides on SAM2, visit [SAM2 on RunComfy](https://www.runcomfy.com/comfyui-workflows/comfyui-segment-anything-v2-SAM2-image-and-video-segmentation)\n\n### Credit to Original Authors\nDiffuEraser was created by *Xiaowen Li*, *Haolan Xue*, *Peiran Ren*, and *Liefeng Bo* from Tongyi Lab, Alibaba Group, with ComfyUI integration by *smthemex*. Runcomfy Crew enhanced the workflow with automatic mask generation via SAM2. All credit goes to the original authors for their groundbreaking contributions.\n"
    },
    {
        "id": "1201",
        "readme": "## ComfyUI SkyReels V1 Workflow Description\n\n### 1. What is SkyReels V1 Workflow?\nSkyReels V1 is an advanced human-centric video generation workflow for ComfyUI, designed to create realistic human movements and expressions from text prompts or images. Built on the SkyReels-V1-Hunyuan-I2V model, which has been fine-tuned on over 10 million high-quality film and television clips, SkyReels generates Hollywood-caliber human videos with authentic facial expressions and natural motion at high resolution (544×960).\n\n### 2. Benefits of SkyReels V1\n- **SkyReels Cinema-quality output:** Produces professional-grade videos with realistic human motion and high production value\n- **SkyReels Human expression support:** Supports 33 distinct facial expressions for emotionally realistic characters\n- **Natural movement:** SkyReels includes 400+ true-to-life human movements\n- **High resolution:** SkyReels generates videos at 544×960 resolution with smooth 24fps playback\n- **Optimized performance:** SkyReels features advanced compilation options for faster rendering\n- **Easy customization:** SkyReels is fully compatible with visual workflows and adjustable parameters\n\n### 3. How to Use SkyReels V1\n\n#### 3.1 Generation with SkyReels\n##### Image-to-Video with Text Prompt\n- **Inputs:** Reference image and text prompt\n- **Best for:** Generating SkyReels human-centric videos with lifelike expressions and movement\n- **Characteristics:**\n  - SkyReels realistic human motion and emotional expressions\n  - Cinematic composition and professional framing\n  - Stable SkyReels video generation with minimal artifacts\n\n<p align=\"center\">\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1201/readme01.webp\" alt=\"SkyReels\" width=\"750\"/>\n</p>\n\n###### Generation Example\n1. **Prepare SkyReels inputs:**\n   - In **Load Image,** upload your base image\n   - In **CLIPTextEncode,** enter a prompt describing desired motions\n2. **SkyReels Refinement (Optional):**\n   - In **EmptyHunyuanLatentVideo,** set desired dimensions and frames (default: 544×544, 49 frames)\n   - In **FluxGuidance,** adjust `guidance` values for stronger prompt adherence\n3. **Generate SkyReels output:**\n   - Click **Queue Prompt** to run the workflow\n   - In **Video Combine,** check the preview output (also saved in *ComfyUI > Output* folder)  \n\n#### 3.2 Parameter Reference for SkyReels V1\n- **EmptyHunyuanLatentVideo:**\n  - `length`: [49] - Defines frame count\n  - `batch_size`: [1] - Sets batch generation size\n- **SamplerCustom (First stage):**\n  - `noise_seed`: [15] - Controls randomness of output\n  - `cfg`: [7.0] - Controls prompt adherence strength\n- **SamplerCustom (Second stage):**\n  - `add_noise`: [false] - No additional noise added during refinement\n\n<p align=\"center\">\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1201/readme02.webp\" alt=\"SkyReels\" width=\"300\"/>\n</p>\n\n### 4. SkyReels Tips\n- **Result variability:** SkyReels can generate different results per seed. If unsatisfied, try changing the seed\n- **SamplerCustom settings:** Run the workflow with default parameters (15 steps for the first sampler, 0 steps for the refining sampler)\n- **Negative prompts:** Use negative prompts (e.g., \"blurry, bad quality video, chaotic movement\") to refine output quality\n\n## More Information\nFor the latest updates and details on SkyReels V1, visit [SkyReels V1 on Hugging Face](https://huggingface.co/Skywork/SkyReels-V1-Hunyuan-I2V).\n\n### Acknowledgments\nSkyReels V1 was developed by the SkyReels-AI team, based on the Hunyuan base model. The ComfyUI integration for SkyReels, created by ***Kijai***, enables seamless workflow compatibility by converting the SkyReels model from diffusers format into HunyuanVideo format. Full credit goes to the original SkyReels developers for pioneering human-centric video generation.\n"
    },
    {
        "id": "1202",
        "readme": "## ComfyUI Hunyuan Image-to-Video Workflow Description\n\n### 1. What is Hunyuan Image-to-Video Workflow?\nThe **Hunyuan Image-to-Video** workflow is a powerful pipeline designed to transform still images into high-quality videos with natural motion. Developed by *Tencent*, this cutting-edge technology enables users to create cinematic animations with smooth 24fps playback at resolutions up to 720p. By leveraging latent image concatenation and a Multimodal Large Language Model, **Hunyuan Image-to-Video** interprets image content and applies consistent motion patterns based on text prompts.\n\n### 2. Benefits of Hunyuan Image-to-Video:\n* **High-Resolution Output** - Generates videos up to 720p at 24fps\n* **Natural Motion Generation** - Creates fluid, realistic animations from static images\n* **Text-Guided Animation** - Uses text prompts to guide motion and visual effects\n* **Cinematic Quality** - Produces professional-grade videos with high fidelity\n* **Customizable Effects** - Supports LoRA-trained effects like hair growth, facial expressions, and style adjustments\n* **Optimized Memory Usage** - Utilizes FP8 weights for better resource management\n\n### 3. How to Use Hunyuan Image-to-Video Workflow\n\n#### 3.1 Generation Methods with Hunyuan Image-to-Video\n\n###### Example Workflow:\n\n<p align=\"center\">\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1202/readme01.webp\" alt=\"Hunyuan Image-to-Video\" width=\"750\"/>\n</p>\n\n1. **Prepare Inputs**\n   - In Load Image: Upload your source image\n2. **Enter Motion Description**\n   - In HyVideo I2V Encode: Input a descriptive text prompt for the desired motion\n3. **Refinement (Optional)**\n   - In HunyuanVideo Sampler: Adjust `frames` to control video length (default: 129 frames ≈ 5 seconds)\n   - In HunyuanVideo TeaCache: Modify `cache_factor` for optimized memory usage\n   - In HunyuanVideo Enhance A Video: Enable for temporal consistency and flickering reduction\n4. **Output**\n   - In Video Combine: Check the preview and find saved result in ComfyUI > Output folder\n\n#### 3.2 Parameter Reference for Hunyuan Image-to-Video\n\n<p align=\"center\">\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1202/readme02.webp\" alt=\"Hunyuan Image-to-Video\" width=\"750\"/>\n</p>\n\n* **HunyuanVideo Model Loader**\n  - `model_name`: **hunyuan_video_I2V_fp8_e4m3fn.safetensors** - Core model for image-to-video conversion\n  - `weight_precision`: **bf16** - Defines precision level for model weights\n  - `scale_weights`: **fp8_e4m3fn** - Optimizes memory use\n  - `attention_implementation`: **flash_attn_varlen** - Controls attention processing efficiency\n* **HunyuanVideo Sampler**\n  - `frames`: **129** - Number of frames (5.4 seconds at 24fps)\n  - `steps`: **20** - Sampling steps (higher values improve quality)\n  - `cfg`: **6** - Controls prompt adherence strength\n  - `seed`: **varies** - Ensures generation consistency\n* **HyVideo I2V Encode**\n  - `prompt`: **[text field]** - Descriptive prompt for motion and style\n  - `add_prepend`: **true** - Enables automatic text formatting\n\n#### 3.3 Advanced Optimization with Hunyuan Image-to-Video\n* **Memory Optimization**\n  - HunyuanVideo BlockSwap: CPU offloading for VRAM efficiency\n  - HunyuanVideo TeaCache: Controls cache behavior to balance memory vs. speed\n  - scale_weights: FP8 weights (`e4m3fn format`) for memory reduction\n* **Speed Optimization**\n  - HunyuanVideo Torch Compile Settings: Enables Torch compile for faster processing\n  - attention_implementation: Selects efficient attention mechanisms for performance boost\n  - offload_device: Configures GPU/CPU memory management\n\n## More Information\nFor more details on the **Hunyuan Image-to-Video** workflow, visit [Tencent's HunyuanVideo-I2V repository](https://github.com/tencent/HunyuanVideo-I2V).\n\n### Acknowledgements\nThis workflow is powered by **Hunyuan Image-to-Video**, developed by *Tencent*. The ComfyUI integration includes wrapper nodes created by *Kijai*, enabling advanced features such as context windowing and direct image embedding support. Full credit goes to the original creators for their contributions to **Hunyuan Image-to-Video** workflow!\n\n"
    },
    {
        "id": "1203",
        "readme": "Wan 2.1 LoRA enhances the base Wan 2.1 model by enabling fine-tuned control over motion dynamics, artistic styles, and specialized video outputs with minimal computational cost. Built for Text-to-Video, Image-to-Video, and Video Editing, the Wan 2.1 LoRA optimizations allow users to personalize generation results while maintaining Wan 2.1's high fidelity. With efficient training requiring lower VRAM, creators can adapt Wan 2.1 LoRA for cinematic, stylized, or domain-specific video projects, pushing the boundaries of AI-driven video generation with greater precision and flexibility.\n\nThe Wan 2.1 LoRA collection currently features a diverse range of special effects across multiple categories:\n\n* **Physical Transformation:** Squish, Rotate, Inflate, Deflate, Crush\n* **Thematic Transformation:** Cakeify\n* **Action Effects:** Gun-Shooting\n* **Appearance Enhancement:** Muscle, Bride, Puppy, Baby, VIP\n* **Artistic Style:** Mona-Lisa, Painting\n* **Character Transformation:** Princess, Disney-Princess, Snow-White, Pirate-Captain, Samurai, Assassin, Warrior\n* **Scene & Atmosphere**: Jungle, Zen, Classy\n\n**Download Wan 2.1 LoRA models here:** [Wan2.1 14B 480p I2V LoRAs](https://huggingface.co/collections/Remade-AI/wan21-14b-480p-i2v-loras-67d0e26f08092436b585919b)\n\nThe Wan 2.1 LoRA workflow enhances video generation by allowing users to apply fine-tuned Wan 2.1 LoRA models for greater control over motion, style, and consistency. Instead of relying solely on text or image inputs, users can load specialized Wan 2.1 LoRAs to refine visual aesthetics, maintain character fidelity, or introduce unique artistic effects. This Wan 2.1 LoRA workflow streamlines customization, making it easier to achieve cohesive, high-quality video outputs while leveraging Wan 2.1's capabilities.\n\n\n## How to Use Wan 2.1 LoRA Workflow?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1203/readme01.webp\" alt=\"Wan 2.1 Lora\" width=\"750\"/>\n</p>\n\nNodes in the Wan 2.1 LoRA workflow are color-coded for clarity:\n\n1. Green - Inputs for implementation \n2. Purple - Wan 2.1 Models and LoRA\n3. Pink - Wan 2.1 Sampler  \n4. Red - VAE + Decoding\n5. Grey - Output Video\n\nEnter your Positive text, Upload Image and Load any Wan 2.1 LoRA you Desire in the green nodes and adjust video settings, such as duration and resolution, in the cyan blue node in clip vision group.\n\n### Input (Wan 2.1 LoRA, Image and Text)\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1203/readme02.webp\" alt=\"Wan 2.1 Lora\" width=\"450\"/>\n</p>\n\n1. Download your desired Wan 2.1 LoRA models from [Wan2.1 14B 480p I2V LoRAs](https://huggingface.co/collections/Remade-AI/wan21-14b-480p-i2v-loras-67d0e26f08092436b585919b)\n2. Refresh the browser to see them in the WanVideo Lora Select list\n3. Enter your desired prompts and trigger keywords in the text box to trigger the Wan 2.1 LoRA\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1203/readme03.webp\" alt=\"Wan 2.1 Lora\" width=\"450\"/>\n</p>\n\nYou can chain multiple Wan 2.1 LoRA models by duplicating the WanVideo LoRA node and connecting them as you do with standard SD models.\n\n### Sampler for Wan 2.1 LoRA\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1203/readme04.webp\" alt=\"Wan 2.1 Lora\" width=\"450\"/>\n</p>\n\nWhen working with Wan 2.1 LoRA, use these recommended settings:\n\n1. steps: 20-30, higher values improve quality but increase time\n2. cfg: 6.0, controls prompt adherence strength\n3. scheduler: \"simple\", determines noise scheduling approach\n4. sampler_name: \"uni_pc\", recommended sampler for Wan 2.1\n\n### Set Resolutions, VAE and Clip Vision for Wan 2.1 LoRA\n\nIn this Group, the VAE and clip vision models will auto-download on first run. Please allow 3-5 minutes for the download to complete in your temporary storage.\n\nIn the cyan blue node, you can set the video's frame duration and resolution.\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1203/readme05.webp\" alt=\"Wan 2.1 Lora\" width=\"450\"/>\n</p>\n\nLinks for Wan 2.1 Base Models: https://huggingface.co/Kijai/WanVideo_comfy/tree/main\n\n### Wan 2.1 LoRA Outputs \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1203/readme06.webp\" alt=\"Wan 2.1 Lora\" width=\"450\"/>\n</p>\n\nThe rendered Wan 2.1 LoRA video will be saved in the Outputs folder in ComfyUI.\n\n---\n\nUnlock the full potential of Wan 2.1 LoRA and bring your video creations to life with enhanced style, consistency, and control. Whether you're crafting cinematic sequences, AI-driven animations, or artistic storytelling, Wan 2.1 LoRA empowers you to push the boundaries of video generation. Experiment with different LoRA models, refine your visuals, and create stunning, high-quality videos with ease. Try Wan 2.1 LoRA today and elevate your video generation workflow to new heights!\n"
    },
    {
        "id": "1204",
        "readme": "## 1. What is PuLID Flux II?\nPuLID Flux II is an advanced consistent character technique for AI image generation in ComfyUI. It solves the critical \"model pollution\" issue that plagued earlier character insertion methods, allowing you to organically integrate specific characters into AI-generated art while maintaining the original model's artistic style, lighting, and composition integrity. Think of PuLID Flux II as giving the AI a reference photo and saying, \"Preserve this person's unique features while maintaining your artistic vision for everything else.\"\n\n## 2. Key Improvements in PuLID Flux II:\n* **Zero Model Pollution:** Major improvement over the original - insert characters without compromising background, style, or lighting quality  \n* **TeaCache Compatibility:** Dramatically accelerates workflows by supporting caching of intermediate results  \n* **WaveSpeed Support:** Turbocharges generation times for production environments  \n* **Attention Mask Control:** Fine-tunes how the model focuses on different image elements for enhanced detail  \n\n\n## 3. How to Use ComfyUI PuLID Flux II\n\n### 3.1 Creating Dual Character Images with PuLID Flux II\nThis PuLID Flux II workflow is designed to create scenes featuring two different people with perfectly consistent characters:\n\n**Basic Setup for Dual Character Generation with PuLID Flux II:**\n1. Upload your reference images:\n   - Add your first person's reference image to the left `Load Image` node  \n   - Add your second person's reference image to the right `Load Image` node  \n2. Create your scene:\n   - Enter a prompt describing a scene with two people in the `CLIP Text Encode` node  \n3. Configure character consistency in PuLID Flux II:\n   - Adjust both `Apply PuLID Flux` nodes' weight parameters (higher values = stronger character consistency)  \n   - Fine-tune `FluxGuidance` settings (recommendation: start with 2.5–3.5)\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1204/readme01.webp\" alt=\"PuLID Flux II\" width=\"650\"/>\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1204/readme02.webp\" alt=\"PuLID Flux II\" width=\"550\"/>\n4. Generate your image with `Queue`  \n\n> **TIP:** When using PuLID Flux II to generate scenes with two people, the order of your `Load Image` nodes is important. The left reference image typically corresponds to the first person described in your prompt, and the right reference image to the second person. Using descriptive characteristics in your prompt may help guide the model to place characters correctly. Note that attention mask settings can also influence the final character placement.\n\n**Tips for Best Results with PuLID Flux II:**\n- Use clear, front-facing reference photos with good lighting  \n- Create prompts that naturally incorporate two people  \n- Balance the weight parameters for consistent character representation across both subjects  \n\n### 3.2 Parameter Reference for PuLID Flux II\n\n**Apply PuLID Flux Node in PuLID Flux II:**\n- `weight`: Controls character consistency strength (1.0 = strongest)  \n- `start_at`: Controls when in the diffusion process to begin applying character features (0.0 = start from the beginning of generation)  \n- `end_at`: Controls when in the diffusion process to stop applying character features (1.0 = continue until the end of generation)  \n- `attn_mask`: Enables attention masking for enhanced detail control  \n\n**Understanding the Timing Parameters in PuLID Flux II:**\nAI image generation happens through a multi-step diffusion process (typically 20–50 steps). The `start_at` and `end_at` parameters control when during this process the character features are applied:\n- Default values (0.0 to 1.0) apply character features throughout the entire generation process  \n- Adjusting these values (e.g., 0.3 to 0.7) applies character features only during specific portions of the generation  \n- This fine control can help balance character consistency with artistic freedom\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1204/readme03.webp\" alt=\"PuLID Flux II\" width=\"650\"/>\n\n**Masking System in PuLID Flux II:**\nThe PuLID Flux II workflow includes a masking system (`SolidMask`, `MaskComposite`, `InvertMask` nodes) that provides fine-grained control over which areas of the image receive character consistency:\n- This advanced feature allows you to selectively apply character features to specific regions  \n- Useful for complex scenes where you want character consistency only on faces, not backgrounds  \n- For most use cases, the default settings work well without manual mask adjustments\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1204/readme04.webp\" alt=\"PuLID Flux II\" width=\"650\"/>  \n\n**WaveSpeed & TeaCache Integration with PuLID Flux II:**\n- This PuLID Flux II workflow has already been optimized with proper node connections for both WaveSpeed and TeaCache  \n- The correct order of nodes is pre-configured to ensure maximum performance  \n- When using WaveSpeed with PuLID Flux II, remember that `fp8_e5m2` data type generally provides the best balance of speed and quality  \n\n### 3.3 Advanced Optimization Tips for PuLID Flux II\n\n**Balancing Dual Characters in PuLID Flux II:**\n- For equal prominence, use similar weight values for both characters  \n- To make one character more dominant, increase its weight value relative to the other  \n- Start with medium values (0.6–0.8) and adjust based on results  \n\n**Timing Control for Better Results with PuLID Flux II:**\n- For stronger character consistency with more creative composition, try `start_at`=0.0, `end_at`=0.8  \n- For stronger artistic style with subtle character hints, try `start_at`=0.2, `end_at`=1.0  \n- Experiment with different combinations to find the perfect balance for your specific needs  \n\n**Performance Tuning for PuLID Flux II:**\n- Enable TeaCache for repeat generations of similar prompts  \n- Utilize WaveSpeed for rapid results in production environments  \n- For other configuration options and single-character workflows, refer to the [official PuLID Flux II repository](https://github.com/lldacing/ComfyUI_PuLID_Flux_ll)  \n\n## Acknowledgements\nThis workflow is based on **PuLID Flux II** developed by **lldacing**, which improves upon the original PuLID Flux by solving the model pollution problem. The PuLID Flux II integration enables high-fidelity character consistency while maintaining artistic integrity across diverse generation scenarios. Full credit goes to the original author for their awesome work on PuLID Flux II.\n "
    },
    {
        "id": "1205",
        "readme": "## 1. What is the ComfyUI MatAnyone Workflow?\nThe **ComfyUI MatAnyone** workflow integrates the powerful MatAnyone AI video matting model into the ComfyUI environment. Developed to remove video backgrounds using just one initial mask frame, MatAnyone provides precise, consistent, and high-quality alpha matte extraction across entire videos. This workflow significantly simplifies the traditional frame-by-frame masking process, delivering remarkable stability and clarity in subject isolation.\n  \n## 2. Benefits of ComfyUI MatAnyone:\n* **Single-Frame Efficiency:**  \n  MatAnyone requires only one mask frame to remove backgrounds across thousands of video frames.\n* **Automatic Mask Generation:**  \n  MatAnyone automatically generates a high-quality initial mask using the built-in RemBG feature, simplifying your workflow even further.\n* **High Precision Edges:**  \n  MatAnyone ensures exceptional detail preservation and crisp edge accuracy in video matting.\n* **Optimized Workflow:**  \n  Designed specifically for filmmakers, VFX artists, and content creators demanding efficient, cinema-ready results.\n\n## 3. How to Use the ComfyUI MatAnyone Workflow\n\n### 3.1 Generation Steps with ComfyUI MatAnyone:\n\n**Example Setup for MatAnyone:**\n\n1. **Prepare Video and Mask:**\n   - In the `Load Video` node, upload your source video.\n2. **Apply Mask with MatAnyone: (no action required)**\n   - Connect your prepared mask frame to the **MatAnyone Video Kytra** node to initiate precise background removal automatically across the entire video.\n3. **Final Video:**\n   - The output video with background removed is in the `Video Combine` node.\n\n> **NOTE:**  \n>By default, MatAnyone automatically generates the initial mask using the **RemBG Session** node. If you prefer using your own mask, simply replace this node with the `\"Load Image (as Mask)\"` node.\n\n### 3.2 Parameter Reference for ComfyUI MatAnyone:\n\n**RemBG Session Node:**  \nGenerates an initial high-quality mask using the RemBG model (`isnet-general-use`) to simplify the mask extraction process.\n\n**MatAnyone Video Kytra Node:**  \nApplies single-mask background removal to videos.\n- `warmup_frames`: Number of initial frames for model initialization.\n- `erode_kernel`: Erosion kernel size to refine alpha matte edges.\n- `dilate_kernel`: Dilation kernel size to adjust boundary smoothing.\n- `bg_red`, `bg_green`, `bg_blue`: RGB values for replacement background color.\n\n### 3.3 Advanced Tips for ComfyUI MatAnyone:\n\n- **Mask Frame Quality:**  \n  Ensure your initial mask frame has clear, sharp boundaries to optimize MatAnyone's matting accuracy.\n- **Adjustment of Kernel Sizes:**  \n  Use erosion and dilation parameters to precisely tune edge sharpness and smoothness depending on your source video.\n- **Manual Mask Recommended for Optimal Results:**  \n  While automatic mask generation via the RemBG Session node is convenient, providing your own manually created mask typically yields superior matting results.\n\n### 3.4 Important Notes for Manual Mask Uploads:\n\nWhen uploading your mask using the `\"Load Image (as Mask)\"` node, select the appropriate channel according to your mask type to ensure accurate results. Incorrect selection can cause unexpected visual artifacts.\n\n- **RGB black-and-white masks (no transparency):** choose **RGB** channel\n- **True Alpha masks (with transparency):** choose **Alpha** channel\n- **Colored RGB images:** not intended as masks\n\n## More Information about MatAnyone\nFor additional details and technical references:\n- ComfyUI implementation of MatAnyone by [KytraScript](https://github.com/KytraScript/ComfyUI_MatAnyone_Kytra)\n- Original MatAnyone matting framework [MatAnyone](https://github.com/pq-yang/MatAnyone)\n\n## Acknowledgements\nThis workflow is powered by the **MatAnyone** model developed by **Peiqing Yang, Shangchen Zhou, Jixin Zhao, Qingyi Tao, and Chen Change Loy** from Nanyang Technological University and SenseTime Research. The **ComfyUI MatAnyone integration** is provided by **KytraScript**, enabling efficient single-frame mask-based background removal. Full credit goes to the original authors for their pioneering work.\n"
    },
    {
        "id": "1206",
        "readme": "## ComfyUI Flux-TTP-Upscale | Advanced Face Restore & 4K Image Enhancement\n\n### 1. What is the ComfyUI Flux-TTP-Upscale Face Restore Workflow?\n\nThe **Flux-TTP-Upscale** workflow offers an advanced **Face Restore** pipeline within the ComfyUI environment. It integrates **Flux's face restoration technology** with **TTP (Tile-to-Patch)** enhancement to fix distorted or low-quality faces in AI-generated images. This is especially effective for group portraits, profile shots, or any visuals with facial artifacts.\n\nBy combining **FluxGuidance**, **tile-aware image enhancement**, and **LoRA-based identity control**, Flux-TTP-Upscale Face Restore delivers reliable **Face Restore** performance while upscaling to crisp 4K resolution.\n\n### 2. Key Face Restore Features of ComfyUI Flux-TTP-Upscale\n\n- **High-Precision Face Restore:**\n  Detects and restores small or distorted faces without harming overall image composition.\n- **4K Image Upscaling:**\n  Enhances resolution through TTP tile workflows and super-resolution models.\n- **Tile-Based Patch Enhancement:**\n  Splits the image into tiles to reduce artifacting, ensuring local **Face Restore** improvements blend seamlessly.\n- **LoRA Switching for Identity Preservation:**\n  Select the right LoRA models for Asian or non-Asian faces to improve **Face Restore** accuracy across different ethnicities.\n\n### 3. Getting Started with the Face Restore Workflow\n\n> **IMPORTANT NOTE:**\n> This Face Restore workflow handles both image enhancement and face repair simultaneously. Proper input and model selection ensure optimal results.\n\n**Quick Start Guide:**\n\n1. **Upload Image for Face Restore:**\n   Use the `Load Image` node to input a low-resolution portrait, group photo, or any AI-generated image needing facial repair.\n2. **Choose the Correct LoRA Model:**\n   - Use **flux1-dev-fp8** for restoring Asian faces.\n   - Use original **flux** for general or non-Asian faces.\n3. **Preprocessing Settings (Optional):**\n   Images are automatically resized to 1024x1024 and scaled to an 8MP target for better **Face Restore** quality.\n4. **Run the Face Restore Pipeline:**\n   Click `Queue Prompt` to initiate the restoration and upscale process.\n5. **Save Your Output:**\n   Restored images are saved via the `Save Image` node.\n\n### 4. Node Reference & Parameters for Face Restore\n\n#### Guidance and Denoising\n\n- `FluxGuidance`: Drives facial restoration accuracy during generation.\n- `BasicGuider`: Adds global image consistency around the restored face.\n- `SamplerCustomAdvanced`: Uses `euler` sampler with fine-tuned denoise strength (`denoise = 0.3`).\n\n#### Preprocessing for Better Face Restore\n\n- `Resize Image`: Sets up correct image dimensions for effective tile repair.\n- `Upscale Model`: Uses `4xNMKD-Superscale` to refine face patches.\n- `Scale to Total Pixels`: Ensures final resolution is high enough for detailed **Face Restore**.\n\n#### Tile-to-Patch (TTP) Enhancements\n\n- `TTP_Image_Tile_Batch`: Breaks down the image into tiles for localized **Face Restore**.\n- `TTP_Image_Assy`: Rebuilds a seamless image after tile-level repair, using 128px padding.\n\n#### Interrogate\n\n- `Joy Caption Two`: Automatically describes restored images to help validate **Face Restore** results.\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1206/readme01.webp\" alt=\"Face Restore\" width=\"28%\" style={{display:'inline-block', marginRight: '2%' }} />\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1206/readme02.webp\" alt=\"Face Restore\" width=\"70%\" style={{display:'inline-block'}}/>\n</p>\n\n---\n\n## More About This Face Restore Workflow\n\nBased on the original technique by **Xing Jiu**, this workflow demonstrates how tile-based processing and identity-aware modeling can significantly improve **Face Restore** results on difficult image inputs.\n[Original article](https://mp.weixin.qq.com/s/27YAj5eEkZ8Pk53dfHvvhg)\n[Liblib Model Page](https://www.liblib.art/modelinfo/6c7e5ca0f97d4cdd8f652182ee68489a?versionUuid=b871a3e13b1b47d2bdb0de84c3783002)\n\n### Acknowledgements\n\nThis ComfyUI-based **Face Restore** workflow is adapted from the **Flux TTP Tile Upscale** method shared by **Xing Jiu**, and built using community tools like `comfyui-ttp-toolset`, `ky-nodes`, and `easy-use`. The combination of tile patching, FluxGuidance, and LoRA integration enables professional-grade **Face Restore** results even on challenging inputs.\n"
    },
    {
        "id": "1207",
        "readme": "# Wan 2.1 Control LoRA: Efficient Video Generation\n\nWan 2.1 Control LoRA introduces a lightweight and efficient approach to video generation, providing an alternative to traditional ControlNet methods. Designed specifically for Wan 2.1 models, these depth and tile-based LoRAs allow users to guide Wan 2.1 video structure and detail without increasing inference costs. Unlike ControlNet, which can be resource-intensive and complex to train, this Control LoRA seamlessly integrates with Wan 2.1, ensuring precise and optimized outputs with minimal computational overhead.\n\n## How to Use Wan 2.1 Control LoRA Workflow?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1207/readme01.webp\" alt=\"Wan 2.1 Control LoRA\" width=\"750\"/>\n</p>\nNodes in the Wan 2.1 workflow are organized for clarity when using the Depth and Tile LoRAs with Wan 2.1.\n\n1) Wan 2.1 Depth Controlnet Video Group\n2) Upscaling - Tile Controlnet Group\n\nUnmute the group you'd like to use and upload your desired video.\nEnter your positive text and, if necessary, adjust the Wan 2.1 video settings like duration and resolution for both groups.\n\nAll the rendered Wan 2.1 Control LoRA videos will be saved in the Outputs folder in ComfyUI.\n\n### 1 - Depth Control LoRA Video Group\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1207/readme02.webp\" alt=\"Wan 2.1 Control LoRA\" width=\"550\"/>\n</p>\n\nThis Depth Control LoRA group is used to extract depth information from video clips to enhance Wan 2.1 video generation. It offers a more structured and depth-aware alternative to traditional ControlNet methods, helping you create more coherent and visually consistent outputs with Wan 2.1.\n\nUnmute it if it's muted.\n\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1207/readme03.webp\" alt=\"Wan 2.1 Control LoRA\" width=\"450\"/>\n</p>\n\n\nTo use it, upload a video and adjust the resolution and length based on your hardware limits and project needs. Higher resolutions can capture more detailed depth, but may increase processing time and VRAM usage. Shorter clips will run faster. Test different settings to find the right balance between quality and performance for your Wan 2.1 workflow.\n\n\n\n### 2 - Upscaling - Tile Control LoRA Group\n\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1207/readme04.webp\" alt=\"Wan 2.1 Control LoRA\" width=\"550\"/>\n</p>\n\nThis is your Tile LoRA Usage group, designed to upscale videos while preserving detail and consistency. By leveraging the Tile Control LoRA method with Wan 2.1, you can enhance video resolution efficiently without significant performance loss. Ensure that your input video is under 1K resolution, as the workflow supports upscaling up to 1.5K. However, be cautious of potential out-of-memory crashes, particularly when processing longer videos or working with GPUs with limited VRAM. Adjust settings as needed to achieve the best balance between quality and system performance.\n\nUnmute the group if it is muted.\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1207/readme05.webp\" alt=\"Wan 2.1 Control LoRA\" width=\"450\"/>\n</p>\n\n\nUpload your video for upscaling using the Tile Control LoRA, which enhances resolution while maintaining visual consistency and detail in Wan 2.1 outputs. The input video should be under 1K resolution, as the workflow allows for upscaling up to 1.5K. However, be mindful of potential out-of-memory crashes, especially when working with longer videos or limited VRAM. Adjust your settings accordingly to balance quality and performance for the best results with Wan 2.1.\n  \n### Models Download Links \n\nThe Control LoRA models for Wan 2.1 are available on the workflow's boot up machine, but here are the necessary links for offline use if needed:\n\n- [Depth and Tile LoRAs](https://huggingface.co/spacepxl/Wan2.1-control-loras/tree/main)\n- [Wan 2.1 Model](https://huggingface.co/Kijai/WanVideo_comfy/tree/main)\n- [CLIP](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/text_encoders)\n- [VAE](https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P/blob/main/Wan2.1_VAE.pth)\n\nPlease allow 3-5 minutes for the Auto download to complete in your machine's temporary storage.\n\n## Benefits of Wan 2.1 Control LoRA\n\nWith Wan 2.1 Control LoRA, users can enhance video consistency and structure while maintaining an efficient and streamlined workflow. The lightweight nature of these specialized LoRAs makes them ideal for AI-driven animations, cinematic video projects with Wan 2.1, and creative experiments without the drawbacks of traditional control systems. By leveraging Control LoRAs, creators can achieve greater control over their Wan 2.1 video outputs while keeping processing times low and maintaining high-quality results.\n"
    },
    {
        "id": "1208",
        "readme": "# Wan 2.1 Video Restyle: Restyled First Frame for Consistent Stylization  \n\n**Wan 2.1 Restyled First Frame** enables seamless video-to-video stylization by applying the aesthetic of a stylized first frame to an entire video. Instead of relying solely on text prompts or frame-by-frame editing, this **wan 2.1 video restyle** workflow ensures that the look and feel of the first frame are consistently maintained throughout the sequence.  \n\nDesigned specifically for Wan 2.1 models, this method offers an efficient and creative solution for stylizing animations, cinematic clips, or artistic transformations while preserving visual coherence.\n\n## How to Use the Wan 2.1 Video Restyle Workflow?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1208/readme01.webp\" alt=\"Wan 2.1 Restyled First Frame\" width=\"750\"/>\n</p>\n\n### Color-Coded Workflow Overview\n- **Green:** User Inputs – Stylized First Frame, Video, Positive Prompt  \n- **Purple:** Models and LoRA used  \n\n**Steps:**\n1. Upload your original video and its restyled first frame.  \n2. Enter a positive prompt to guide stylization effects.  \n3. Optionally adjust settings like duration or resolution.  \n4. Final output will be saved automatically in your ComfyUI `Outputs` folder.\n\n---\n\n## 1 - Inputs: Video & Stylized First Frame  \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1208/readme02.webp\" alt=\"Wan 2.1 Restyled First Frame\" width=\"550\"/>\n</p>\n\nTo begin your **wan 2.1 video restyle**, you need two primary inputs:\n\n1. **Upload Video:** This is your source content. For optimal results, use videos under 1K resolution.  \n2. **Restyled First Frame:** Capture the first frame and stylize it using tools like Stable Diffusion or ComfyUI. This becomes the reference image for the full video stylization.\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1208/readme03.webp\" alt=\"Wan 2.1 Restyled First Frame\" width=\"550\"/>\n</p>\n\nMake sure to resize your video appropriately using a resolution that’s compatible with Wan 2.1.\n\n---\n\n## 2 - Input: Positive Prompt  \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1208/readme04.webp\" alt=\"Wan 2.1 Restyled First Frame\" width=\"550\"/>\n</p>\n\nThe **positive prompt** drives the motion, detailing, and depth of your video restyle. Use descriptive and artistic language to enhance your final output.\n\n---\n\n## Required Model Downloads  \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1208/readme05.webp\" alt=\"Wan 2.1 Restyled First Frame\" width=\"550\"/>\n</p>\n\nAllow 3–5 minutes for automatic downloads. These are saved temporarily on your machine.\n\n- [Wan 2.1 Fun Models](https://huggingface.co/Kijai/WanVideo_comfy/tree/main)  \n- [CLIP Text Encoder](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/text_encoders)  \n- [Wan 2.1 VAE](https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P/blob/main/Wan2.1_VAE.pth)\n\n---\n\n## Output Preview  \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1208/readme06.webp\" alt=\"Wan 2.1 Restyled First Frame\" width=\"550\"/>\n</p>\n\nOnce rendering is complete, the stylized video and its side-by-side comparison will be saved in the `output` folder in ComfyUI.\n\n---\n\n## Why Use Wan 2.1 Video Restyle?\n\nThe **Wan 2.1 Restyled First Frame** workflow provides:\n\n- Consistent visual style throughout the video  \n- No need for tedious frame-by-frame editing  \n- Full creative control with minimal setup  \n- Fast rendering and efficient performance  \n- Ideal for AI animations, film-grade effects, and stylized storytelling\n\nWhether you're creating narrative films, stylized shorts, or experimental art, this **wan 2.1 video restyle** approach helps you maintain cohesion from frame one to the final cut.\n\n## Acknowledgement\n\nThe **Wan 2.1 Restyled First Frame** workflow was developed by **hakoniwa**, who contributed to advancing video-to-video stylization techniques using the Wan 2.1 model. This implementation builds on the power of AI-generated references to maintain visual coherence across frames. We extend our appreciation to hakoniwa for enabling accessible and high-quality video transformations within the ComfyUI community.\n"
    },
    {
        "id": "1209",
        "readme": "## ComfyUI InfiniteYou Description\n\n### 1. What is the InfiniteYou Workflow?\nThe ComfyUI InfiniteYou workflow integrates ByteDance's advanced identity-preserving model into the environment. Built on the FLUX diffusion transformer and powered by InfuseNet, this technology enables flexible and high-fidelity image generation while preserving identity features. The InfiniteYou toolkit includes two tailored workflows: Face Combine and Zero-Shot Task, each designed for different creative goals within the InfiniteYou system.\n\n### 2. Benefits of InfiniteYou:\n* **Identity Preservation:** InfiniteYou retains facial features even in stylistically transformed prompts.\n* **Aesthetic Quality:** The aes_stage2 mode offers enhanced prompt-image alignment and beauty.\n* **Workflow Variety:** InfiniteYou includes both Face Combine and Zero-Shot Task for different use cases.\n* **Parameter Control:** InfiniteYou allows adjustments to guidance, fusion weights, and control timing for precise generation.\n* **Plug-and-Play Integration:** Seamlessly integrates with standard ComfyUI workflows.\n\n### 3. How to Use the InfiniteYou Workflow\n\n> **IMPORTANT NOTE:** When you first load the InfiniteYou workflow, you will see both Zero-Shot Task and Face Combine sections. These are two separate workflows that operate independently. While technically both can run at the same time, they are designed to be used as separate tools for different purposes, not as connected steps in a single process.\n\n#### 3.1 Generation Methods with InfiniteYou\n\n**Example Setup for InfiniteYou:**\n1. Prepare inputs:\n   In `Load Image` nodes: \n   - upload two reference faces for **Face Combine**\n   - **or** upload one reference face image for **Zero-Shot Task**\n   In `CLIP Text Encode` nodes: \n   - describe desired scene (e.g., \"a boy, 10 years old, handsome in the classroom\")\n   - negative prompt is optional\n2. Click `Queue Prompt` button to run the InfiniteYou workflow\n3. In `Save Image`: get your output\n\n##### Face Combine Workflow (Blending Two Faces)\n* **Best for:** Combining facial features from two identities with InfiniteYou's strong identity control\n* **Characteristics:**\n  - Merges identity from two images\n  - Controlled blending with weights\n  - Precise start and end fusion timing\n\n##### Zero-Shot Task Workflow (Single Image + Prompt)\n\n* **Best for:** Generating portraits from a single identity and a rich text prompt\n* **Characteristics:**\n  - High identity fidelity with sim_stage1\n  - No need for dual-face comparison\n  - Text-guided face recrafting\n\n#### 3.2 Parameter Reference for InfiniteYou\n\n**Face Combine Node:**\nThis node blends facial features from two images.\n- `adapter_file`: Specifies the model file used for identity blending (e.g., aes_stage2_img_proj.bin).\n- `weight`: Controls how strongly the fusion blends both faces.\n- `balance`: Adjusts which image contributes more to the final face.\n- `start_at`: When the fusion starts in the generation timeline.\n- `end_at`: When the fusion ends during generation.\n- `fixed_face_pose`: Locks face pose if true, allows variation if false.\n\n**Apply Node:**\nApplies the InfiniteYou model to a single reference image.\n- `adapter_file`: Defines which stage model is used.\n- `weight`: Intensity of identity preservation.\n- `start_at`: Start of effect application during generation.\n- `end_at`: End of effect application.\n- `fixed_face_pose`: If true, keeps original pose rigid.\n\n**FluxGuidance / BasicGuider:**\nApplies extra influence to preserve identity or control prompt alignment with InfiniteYou.\n- `guidance`: Strength of conditioning—higher = more control, lower = more variety in outputs.\n\n**Samplers:**\nUsed to control how the image is created from noise in InfiniteYou.\n- `sampler_name`: Algorithm to generate the image (e.g., euler).\n- `steps`: Number of iterations to refine the image.\n- `denoise`: How much noise is removed: higher = cleaner image.\n\n#### 3.3. Advanced Optimization with InfiniteYou\n\n**Switching Models:**\n- `aes_stage2`: Better text-image coherence and style (after fine-tuning).\n- `sim_stage1`: More accurate face identity retention (pre-fine-tuning).\n- Always update both `adapter_file` and `control_net` file together when switching InfiniteYou model modes.\n\n**Prompt Tips for InfiniteYou:**\n- Add specific identity cues like \"a woman\", \"an elderly man\", etc. to improve output alignment\n- Be clear and concise with subject and setting for optimal results\n\n## More Information about InfiniteYou\nFor additional details and development references:\n- InfiniteYou original model by [ByteDance](https://github.com/ByteDance/InfiniteYou)\n- Implementation by [ZenAI-Vietnam](https://github.com/ZenAI-Vietnam/ComfyUI_InfiniteYou)\n\n### Acknowledgements\nThis workflow is powered by **InfiniteYou**, developed by **ByteDance Intelligent Creation**. The integration is provided by **ZenAI-Vietnam**, including tailored workflows and model conversions that enable zero-shot and multi-reference identity-preserving generation. Full credit goes to the original authors for their work.\n"
    },
    {
        "id": "1210",
        "readme": "## Wan 2.1 Fun | ControlNet Video Generation\n\n**Wan 2.1 Fun** introduces an intuitive and powerful method for controlled AI content creation using **Wan 2.1 Fun** models. \nBuilt with a **ControlNet-inspired** framework, **Wan 2.1 Fun** ensures compatibility with standard ControlNet preprocessor modules. By extracting **Depth**, **Canny**, or **OpenPose** passes from input footage, this **ControlNet-based** ComfyUI workflow allows users to influence the structure, motion, and style of the output with precision, instead of relying solely on text prompts. \n\n**Wan 2.1 Fun** brings structured visual data into the process—preserving motion accuracy, enhancing stylization, and enabling more deliberate transformations. Whether you're building dynamic animations, pose-driven performances, or experimenting with abstract motion art, Wan 2.1 Fun puts artistic control directly into your hands while leveraging the expressive power of Wan 2.1 Fun models and **ControlNet** principles.\n\n---\n\n### Why Use Wan 2.1 Fun?\n\nThe **Wan 2.1 Fun** workflow offers a flexible way to guide AI motion and structure using **ControlNet-style** visual conditioning:\n\n- Use **Depth**, **Canny**, or **OpenPose** for structured control  \n- Built on a ControlNet-inspired system, enabling guided transformations  \n- Compatible with multiple ControlNet preprocessors including third-party extensions  \n- Achieve clearer spatial consistency, form, and dynamic flow with ControlNet principles \n- No need for complex prompt engineering or training  \n- Lightweight and responsive with high visual fidelity  \n- Excellent for action design, stylized choreography, or performance-driven motion synthesis\n\n---\n\n### How to Use the Wan 2.1 Fun ControlNet Workflow\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1210/readme01.webp\" alt=\"wan 2.1 fun\" width=\"750\"/>\n</p>\n\n#### Wan 2.1 Fun Overview\n- `Load WanFun Model` (*purple*): Model Loader  \n- `Enter Prompts` (*green*): Positive and Negative Prompts  \n- `Upload Your Video and Resize` (*cyan blue*): User Inputs – Reference Footage and Resizing  \n- `Choose Control Video Preprocessor` (*orange*): ControlNet Node for Depth, Canny, or OpenPose  \n- `Wan Fun Sampler + Save Video` (*pink*): Sampling & Output  \n\n#### **Quick Start Steps:**\n1. Select your **Wan 2.1 Fun** model (`Wan2.1-Fun-Control (1.3B / 14B)`) \n2. Input your positive and negative prompts  \n3. Upload reference footage  \n4. Run the workflow via `Queue Prompt`  \n5. Retrieve results from the last node or the `Outputs` folder\n\n---\n\n### 1 - Load WanFun Model\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1210/readme06.webp\" alt=\"wan 2.1 fun\" width=\"550\"/>\n</p>\n\nChoose the appropriate `Wan2.1-Fun-Control` variant for **ControlNet-style** conditioning:\n- Use `model_cpu_offload` for smoother runs with **1.3B**  \n- Use `sequential_cpu_offload` for lower GPU load on **14B**  \n\n\n---\n\n### 2 - Enter Prompts\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1210/readme03.webp\" alt=\"wan 2.1 fun\" width=\"550\"/>\n</p>\n\n- **Positive Prompt:**\n  - Describes motion style, atmosphere, or visual texture  \n  - Vivid and descriptive prompts enhance creative control  \n- **Negative Prompt:**\n  - To improve output stability, try words like `\"Blurring, mutation, deformation, distortion, dark and solid, comics.\"`  \n  - For more dynamic motion, add terms like `\"quiet, solid\"`\n\n---\n\n### 3 - Upload Your Video and Resize\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1210/readme02.webp\" alt=\"wan 2.1 fun\" width=\"550\"/>\n</p>\n\nUpload your source footage. For optimal results with **Wan 2.1 Fun**, resize appropriately to meet desired frame dimensions.\n\n---\n\n### 4 - Choose Control Video Preprocessor\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1210/readme04.webp\" alt=\"wan 2.1 fun\" width=\"550\"/>\n</p>\n\nThis section activates the ControlNet-based preprocessing system for Wan 2.1 Fun:\n- `Depth`: Captures spatial layout as a depth map  \n- `Canny`: Extracts strong edge contours and structure  \n- `OpenPose`: Identifies joints, limbs, and posture for motion-driven work  \n\nThese guides condition the Wan 2.1 Fun workflow to follow visual cues instead of relying solely on prompts.  \nAll modules align with ControlNet preprocessing standards, allowing you to extend functionality via custom nodes.\n\n---\n\n### 5 - Wan Fun Sampler + Save Output  \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1210/readme05.webp\" alt=\"wan 2.1 fun\" width=\"550\"/>\n</p>\n\nThe **Wan 2.1 Fun Sampler** is tuned for clarity and creative consistency.  \nCustomize the configuration if needed. Outputs are automatically saved to the designated folder.\n\n---\n\n## Acknowledgement\n\nThe **Wan 2.1 Fun** workflow was developed by [bubbliiiing](https://github.com/bubbliiiing) and [hkunzhe](https://github.com/hkunzhe), whose contributions to AI-based motion control and stylization made advanced ControlNet integration possible. This project applies the expressive depth of the **Wan 2.1 Fun** models alongside Depth, Canny, and OpenPose inputs to enable compositional and dynamic precision in AI-assisted visual production.  \nWe thank them for making these tools available to the creative community.\n"
    },
    {
        "id": "1211",
        "readme": "## Wan 2.1 Fun | Trajectory Motion Control\n\n**Wan 2.1 Fun Trajectory Control** is a creative photo-to-video workflow that transforms a single image into a dynamic motion sequence using a custom trajectory path. Built on the Wan 2.1 Fun model ecosystem, this workflow enables artists to define the exact motion path—whether it's a camera pan, zoom, orbit, or cinematic fly-through—and generate smooth, AI-driven animations that follow it precisely. \n\nInstead of relying on random motion prompts, users can sketch or input clear paths to animate static images with purpose using Wan 2.1 Fun control features. This unlocks powerful storytelling possibilities, from dreamy pans across landscapes to character-focused movement and visual rhythm in music videos or short films. Wan 2.1 Fun Trajectory Control empowers creators to blend detailed visual aesthetics with meaningful motion, all with minimal input and high-quality results.\n\n### Why Use Wan 2.1 Fun Trajectory Control?\n\nThe **Wan 2.1 Fun Trajectory Control** workflow brings structured motion to your AI video generation by animating a single photo along a defined path:\n\n- Input an image and a custom **trajectory path** to guide motion\n- Generate smooth, cinematic camera movements or creative animation effects  \n- Maintain visual fidelity while adding dynamic motion  \n- Avoid tedious keyframing or complex 3D setups  \n- Perfect for character reveals, scenic pans, zooms, and expressive photo animations  \n\nWith Wan 2.1 Trajectory Control, your still images can come to life in a way that's both artistically rich and technically efficient.\n\n\n### How to Use the Wan 2.1 Trajectory Control?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1211/readme01.webp\" alt=\"wan 2.1 Fun\" width=\"750\"/>\n</p>\n\n#### Wan 2.1 Fun Trajectory Control Overview\n- `Load WanFun Model` : Model Loader  \n- `Enter Prompts` : Positive and Negative Prompts  \n- `Upload Your Image and Set Motion Path` : User Inputs – Image and Motion Path  \n- `Wan Fun Sampler + Save Video` : Frames Duration and resolution can be found in Wan Fun Sampler Node  \n\n#### **Quick Start Steps:**\n1. Select your `Wan 2.1 Fun` model in the Load EasyAnimate Group \n2. Enter positive and negative prompts to guide generation  \n3. Upload your input image\n4. Input your motion path for trajectory control\n5. Run the workflow by clicking `Queue Prompt` button\n6. Check the video node for the final output (also saved to the `Outputs` folder)\n\n---\n\n### 1 - Load WanFun Model\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1211/readme06.webp\" alt=\"wan 2.1 Fun\" width=\"550\"/>\n</p>\n\nChoose the right **model variant** for your task:\n- `Wan2.1-Fun-Control (1.3B / 14B)`: For guided video generation with Depth, Canny, OpenPose, and trajectory control  \n- `Wan2.1-Fun-InP (1.3B / 14B)`: For text-to-video with start and end frame prediction  \n\n**Memory Tips:**\n- use `model_cpu_offload` for faster generation with **1.3B**  \n- use `sequential_cpu_offload` to reduce GPU memory usage with **14B**\n\n\n### 2 - Enter Prompts\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1211/readme03.webp\" alt=\"wan 2.1 Fun\" width=\"550\"/>\n</p>\n\n- **Positive Prompt:**\n  - drive the motion, detailing, and depth of your Wan 2.1 Fun video restyle\n  - using descriptive and artistic language can enhance your final output\n \n    \n- **Negative Prompt:**\n  - using longer negative prompts such as *\"Blurring, mutation, deformation, distortion, dark and solid, comics.\"* can increase stability in Wan 2.1 Fun generation\n  - adding words such as *\"quiet, solid\"* can increase dynamism in your Wan 2.1 Fun control\n\n\n### 3 - Upload Your Image\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1211/readme02.webp\" alt=\"wan 2.1 Fun\" width=\"550\"/>\n</p>\n\nUpload your source image to begin the generation. Make sure a resolution that's compatible with **Wan 2.1 Fun**. \n\nYou can cahnge resolution in Wan Fun Sampler node.\n\n\n### 4 - Enter Motion to Control Video\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1211/readme04.webp\" alt=\"wan 2.1 Fun\" width=\"550\"/>\n</p>\n\nHere you set the trajectory path to control the motion of the subject and background for the uploaded photo. \n  \n<p><strong>Node Guide for Wan 2.1 Fun Trajectory Control:</strong></p>\n<ul>\n  <li>Shift + click to add control point at end. Ctrl + click to add control point (subdivide) between two points.</li>\n  <li>Right click on a point to delete it. Note that you can't delete from start/end.</li>\n  <li>Right click on canvas for context menu.</li>\n</ul>\n\n<p><strong>Toggle handles visibility:</strong></p>\n<ul>\n  <li><code>points_to_sample</code> value sets the number of samples returned from the drawn spline itself, this is independent from the actual control points, so the <strong>interpolation</strong> type matters.</li>\n</ul>\n\n<p><strong>Sampling_method:</strong></p>\n<ul>\n  <li><code>time</code> samples along the time axis, used for schedules</li>\n  <li><code>path</code> samples along the path itself, useful for coordinates</li>\n</ul>\n\n  \n### 5 - Wan Fun Sampler + Save Video  \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1211/readme05.webp\" alt=\"wan 2.1 Fun\" width=\"550\"/>\n</p>\n\n\n`Wan 2.1 Fun Sampler` is optimized with the best settings. You can experiment with different configurations if desired.\n\nVideo Resolution and Duration in frames settings can be found here.\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1211/readme07.webp\" alt=\"wan 2.1 Fun\" width=\"550\"/>\n</p>\n\nOnce rendering is complete, the Wan 2.1 Fun stylized video will be saved.\n\n---\n## Acknowledgement\n\nThe **Wan 2.1 Fun Trajectory Control** workflow was developed by [bubbliiiing](https://github.com/bubbliiiing) and [hkunzhe](https://github.com/hkunzhe), who have pioneered accessible tools for motion-guided AI video generation. By combining trajectory path input with the power of Wan 2.1 Fun models, this workflow enables users to animate still images with precision and artistic control. The Wan 2.1 Fun Trajectory Control workflow advances photo-to-video generation using motion trajectory guidance. We acknowledge and thank them for their valuable contributions to the creative AI community.\n"
    },
    {
        "id": "1212",
        "readme": "## Wan 2.1 Fun | Image-to-Video and Text-to-Video AI Generation\n\n**Wan 2.1 Fun Image-to-Video and Text-to-Video** offers a highly versatile AI video generation workflow that brings both static visuals and pure imagination to life. Powered by the Wan 2.1 Fun model family, this workflow lets users animate a single image into a full video or generate entire motion sequences directly from text prompts—no initial footage required.\n\nWhether you're crafting surreal dreamscapes from a few words or turning a concept art piece into a living moment, this Wan 2.1 Fun setup makes it easy to produce coherent, stylized video outputs. With support for smooth transitions, flexible duration settings, and multilingual prompts, Wan 2.1 Fun is perfect for storytellers, digital artists, and creators looking to push visual boundaries with minimal overhead.\n\n### Why Use Wan 2.1 Fun Image-to-Video + Text-to-Video?\n\nThe **Wan 2.1 Fun Image-to-Video and Text-to-Video** workflow provides an easy and expressive way to generate high-quality video from either an image or a simple text prompt:\n\n- Convert a single **image into motion** with automatic transitions and effects  \n- Generate **videos directly from text prompts**, with smart frame prediction  \n- Includes **InP (start/end frame prediction)** for controlled visual narratives  \n- Works with **1.3B and 14B** model variants for scalable quality and speed  \n- Great for creative ideation, storytelling, animated scenes, and cinematic sequences  \n\nWhether you're visualizing a scene from scratch or animating a still image, this Wan 2.1 Fun workflow offers fast, accessible, and visually impressive results using Wan 2.1 Fun models.\n\n\n### How to Use the Wan 2.1 Fun Image-to-Video + Text-to-Video?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1212/readme01.webp\" alt=\"wan 2.1 Fun\" width=\"750\"/>\n</p>\n\n#### Wan 2.1 Fun Image-to-Video + Text-to-Video Overview\n\n- `Load WanFun Model` : Load the appropriate Wan 2.1 Fun model variant (1.3B or 14B)  \n- `Enter Prompts or Upload Image` : Supports both text prompts and image inputs using their separate group \n- `Set Inference Settings` : Adjust frames, duration, resolution, and motion options  \n- `Wan Fun Sampler` : Uses WanFun for start/end prediction and temporal coherence  \n- `Save Video` : Output video is rendered and saved automatically after sampling\n\n\n#### **Quick Start Steps:**\n1. Select your `Wan 2.1 Fun` model in the Load Model Group  \n2. Enter positive and negative prompts to guide generation  \n3. Choose your input mode:\n   - Upload an image for **Image-to-Video** Group \n   - Or rely on text prompts alone for **Text-to-Video** Group \n4. Adjust settings in the `Wan Fun Sampler` node (frames, resolution, motion options)  \n5. Run the workflow by clicking the `Queue Prompt` button  \n6. View and download your final Wan 2.1 Fun video from the Video Save node (`Outputs` folder)\n\n\n---\n\n### 1 - Load WanFun Model\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1212/readme04.webp\" alt=\"wan 2.1 Fun\" width=\"550\"/>\n</p>\n\nChoose the right **model variant** for your task:\n- `Wan2.1-Fun-Control (1.3B / 14B)`: For guided video generation with Depth, Canny, OpenPose, and trajectory control  \n- `Wan2.1-Fun-InP (1.3B / 14B)`: For text-to-video with start and end frame prediction  \n\n**Memory Tips for Wan 2.1 Fun:**\n* use `model_cpu_offload` for faster generation with **1.3B** Wan 2.1 Fun  \n* use `sequential_cpu_offload` to reduce GPU memory usage with **14B** Wan 2.1 Fun\n\n\n### 2 - Enter Prompts\n\nIn the appropriate you choose, Image-2-Video group or Text-2-Video Group, enter your positive and negative promopt.\n\n- **Positive Prompt:**\n  - drive the motion, detailing, and depth of your video restyle\n  - using descriptive and artistic language can enhance your final Wan 2.1 Fun output\n \n    \n- **Negative Prompt:**\n  - using longer negative prompts such as *\"Blurring, mutation, deformation, distortion, dark and solid, comics.\"* can increase stability Wan 2.1 Fun\n  - adding words such as *\"quiet, solid\"* can increase dynamism in Wan 2.1 Fun videos\n\n\n### 3 - Image 2 Video Group with Wan 2.1 Fun\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1212/readme02.webp\" alt=\"wan 2.1 Fun\" width=\"550\"/>\n</p>\n\nUpload your start image to initiate Wan 2.1 Fun generation. You can adjust the resolution and duration in the Wan Fun Sampler node.\n\n[Optional] Unmute the end image node; this image will serve as the final image, with the in-between rendered through the Wan 2.1 sampler.\n\nYour final Wan 2.1 Fun video is located in the `Outputs` folder of the Video Save node.\n\n### 4 - Text 2 Video Group with Wan 2.1 Fun\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1212/readme03.webp\" alt=\"wan 2.1 Fun\" width=\"550\"/>\n</p>\n\nEnter your prompts to initiate generation. You can adjust the resolution and duration in the Wan Fun Sampler node.\n\nYour final Wan 2.1 Fun video is located in the `Outputs` folder of the Video Save node.\n\n\n\n---\n## Acknowledgement\n\nThe **Wan 2.1 Fun Image-to-Video and Text-to-Video** workflow was developed by [bubbliiiing](https://github.com/bubbliiiing) and [hkunzhe](https://github.com/hkunzhe), whose work on the Wan 2.1 Fun model family has made prompt-based video generation more accessible and flexible. Their contributions enable users to turn both still images and pure text into dynamic, stylized videos with minimal setup and maximum creative freedom using Wan 2.1 Fun. We deeply appreciate their innovation and ongoing impact in the AI video generation space.\n"
    },
    {
        "id": "1213",
        "readme": "## SkyReels-A2: Advanced Element-to-Video Technology\n\n### 1. What is the ComfyUI SkyReels-A2 Workflow?\nSkyReels-A2 is an advanced Element-to-Video (E2V) workflow that combines multiple reference images into fluid, dynamically generated videos. Developed by Skywork AI, this SkyReels-A2 technology enables you to integrate characters, objects, and backgrounds while maintaining the identity and visual fidelity of each element. Unlike traditional image-to-video approaches, SkyReels-A2 specifically addresses the challenge of preserving reference fidelity for each element while creating natural interactions between them, making it ideal for virtual product demonstrations, character animations, and complex scene compositions with SkyReels-A2 ComfyUI integration.\n\n### 2. Benefits of ComfyUI SkyReels-A2:\n* **Multi-Element Integration with SkyReels-A2:** Combine three distinct reference elements (person, object/another person, and scene) into a cohesive video.\n* **Identity Preservation:** SkyReels-A2 maintains high fidelity to reference images while creating natural motion and interactions.\n* **Flexible Composition:** Control relationships between elements through text prompts that describe their interactions with the SkyReels-A2 system.\n* **Resolution Control:** This SkyReels-A2 workflow enables you to adjust input image resolutions for different scene types.\n* **Background Removal:** Automatically isolates subjects from backgrounds for better composition.\n* **Precise Padding Control:** Fine-tune element placement and prominence with mathematical padding calculations in SkyReels-A2.\n* **Seamless Integration:** Compatible with WAN2.1 video generation technology for high-quality SkyReels-A2 results.\n\n### 3. How to Use the ComfyUI SkyReels-A2 Workflow\n\n#### 3.1 SkyReels-A2 Image Preparation Process\n\n**Element Images Preparation for SkyReels-A2:**\n\nThe **SkyReels-A2 workflow** has 3 main image groups:\n\n##### **Subject1Image in SkyReels-A2:** The main character/person\n  - Load your image in the \"Subject1Image\" group\n  - Use the Mask Bounding Box node to get optimal framing\n  - Apply Simple Math for padding calculation (a-40)\n  - Resize to 640×480 or 832×480 (depending on scene type)\n  - Use ImageCompositeMasked to place the subject on a clean background\n    <img src=\"https://cdn.runcomfy.net/workflow_assets/1213/readme01.webp\" alt=\"SkyReels-A2\" width=\"600\"/>\n\n##### **Subject2Image in SkyReels-A2:** The object or secondary character/person\n  - Load your image in the \"Subject2Image\" group\n  - Use the Mask Bounding Box node to get optimal framing\n  - Apply Simple Math for padding calculation (a-40)\n  - Place on a clean background using ImageCompositeMasked\n  - Resize to match the resolution of your subject image\n    <img src=\"https://cdn.runcomfy.net/workflow_assets/1213/readme02.webp\" alt=\"SkyReels-A2\" width=\"600\"/>\n  > **Important:** For smaller objects like pens, brushes, or handheld items, adjust the width parameter in the Resize Image node to a lower value (10-20) while keeping keep_proportion set to true. This controls the relative size of your object in the final composition and prevents objects from appearing unnaturally large when held by characters.\n\n##### **BackgroundImage for SkyReels-A2:** The environment\n  - Load your image in the \"BackgroundImage\" group\n  - Resize to match your chosen resolution (640×480 or 832×480)\n  - Apply center crop to maintain aspect ratio\n  - Use a higher divisible_by value (16) to ensure compatibility with SkyReels-A2\n    <img src=\"https://cdn.runcomfy.net/workflow_assets/1213/readme03.webp\" alt=\"SkyReels-A2\" width=\"600\"/>\n\n\n#### 3.2 SkyReels-A2 Element Composition Process\n\n**SkyReels-A2 Batch Assembly:**\n1. Set the ImageBatchRepeatInterleaving to ```4``` repeats\n2. Ensure all images maintain the same resolution and aspect ratio for optimal SkyReels-A2 processing\n\n#### 3.3 SkyReels-A2 Video Generation Settings\n\n**SkyReels-A2 Text Encoding:**\n1. Configure your prompt to describe the relationship between elements\n2. Ensure all three elements are mentioned\n\n#### 3.4 SkyReels-A2 Resolution Guidelines\n\n**For Close-up Portraits in SkyReels-A2:**\n- Use ```640×480``` resolution\n- Adjust padding to ```20px``` on all sides (a-40 formula)\n\n**For Full-body Shots with SkyReels-A2:**\n- Use ```832×480``` resolution\n- Adjust padding to maintain proper framing\n\n**For Multiple Subjects in SkyReels-A2:**\n- Experiment with different padding values to control element prominence\n\n### 4. Technical Explanation of SkyReels-A2 E2V Technology\n\nThe **SkyReels-A2 workflow** uses a two-branch encoding approach:\n1. **General Feature Encoding in SkyReels-A2:** CLIP Vision Encoder extracts token-like representations similar to text prompts\n2. **Spatial Feature Encoding with SkyReels-A2:** 3D VAE Encoder captures detailed spatial characteristics of each element\n\nThese encodings are combined with text prompts that describe the desired relationship between elements.\n\n### 5. SkyReels-A2 Performance Optimization\n\n**SkyReels-A2 VRAM Management:**\n- The note in the SkyReels-A2 workflow warns that batch sizes of ```10``` can easily cause out-of-memory errors\n- For testing, use 33 frames; increase for final SkyReels-A2 production\n\n**SkyReels-A2 Optimization Nodes:**\n- Use_jit: true (for both background removal nodes in SkyReels-A2)\n- WanVideo TECache: enabled with latent_scale_factor: ```0.25```\n- WanVideo VRAM Management: offload_percent: 1.00\n- Enable the use_non_blocking option for better memory efficiency with SkyReels-A2\n\n### Acknowledgements\nThis workflow is based on the **SkyReels-A2 model** developed by the Skywork AI research team. The original model and research is available at [SkyworkAI/SkyReels-A2](https://github.com/SkyworkAI/SkyReels-A2). The ComfyUI implementation of **SkyReels-A2** is provided by [Yuan-ManX](https://github.com/Yuan-ManX/ComfyUI-SkyReels-A2). All credit goes to the original authors for their innovative work in video diffusion transformers and **SkyReels-A2** element-to-video technology.\n"
    },
    {
        "id": "1216",
        "readme": "## HiDream-I1 | Text-to-Image AI Generation\n\n**HiDream-I1** is a cutting-edge open-source text-to-image foundation model with **17 billion parameters**, designed for ultra-fast, high-quality image generation. This ComfyUI-based workflow enables users to convert natural language prompts into stunning visuals—ideal for concept art, storytelling, ideation, or professional-grade image synthesis with **HiDream-I1**.\n\nWhether you're illustrating a sci-fi world, sketching product ideas, or imagining surreal environments, **HiDream-I1** delivers remarkable fidelity, speed, and ease of use.\n\n---\n\n### Why Use HiDream-I1 Text-to-Image?\n\nThe **HiDream-I1** workflow provides an intuitive and expressive platform to generate high-quality images from text alone:\n\n- Turn **text prompts into images** in seconds with minimal setup using **HiDream-I1**  \n- Backed by a **17B parameter model** for detailed, coherent visual generation with **HiDream-I1**  \n- Supports **high resolution output** and customizable sampling options in the **HiDream-I1** workflow  \n- Great for **concept design**, digital art, visual storytelling, and ideation with **HiDream-I1**  \n- Runs seamlessly inside ComfyUI with easy-to-modify **HiDream-I1** workflow components  \n\nFrom single-sentence prompts to layered descriptive passages, **HiDream-I1** captures your intent and renders it with clarity and style.\n\n---\n\n### How to Use the HiDream-I1 Text-to-Image Workflow\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1216/readme01.webp\" alt=\"HiDream-I1\" width=\"750\"/>\n</p>\n\n\n#### HiDream-I1 Text-to-Image Overview\n\n- `Load HiDream-I1 Model` : Load the official **HiDream-I1** 17B model  \n- `Enter Prompt Text` : Input your text prompt for **HiDream-I1** (positive and optional negative prompts)  \n- `Set Image Parameters` : Adjust resolution, sampling method, and steps for your **HiDream-I1** generation  \n- `Generate Image` : The workflow will render your result using **HiDream-I1**'s fast inference  \n- `Save Image` : Output image is automatically saved and viewable in the save node  \n\n---\n\n#### **Quick Start Steps with HiDream-I1:**\n1. Load the **HiDream-I1** model using the provided model loader node  \n2. Enter a **positive prompt** describing what you want to see with **HiDream-I1**  \n3. Optionally add a **negative prompt** to filter out unwanted features in your **HiDream-I1** generation  \n4. Set the desired **image resolution** and sampling configuration for optimal **HiDream-I1** results  \n5. Click the `Queue Prompt` button in ComfyUI to generate with **HiDream-I1**  \n6. View and download your **HiDream-I1** generated image from the Save Image node  \n\n\n#### Example Prompts for HiDream-I1\n\n- \"A futuristic cityscape at sunset, neon lights, flying cars, high detail, cinematic composition\"\n- \"A serene forest in autumn, golden leaves, misty atmosphere, photorealistic\"\n- \"A wizard casting a spell in a stormy mountain landscape, dramatic lighting, fantasy art style\"\n- \"Cyberpunk samurai walking through a rainy alley, glowing signage, cinematic angle\"\n- \"Elegant product mockup of a smartwatch on a marble pedestal, studio lighting, minimal background\"\n\n\n\n---\n\n\n\n\n### 1 - Load HiDream-I1 Model\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1216/readme02.webp\" alt=\"HiDream-I1\" width=\"550\"/>\n</p>\n\nChoose the right **HiDream-I1** model variant for your task:\n\n- **HiDream-I1**-Full: Full version model for more detailed inference and longer sequences\n- **HiDream-I1**-Dev: Development version, optimized for slightly faster performance with fewer steps\n- **HiDream-I1**-Fast: Optimized for quick results with the fastest generation speed\n\n\n\n### 2 - Enter Prompts for HiDream-I1\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1216/readme03.webp\" alt=\"HiDream-I1\" width=\"550\"/>\n</p>\n\nIn the **Text-to-Image** group, enter your **positive** and **negative** prompts to guide image generation with **HiDream-I1**.\n\n- **Positive Prompt for HiDream-I1:**\n  - Describe the **scene**, **subject**, **style**, and **mood** you want **HiDream-I1** to generate.\n  - Use **detailed, creative, and descriptive language** to leverage the full capability of the **HiDream-I1** 17B parameter model.\n  - Example for **HiDream-I1**:\n    ```\n    A futuristic city skyline at dusk, glowing neon lights, cinematic lighting, highly detailed, ultra realistic, photo style\n    ```\n- **Negative Prompt for HiDream-I1:**\n  - Specify what you want to **exclude** from the **HiDream-I1** generated image.\n  - Helps improve clarity and avoid artifacts or unwanted features in your **HiDream-I1** creation.\n  - Example:\n    ```\n    blurry, extra limbs, grainy, distorted faces, watermark, text, low quality, poorly drawn\n    ```\n\n> 💡 Tip: The more specific and vivid your positive prompt, the better the **HiDream-I1** output. Use negative prompts to avoid common generative flaws in **HiDream-I1** images.\n\n\n### 3 - HiDream-I1 Sampler Settings\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1216/readme04.webp\" alt=\"HiDream-I1\" width=\"550\"/>\n</p>\n\n- **HiDream-I1 Full**\n  - `hidream_i1_full_fp16.safetensors`\n  - `shift`: 3.0  \n  - `steps`: 50  \n  - `sampler`: uni_pc  \n  - `scheduler`: simple  \n  - `cfg`: 5.0  \n- **HiDream-I1 Dev**\n  - `hidream_i1_dev_bf16.safetensors`\n  - `shift`: 6.0  \n  - `steps`: 28  \n  - `sampler`: lcm  \n  - `scheduler`: normal  \n  - `cfg`: 1.0 *(no negative prompt)*  \n- **HiDream-I1 Fast**\n  - `hidream_i1_fast_bf16.safetensors`\n  - `shift`: 3.0  \n  - `steps`: 16  \n  - `sampler`: lcm  \n  - `scheduler`: normal  \n  - `cfg`: 1.0 *(no negative prompt)*  \n\n\n---\n\n\n\n## Acknowledgement\n\nThe **HiDream-I1 Text-to-Image** workflow is made possible through the powerful and modular architecture of **ComfyUI**. We extend our deep appreciation to the ComfyUI development team and open-source contributors for building a flexible, node-based framework that allows seamless integration of advanced generative models like **HiDream-I1**.\n\nTheir dedication to accessible and extensible tooling empowers creators and researchers to explore state-of-the-art image generation with **HiDream-I1** with minimal effort and maximum control.\n"
    },
    {
        "id": "1217",
        "readme": "## ComfyUI FramePack Wrapper Description\n\n### 1. What is the ComfyUI FramePack Wrapper?\nThe ComfyUI FramePack Wrapper integrates Stanford University's advanced FramePack technology into the ComfyUI environment. FramePack is built on innovative frame context packing techniques that enable efficient and high-quality long video generation with minimal computing resources. The FramePack technology allows you to generate thousands of frames at full 30fps using even modest GPU configurations like 6GB laptop GPUs, making professional video creation accessible to everyone. FramePack's unique approach to video generation feels like image diffusion but delivers exceptional video results.\n\n### 2. Benefits of ComfyUI FramePack Wrapper:\n* **Efficient Resource Usage:** FramePack generates videos on smaller cloud machines with minimal GPU memory requirements\n* **O(1) Computation Complexity:** FramePack maintains constant processing speed regardless of video length\n* **Anti-drifting Technology:** FramePack solves common issues with video quality degradation over time\n* **Consistent Quality for Long Videos:** FramePack maintains high visual quality even for videos of 30-60+ seconds, without the degradation typical in other video generation models\n* **No Timestep Distillation:** FramePack preserves quality while maintaining efficiency\n  \n### 3. How to Use the ComfyUI FramePack Wrapper\n\n#### 3.1 Generation Methods with ComfyUI FramePack\n\n**Example Setup for FramePack:**\n1. Prepare inputs:\n   - In `Load Image` node: Upload a reference image that will be animated into a video with FramePack\n2. Configure key parameters:\n   - In `FramePackSampler`: Set video length (e.g., 5.0 seconds)\n   - In `Video Combine`: Set frame rate (e.g., 30) and output format (e.g., video/h264-mp4)\n3. Click `run` button to run the FramePack workflow\n4. The generated FramePack video will be saved according to the settings in your `Video Combine` node\n\n#### 3.2 Parameter Reference for ComfyUI FramePack\n\n**FramePackSampler Node:**\n- `steps`: Number of diffusion steps (e.g., 30) - higher values produce better quality but take longer in FramePack\n- `guidance_scale`: Additional guidance strength in FramePack (e.g., 10.00) - higher values adhere more strictly to prompts\n- `seed`: Random seed for reproducible results in FramePack (e.g., 47)\n- `total_second_length`: Duration of the generated video in seconds (e.g., 5.0) in FramePack\n- `control_after_generate`: Controls the anti-drifting sampling method in FramePack (keep as \"fixed\" for best results)\n\n**Video Combine Node:**\n- `frame_rate`: Frames per second in the output FramePack video (e.g., 30)\n- `crf`: Compression quality factor (e.g., 19) - lower values = higher quality FramePack videos\n\n#### 3.3. Advanced Optimization with ComfyUI FramePack\n\n**Generating Longer Videos with FramePack:**\nOne of FramePack's key strengths is the ability to generate extended videos efficiently. To create longer videos:\n- Increase the `total_second_length` parameter in the FramePackSampler node (e.g., from 5.0 to 10.0, 30.0, or even 60.0+)\n- Keep `control_after_generate` set to \"fixed\" to utilize FramePack's advanced anti-drifting sampling technology\n- When generating videos longer than 10 seconds, consider slightly increasing the `gpu_memory_preservation` value to manage memory usage\n- For very long videos (30+ seconds), you may need to reduce the `latent_window_size` parameter\n- FramePack's O(1) computation complexity ensures consistent generation speed regardless of video length\n- **Write temporal prompts:** For longer videos, include words like \"gradually,\" \"slowly,\" \"then\" to guide temporal flow and describe how elements change over time\n\n**Adjusting Video Resolution:**\nTo change the output FramePack video resolution, adjust the \"Resize Image\" node in the workflow:\n- Modify the `width` and `height` parameters to set your desired FramePack video dimensions\n- It's recommended to keep dimensions as multiples of 8 for optimal FramePack results\n- Note: Higher resolutions will require more GPU memory and processing time with FramePack\n- For larger resolutions, you may need to adjust the `gpu_memory_preservation` parameter to balance memory usage in FramePack\n\n**FramePack Prompt Tips:**\n- Be highly descriptive about motion and transitions in your prompts for better FramePack results\n- Include specific action verbs that describe the desired movement in FramePack\n- For best results, describe both the scene and how elements should move/change in FramePack videos\n\n## More Information about FramePack\nThe FramePack technology uses innovative bi-directional sampling methods that break traditional causality constraints to prevent quality degradation in longer videos. Particularly for image-to-video generation, FramePack employs \"inverted anti-drifting sampling\" which treats the first frame as an approximation target throughout the generation process, ensuring high-quality results even in videos up to 60 seconds or more (1800+ frames).\n\nFor additional details and FramePack model downloads:\n- FramePack original research by [Lvmin Zhang and Maneesh Agrawala (Stanford University)](https://lllyasviel.github.io/frame_pack_gitpage/)\n- ComfyUI FramePack Wrapper implementation by [kijai](https://github.com/kijai/ComfyUI-FramePackWrapper)\n\n### Acknowledgements\nThis workflow is powered by **FramePack**, developed by **Lvmin Zhang and Maneesh Agrawala at Stanford University**. The **ComfyUI FramePack Wrapper** is provided by **kijai**, including tailored workflows and integrations that enable efficient long video generation with FramePack technology. Full credit goes to the original authors for their groundbreaking work on FramePack.\n"
    },
    {
        "id": "1218",
        "readme": "## ComfyUI UNO Description\n\n### 1. What is ComfyUI UNO?\nThe ComfyUI UNO workflow integrates ByteDance's Universal aNd cOntrollable (UNO) model into the ComfyUI environment. Developed by ByteDance's Intelligent Creation Team, UNO represents a significant advancement in subject-driven image generation technology based on the innovative \"Less-to-More\" generalization paradigm. ComfyUI UNO workflow enables flexible and high-fidelity image generation while preserving identity features of reference subjects with remarkable consistency across various contexts and scenarios.\n\n### 2. Benefits of ComfyUI UNO:\n* **Superior Subject Consistency:** ComfyUI UNO maintains exceptional subject similarity across different context.\n* **Subject-Object Control:** Unlike many alternatives, ComfyUI UNO excels at handling both subject references and object references simultaneously without identity confusion.\n* **Universal Compatibility:** ComfyUI UNO works with diverse types of subjects - people, objects, toys, logos, and more.\n* **Resource Efficiency:** ComfyUI UNO delivers professional-quality results even with limited VRAM resources (works well with 24GB).\n* **Flexible Implementation:** ComfyUI UNO includes workflows for both single-subject and subject-object generation.\n\n### 3. How to Use ComfyUI UNO\n\n#### 3.1 Workflow Setup\n\nComfyUI UNO workflow has two main configurations:\n\n**Top Group - Subject-Object Generation:**\n* This configuration allows you to input a reference subject image and a reference object image that will be combined in a generated scene according to your text prompt.\n\n**Bottom Group - Single-Subject Generation:**\n* This configuration uses a single reference image and generates new images of that subject in different contexts.\n\n#### 3.2 Generation Process with ComfyUI UNO\n\n**Basic Setup:**\n1. **Load Reference Images:**\n   * For subject-object workflow (top group): Upload one subject reference image and one object reference image\n   * For single-subject workflow (bottom group): Upload one reference image\n2. **Set Text Prompt for ComfyUI UNO:**\n   * Write a descriptive prompt for your desired scene, including how the reference subject/object should appear\n3. **Adjust ComfyUI UNO Parameters:**\n   * `Width/Height`: Determine output resolution (recommended 768×1024 or 1024×1024)\n   * `Guidance`: Controls how closely ComfyUI UNO generation follows your text prompt (3.5-4.0 recommended)\n   * `Num_steps`: Higher values (20+) produce more consistent results with ComfyUI UNO, especially for human faces\n   * `Seed`: Set a specific seed for reproducibility or randomize for variety in ComfyUI UNO outputs\n4. **Generate Image with ComfyUI UNO:**\n   * Click \"Queue Prompt\" to run the ComfyUI UNO workflow\n   * Review the result in the \"Save Image\" node\n\n#### 3.3 Parameter Reference for ComfyUI UNO\n\n**UNO Loadmodel Node:**\n* `model_type`: Determines which ComfyUI UNO model variant to use\n  * `flux-dev`: Standard ComfyUI UNO model with good balance of quality and performance\n  * `flux-schnell`: Faster ComfyUI UNO model variant with slightly lower quality\n\n#### 3.4 Tips for Best Results with ComfyUI UNO\n\n**Subject Selection for ComfyUI UNO:**\n* Choose clear, well-lit reference images where the subject is prominent\n* For faces/people, front-facing images with neutral expressions work best with ComfyUI UNO\n* For objects, clear images with minimal background distraction are ideal for ComfyUI UNO processing\n\n**Prompt Engineering for ComfyUI UNO:**\n* Be specific about subject descriptions that match your reference images\n* Describe the desired scene, lighting, and composition for ComfyUI UNO to generate\n* Include reference to the specific subjects (e.g., \"a young man\", \"a vintage bicycle\")\n* Keep prompts concise but detailed for optimal ComfyUI UNO results\n\n### 4. Technical Information about ComfyUI UNO\n\nThe ComfyUI UNO technology is built upon FLUX models and leverages the DiT (Diffusion Transformer) architecture with several key enhancements:\n\n* **Universal Rotary Position Embedding (UnoPE):** A specialized mechanism in ComfyUI UNO that helps the model differentiate between reference subjects and objects, significantly reducing attribute confusion.\n* **Progressive Cross-Modal Alignment:** Training methodology that enables ComfyUI UNO to handle increasingly complex multi-condition controls.\n* **Universal Customization:** ComfyUI UNO is capable of handling diverse subject types without specialized training.\n\n### 5. Acknowledgements\n\nThis ComfyUI workflow is powered by **UNO**, developed by the **Intelligent Creation Team at ByteDance** (Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He). The ComfyUI integration of ByteDance's UNO is provided by community contributors, adapting the groundbreaking research into an accessible workflow.\n\nFor additional information about UNO, visit the [ByteDance UNO GitHub repository](https://github.com/bytedance/UNO).\n"
    },
    {
        "id": "1219",
        "readme": "## Wan 2.1 FLF2V | First-Last Frame Video Generation\n\n**Wan 2.1 FLF2V** (First-Last Frame Video Generation) delivers a seamless way to create smooth, cinematic videos by simply providing two images—a starting frame and an ending frame. Powered by the **Wan 2.1 FLF2V** model from the Alibaba Tongyi Wanxiang team, this **Wan 2.1 FLF2V** workflow automatically generates coherent intermediate frames, outputting high-quality 720p video sequences with natural motion.\n\nDesigned for storytellers, animators, and digital artists, **Wan 2.1 FLF2V** enables effortless video creation that maintains logical visual flow, ideal for visual narratives, transitions, and imaginative scene building with the power of **Wan 2.1 FLF2V** technology.\n\n### Why Use Wan 2.1 FLF2V Generation?\n\nThe **Wan 2.1 FLF2V** workflow offers a powerful, intuitive way to generate videos from just two images:\n\n- Generate a **smooth video sequence** between two frames automatically with **Wan 2.1 FLF2V**  \n- **Wan 2.1 FLF2V** outputs **high-definition 720p** video with consistent logical flow  \n- **Wan 2.1 FLF2V** is built on an **open-source Apache 2.0 license** for free and flexible use  \n- Minimal setup—just **upload start and end images** to the **Wan 2.1 FLF2V** system and go  \n- Great for visual storytelling, creative transitions, scene visualization, and experimental video art using **Wan 2.1 FLF2V**  \n\nWhether you're crafting animated short scenes or connecting key moments with style, the **Wan 2.1 FLF2V** workflow brings fluid video generation within easy reach.\n\n\n### How to Use the Wan 2.1 FLF2V Workflow?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1219/readme01.webp\" alt=\"Wan 2.1 FLF2V\" width=\"750\"/>\n</p>\n\n#### Wan 2.1 FLF2V First-to-Last Frame Video Overview\n\n- `Load Diffusion Model` : Load **wan2.1_flf2v_720p_14B_fp16.safetensors** for **Wan 2.1 FLF2V**  \n- `Load CLIP` : Load **umt5_xxl_fp8_e4m3fn_scaled.safetensors** for **Wan 2.1 FLF2V**  \n- `Load VAE` : Load **wan_2.1_vae.safetensors** for **Wan 2.1 FLF2V**  \n- `Load CLIP Vision` : Load **clip_vision_h.safetensors** for **Wan 2.1 FLF2V**  \n- `Upload Start and End Images` : Add your starting frame and ending frame images to the **Wan 2.1 FLF2V** workflow  \n- `Set Prompts` : Optionally adjust positive and negative prompts for **Wan 2.1 FLF2V** (supports English and Chinese)  \n- `Adjust Video Size` : In the `WanFirstLastFrameToVideo` node of **Wan 2.1 FLF2V**, set the resolution (default **720×1280**, adjust lower like **480×854** for smoother performance if needed)  \n- `Generate Video` : Click Run or press **Ctrl (Cmd) + Enter** to start creating your first-to-last frame video with **Wan 2.1 FLF2V**  \n\n\n#### **Quick Start Steps:**\n\nFollow these steps to generate high-quality first-to-last frame videos using **Wan 2.1 FLF2V**:\n\n1. Ensure the `Load Diffusion Model` node is set to **wan2.1_flf2v_720p_14B_fp16.safetensors** for **Wan 2.1 FLF2V**  \n2. Ensure the `Load CLIP` node is set to **umt5_xxl_fp8_e4m3fn_scaled.safetensors** for **Wan 2.1 FLF2V**  \n3. Ensure the `Load VAE` node is set to **wan_2.1_vae.safetensors** for **Wan 2.1 FLF2V**  \n4. Ensure the `Load CLIP Vision` node is set to **clip_vision_h.safetensors** for **Wan 2.1 FLF2V**  \n5. Upload your starting image into the `Start_image` node of the **Wan 2.1 FLF2V** workflow  \n6. Upload your ending image into the `End_image` node of the **Wan 2.1 FLF2V** workflow  \n7. *(Optional)* Edit the **positive** and **negative** prompts for **Wan 2.1 FLF2V** as needed (supports both English and Chinese)  \n8. *(Important)* In the `WanFirstLastFrameToVideo` node of **Wan 2.1 FLF2V**:  \n   - Adjust the video size settings in **Wan 2.1 FLF2V**.  \n   - Default resolution is **720×1280** for best quality with **Wan 2.1 FLF2V**.  \n   - For lower memory devices, you can temporarily reduce resolution (e.g., **480×854**) for smoother performance with **Wan 2.1 FLF2V**, and switch back to 720×1280 for final high-quality outputs.  \n9. Click the **Run** button or press **Ctrl (or Cmd) + Enter** to start the **Wan 2.1 FLF2V** video generation  \n10. View and download your final **Wan 2.1 FLF2V** video from the Video Save node (`Outputs` folder)\n\n<p align=\"center\">\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1219/readme02.webp\" alt=\"Wan 2.1 FLF2V\" width=\"550\"/>\n</p>\n\n\n---\n\n### 1 - Load Wan 2.1 FLF2V Model\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1219/readme03.webp\" alt=\"Wan 2.1 FLF2V\" width=\"550\"/>\n</p>\n\nSelect the appropriate **model and assets** for **Wan 2.1 FLF2V** first-to-last frame video generation:\n- `wan2.1_flf2v_720p_14B_fp16.safetensors`: Main diffusion model for **Wan 2.1 FLF2V**  \n- `umt5_xxl_fp8_e4m3fn_scaled.safetensors`: CLIP text encoder for prompt understanding in **Wan 2.1 FLF2V**  \n- `wan_2.1_vae.safetensors`: VAE decoder for high-quality frame outputs with **Wan 2.1 FLF2V**  \n- `clip_vision_h.safetensors`: Vision encoder for start/end frame perception in **Wan 2.1 FLF2V**\n\n\n### 2 - Enter Prompts\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1219/readme04.webp\" alt=\"wan 2.1 FLF2V\" width=\"550\"/>\n</p>\n\nOnce your Start and End frames are uploaded to **Wan 2.1 FLF2V**, refine the motion and transition style using prompts:\n- **Positive Prompt** for **Wan 2.1 FLF2V**:\n  - Guides the motion dynamics, visual style, and scene consistency across frames in **Wan 2.1 FLF2V**\n  - Descriptive and vivid language (e.g., \"sunlit meadow, cinematic lighting, vibrant atmosphere\") can produce richer, more coherent animations with **Wan 2.1 FLF2V**\n- **Negative Prompt** for **Wan 2.1 FLF2V**:\n  - Helps avoid unwanted visual artifacts in **Wan 2.1 FLF2V** like *\"blurring, deformation, distortion, dark patches\"*\n  - Including terms like *\"smooth, clear, detailed\"* can further stabilize and enhance the flow of the generated video with **Wan 2.1 FLF2V**\n\n### 3 - First-to-Last Frame with Wan 2.1 FLF2V\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1219/readme05.webp\" alt=\"wan 2.1 FLF2V\" width=\"550\"/>\n</p>\n\nUpload your starting frame to the `Start_image` node and your ending frame to the `End_image` node to set the beginning and end of the motion sequence for **Wan 2.1 FLF2V**.\n\nYou can adjust the resolution and frame settings inside the `WanFirstLastFrameToVideo` node of **Wan 2.1 FLF2V**:\n\n- Default size for **Wan 2.1 FLF2V**: **720×1280** for best quality\n- Lower size (e.g., **480×854**) if you experience memory or speed issues with **Wan 2.1 FLF2V**\n\nThe **Wan 2.1 FLF2V** model will automatically generate a smooth, coherent transition between the two frames.\n\nYour final **Wan 2.1 FLF2V** video will be saved in the `Outputs` folder linked to the Video Save node.\n\n\n---\n## Acknowledgement\n\nThe **Wan 2.1 FLF2V** First-to-Last Frame Video workflow is based on the open-source model developed by the Alibaba Tongyi Wanxiang team. Their work on **Wan 2.1 FLF2V** has made first-frame to last-frame video generation accessible for creators and researchers under the permissive **Apache 2.0** license.\n\nWe also acknowledge the **ComfyUI development team**, whose modular node system makes it easy to integrate and deploy advanced video generation models like **Wan 2.1 FLF2V**. Their continuous contributions enable faster, more flexible, and creative AI workflows for everyone using **Wan 2.1 FLF2V** and similar technologies.\n"
    },
    {
        "id": "1220",
        "readme": "> 🌟🌟🌟 [Update: Check out the latest version of the Wan2.1-VACE 14B workflow.](https://www.runcomfy.com/comfyui-workflows/vace-14b-all-in-one-video-creation-editing-workflow-comfyui) Now featuring the powerful 14B model and five versatile editing capabilities! Make sure to use this updated version for even better results! 🌟🌟🌟\n\n## VACE Wan2.1 | Video to Video\n\n**VACE Wan2.1** enables creative video transformation by applying the style of a single reference image across an entire video. Built on the VACE Wan2.1 model and integrated into ComfyUI, this workflow allows users to generate visually consistent, stylized videos where every frame adopts the texture, color, and mood of a given image.\n\nIdeal for animation restyling, artistic video creation, and experimental aesthetics, VACE Wan2.1 simplifies the style transfer process while maintaining temporal coherence and visual fluency throughout the output. The VACE Wan2.1 technology offers an innovative approach to video styling that artists and creators will find invaluable for their projects.\n\n### Why Use VACE Wan2.1 for Image-to-Video Style Transfer?\n\nThe VACE Wan2.1 workflow offers a flexible, efficient method for transforming videos through reference-driven style:\n\n- Apply a single image's style to an entire video sequence with VACE Wan2.1\n- Maintain temporal consistency across stylized video frames using VACE Wan2.1 technology\n- Output high-quality 720p video guided by a visual reference through VACE Wan2.1 processing\n- Just upload one image and one video—minimal setup required for VACE Wan2.1\n- Great for artistic stylization, concept animation, and visual storytelling workflows powered by VACE Wan2.1\n\nWhether you're turning live footage into painterly scenes or reimagining video clips with a unique aesthetic, VACE Wan2.1 unlocks powerful visual transformation using just one image and one video. The VACE Wan2.1 system excels at preserving the essence of your reference style throughout video frames.\n\n### How to Use the VACE Wan2.1 Workflow?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1220/readme01.webp\" alt=\"VACE Wan2.1\" width=\"750\"/>\n</p>\n\n#### **Quick Start Steps for VACE Wan2.1:**\n\nFollow these steps to generate high-quality first-to-last frame videos using VACE Wan2.1:\n \n1. Upload your Reference Image into the `Load Image` node for VACE Wan2.1 processing\n2. Upload your Source Video into the `Upload Video` node for VACE Wan2.1 transformation\n3. Edit the **positive** and **negative** prompts as needed for VACE Wan2.1 styling\n4. Change Video's duration or resolution as needed for your VACE Wan2.1 project\n5. Click the **Run** button or press **Ctrl (or Cmd) + Enter** to start the VACE Wan2.1 video generation\n6. View and download your final VACE Wan2.1 video from the Video Save node (`Outputs` folder)\n\n\n\n---\n\n### 1 - Load VACE Wan2.1 Model\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1220/readme02.webp\" alt=\"VACE Wan2.1\" width=\"550\"/>\n</p>\n\nSelect the appropriate **Vace Compatible model and assets** for VACE Wan2.1 generation:\n- `Wan2_1_VACE_1_3B_preview_bf16.safetensors`: VACE diffusion model loader for VACE Wan2.1\n- `umt5_xxl_fp8_e4m3fn_scaled.safetensors`: CLIP text encoder for prompt understanding in VACE Wan2.1\n- `wan_2.1_vae.safetensors`: VAE decoder for high-quality VACE Wan2.1 frame outputs\n\n\n### 2 - Enter Inputs for VACE Wan2.1\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1220/readme03.webp\" alt=\"VACE Wan2.1\" width=\"550\"/>\n</p>\n\n### Prompts for VACE Wan2.1\n- **Positive Prompt:**\n  - Guides the motion dynamics, visual style, and scene consistency across VACE Wan2.1 frames\n  - Descriptive and vivid language (e.g., \"sunlit meadow, cinematic lighting, vibrant atmosphere\") can produce richer, more coherent VACE Wan2.1 animations\n- **Negative Prompt:**\n  - Helps avoid unwanted visual artifacts in VACE Wan2.1 outputs like *\"blurring, deformation, distortion, dark patches\"*\n  - Including terms like *\"smooth, clear, detailed\"* can further stabilize and enhance the flow of the generated VACE Wan2.1 video\n\n\n### Reference Image for VACE Wan2.1\n\n- Input your reference image which will guide the style of the VACE Wan2.1 video generation.\n- The Orientation corresponding to the Source video will likely give better results in VACE Wan2.1 processing.\n\n### Source Video for VACE Wan2.1\n\n- Upload the Source video, which will be used as the Depth Control Video for the VACE Wan2.1 model.\n- You can change video's duration and FPS from the VHS load video node directly for customized VACE Wan2.1 outputs.\n\n\n### 3 - VACE Wan2.1 Sampler\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1220/readme04.webp\" alt=\"VACE Wan2.1\" width=\"550\"/>\n</p>\n\nThe Settings are already set to best parameters for VACE Wan2.1, but feel free to change and test with other sampler settings to customize your VACE Wan2.1 experience.\n\n### 4 - VACE Wan2.1 Outputs \n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1220/readme05.webp\" alt=\"VACE Wan2.1\" width=\"550\"/>\n</p>\n\nThe Rendered VACE Wan2.1 videos will be stored in Comfyui > Output Folder. Each VACE Wan2.1 project creates a unique output that combines your reference image style with the source video's motion.\n\n---\n## Acknowledgement\n\nThe **VACE Wan2.1 V2V** from Image to Video workflow is based on the open-source VACE model and node set developed by the community. The VACE Wan2.1 system represents an important advancement in AI-powered video stylization technology.\n\nSpecial thanks to Datou and T8star-Aix for designing the VACE workflow, and to Kijai for creating the VACE node system that powers the advanced capabilities of VACE Wan2.1. Their contributions have made image-guided video stylization easy, flexible, and highly creative within ComfyUI.\n\nWe also appreciate the ongoing support from the ComfyUI developer community, whose modular node architecture continues to empower fast, powerful, and user-friendly AI video workflows like VACE Wan2.1.\n"
    },
    {
        "id": "1221",
        "readme": "### 1. What is Step1X-Edit?\nStep1X-Edit is an advanced image editing model developed by StepFun AI that aims to provide comparable performance to closed-source models like GPT-4o and Gemini2 Flash. The Step1X-Edit framework combines the semantic reasoning capabilities of Multimedia Large Language Models (MLLM) with a Diffusion in Transformer (DiT) architecture to deliver high-quality instruction-based image editing.\n\nStep1X-Edit excels at understanding natural language instructions and applying precise edits while maintaining image fidelity. The Step1X-Edit model was trained on over 1 million high-quality instruction-image pairs covering 11 distinct editing categories, making it extraordinarily versatile for various editing tasks.\n\n### 2. Benefits of ComfyUI Step1X-Edit:\n\n* **Great Instruction Understanding:** Step1X-Edit leverages MLLM technology to comprehend complex editing requests with nuanced understanding of both text and visual content.\n* **Comprehensive Editing Capabilities:** Step1X-Edit handles 11 different editing categories including subject addition/removal, background changes, color alterations, material modifications, motion changes, and more.\n* **High Fidelity Results:** Step1X-Edit maintains a good balance between reference image reconstruction and editing prompt following, preserving image quality.\n* **Simplified Workflow:** No need for masks during the editing process, offering a streamlined Step1X-Edit user experience.\n* **Open Source Alternative:** Step1X-Edit provides comparable results to proprietary models while being fully open-source.\n\n### 3. Quick Start Guide\n\n#### 3.1 System Requirements\n\nStep1X-Edit is a resource-intensive model that performs best with:\n- **VRAM:** Recommended 80GB for optimal performance at 1024×1024 resolution\n- **Note:** RunComfy's cloud GPU service provides all the necessary computational power for Step1X-Edit without any installation required. Simply select a machine with sufficient VRAM from the options available.\n\n#### 3.2 Workflow Options\n\nStep1X-Edit offers two primary workflow configurations:\n\n##### Regular Workflow (Non-Real Person Version)\n* **Best for:** General purpose editing of objects, scenes, and non-human subjects with Step1X-Edit\n* **Characteristics:**\n  - Simple 3-step process: Load Image → Edit with Step1X-Edit → Save Result\n  - Excellent performance for text modification, subject addition/removal, style transfers, background changes, etc.\n  - Direct editing without additional face processing\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1221/readme01.webp\" alt=\"Step1X-Edit\" width=\"750\"/>\n\n##### Real Person Workflow (Extended Version)\n* **Best for:** Editing images containing human faces where facial identity preservation is crucial\n* **Characteristics:**\n  - Combines Step1X-Edit with additional face consistency preservation\n  - Uses Face Bounding Box and simple person description to enhance identity preservation\n  - Preserves identity features better than the standard Step1X-Edit workflow\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1221/readme02.webp\" alt=\"Step1X-Edit\" width=\"750\"/>\n\n#### 3.3 Parameter Reference\n\n**Main Step1X-Edit Node Parameters:**\n- `cfg`: Guidance scale, typically around 6.0 (higher = more adherence to prompt)\n- `size_level`: Controls output resolution (512, 768, or 1024)\n- `num_steps`: Number of diffusion steps (typically 20-31)\n- `mllm_model`: The vision language model (default: Qwen2.5-VL-7B-Instruct)\n\n**For Real Person Workflow Additional Parameters:**\n- **Face Bounding Box Node** (from FaceAnalysis):\n  - `Index`: Face detection control\n    - `-1`: Detect all faces (default)\n    - `0`: Select largest face only\n    - `1`: Select second-largest face\n    - Check workflow carefully when dealing with multiple faces\n  - `padding`: Additional space around face (default: 0)\n  - `padding_percent`: Percentage-based padding (default: 0.30)\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1221/readme05.webp\" alt=\"Step1X-Edit\" width=\"400\"/>\n\n\n#### 3.4 Editing Task Categories\n\nStep1X-Edit has been specifically optimized for these 11 editing categories:\n\n1. **Subject Addition**: Add new objects or people to a scene using Step1X-Edit\n2. **Subject Removal**: Remove unwanted elements from an image with Step1X-Edit\n3. **Subject Replacement**: Swap one object for another using Step1X-Edit\n4. **Background Change**: Modify or replace the background while preserving foreground elements\n5. **Color Alteration**: Change specific colors within the image with Step1X-Edit\n6. **Material Modification**: Transform the material properties of objects (e.g., glass to metal)\n7. **Motion Change**: Alter the position or pose of subjects using Step1X-Edit\n8. **Portrait Beautification**: Enhance or modify portraits with natural improvements\n9. **Style Transfer**: Apply artistic styles to images with Step1X-Edit\n10. **Text Modification**: Edit or replace text within images using Step1X-Edit\n11. **Tone Transformation**: Adjust overall image tone, lighting, or atmosphere\n\n#### 3.5 Step-by-Step Usage Guide\n\n##### Regular Workflow (Non-Real Person Version)\n1. **Upload Your Image** using the Load Image node\n2. **Enter Your Editing Instructions** in the Step1X-Edit Node\n3. **Adjust Parameters** if needed:\n   - `cfg`: 6.0 is a good default for Step1X-Edit\n   - `size_level`: 512 for testing, 1024 for final results\n   - `num_steps`: 20-31 (more steps = better quality but slower)\n4. **Click Run** to process your edit with Step1X-Edit\n\n##### Real Person Workflow (Face Editing)\n1. **Upload Your Image** using the Load Image node\n2. **Enter a Simple Person Description** in the CR Prompt Text node\n   - Just use basic terms like \"young woman\" or \"man\"\n   - This helps the Step1X-Edit model understand who's in the image\n3. **Enter Your Editing Instructions** in the Step1X-Edit Node\n   - Be specific about what you want to change about the person\n4. **Adjust Parameters** if needed:\n   - Same as regular workflow, plus face detection settings if needed\n5. **Click Run** to process your edit with Step1X-Edit\n6. **View and Download** the result\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1221/readme03.webp\" alt=\"Step1X-Edit\" width=\"400\"/>\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1221/readme04.webp\" alt=\"Step1X-Edit\" width=\"400\"/>\n\n#### 3.6 Tips for Best Results\n\n- **Clear Instructions**: Be specific and concise in your Step1X-Edit prompts\n- **Size Considerations**: Larger sizes (1024) produce better quality but take longer to process\n- **Face Handling**: Use the Real Person workflow when editing human faces with Step1X-Edit\n- **Multiple Edits**: For complex edits, consider breaking them down into separate steps\n- **Workflow Selection**: Choose the appropriate Step1X-Edit workflow based on your subject matter\n- **Machine Selection**: Opt for 2X Large (80GB VRAM) or 2XL Plus (80GB VRAM) for optimal Step1X-Edit performance\n\n\n### 4. Acknowledgements\n\nThis implementation is based on the Step1X-Edit model developed by the StepFun AI team ([original repository](https://github.com/stepfun-ai/Step1X-Edit/)). The ComfyUI integration of Step1X-Edit was created by [Quank123WIP](https://github.com/quank123wip/ComfyUI-Step1X-Edit), making this powerful technology accessible within the ComfyUI environment.\n\nRunComfy has integrated the Step1X-Edit technology into an easy-to-use cloud workflow, making advanced AI image editing accessible without the need for local installation or high-end hardware.\n\nSincere thanks to the original authors and the ComfyUI integration developer for making this tool available to the community.\n"
    },
    {
        "id": "1222",
        "readme": "### 1. What is the ComfyUI ICEdit Workflow?\nThe ComfyUI ICEdit-Nunchaku workflow integrates the innovative In-Context Edit (ICEdit) framework with Nunchaku optimization for ultra-fast image editing. Developed by researchers from Zhejiang University and Harvard University, ICEdit enables high-quality instructional image editing with remarkable efficiency. Built on the FLUX diffusion transformer and leveraging ICEdit's in-context learning principles, this workflow preserves identity features while allowing flexible modifications based on natural language instructions. The ICEdit workflow represents an optimal solution for AI image editing, delivering precise results with minimal GPU resources.\n\n### 2. Benefits of ComfyUI ICEdit:\n* **Exceptional Efficiency:** ICEdit achieves good-quality editing using just 0.5% of training data and 1% of parameters required by previous methods.\n* **Ultra-Fast Performance:** The ICEdit-Nunchaku integration enables high-speed editing even on systems with only 4GB VRAM.\n* **Identity Preservation:** ICEdit maintains subject identity even through significant style or attribute modifications.\n* **Natural Language Instructions:** Edit images using simple text prompts with ICEdit's intuitive instruction system.\n* **Multi-turn Editing:** ICEdit supports sequential editing operations on the same image for complex transformations.\n* **High-Resolution Output:** The ICEdit workflow includes three different upscaling methods for high-quality results.\n\n\n### 3. Using ICEdit for Image Editing\nGetting Started:\n1. Image Input:\n   - Upload your source image using the main **Load Image** node located in the lower left corner of the ICEdit workflow\n   - This is the primary input for the image you want to edit with ICEdit\n   - The ICEdit workflow will automatically process your image to the required dimensions (512px width)\n   - For best ICEdit results, use square images when possible, as non-square images may be center-cropped during processing\n   - **Pro Tip**: For higher ICEdit success rates, right-click on the Load Image node and select \"Open in MaskEditor\" to manually create a mask around the area you want to edit\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1222/readme01.webp\" alt=\"ICEdit\" width=\"650\"/>\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1222/readme03.webp\" alt=\"ICEdit\" width=\"550\"/>\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1222/readme04.webp\" alt=\"ICEdit\" width=\"550\"/>\n2. Using the Mask Editor with ICEdit (Optional but Recommended):\n   - In the MaskEditor, use the brush tools to draw a mask around the specific area you want to modify with ICEdit\n   - This helps direct ICEdit's attention to the relevant parts of the image\n   - Use different brush settings (thickness, opacity, hardness) for precise control\n   - Click \"Save\" when finished to apply your custom mask to the ICEdit workflow\n   - This step significantly improves ICEdit's success rates for detailed or specific changes\n3. Entering Your ICEdit Instructions:\n   - Find the **String** input node with the red highlight in the center of the ICEdit workflow\n   - Type your desired ICEdit instruction in this field\n   - For style transfers, simply type instructions like \"Convert the image into Ghibli style\" or \"Convert to anime illustration and maintain shirt be pink, hair be brown\"\n   - Leave the other String node with the fixed prefix unchanged - this contains the required context that makes ICEdit work properly\n   - The workflow will automatically combine your instruction with the required prefix for optimal ICEdit processing\n   <img src=\"https://cdn.runcomfy.net/workflow_assets/1222/readme02.webp\" alt=\"ICEdit\" width=\"650\"/>\n4. Running the ICEdit Workflow:\n   - After uploading your image and entering your edit instruction, click `run` to start ICEdit processing\n   - The ICEdit workflow will generate the edited image through several processing stages\n   \n\n### 4. Parameter Reference for ICEdit\n**ICEdit Parameter Settings:**\n1. Nunchaku FLUX.1 LoRA Loader for ICEdit:\n   - `lora_strength`: 1.00-1.18 (can be adjusted for ICEdit effect intensity, with 1.18 providing best overall results)\n2. ICEdit KSamplerAdvanced Settings:\n   - First KSampler: Default 30 steps, euler sampler, CFG 7\n     - Adjust CFG: Higher values (8-15) follow instructions more strictly, lower values (3-5) increase creativity\n     - Adjust steps: More steps improve detail but increase processing time\n   - Second KSampler: Default 25 steps for upscaling with beta scheduler\n   - Third KSampler: Default 30 steps, kl_optimal sampler for preserving facial details and texture\n     - These parameters can be adjusted based on your specific editing needs\n3. ICEdit Upscaling Configuration:\n   - The ICEdit workflow includes three upscaling methods:\n     - Model-based upscaling using specialized models like 2xNomosUni\n     - ImageScaleBy for detail enhancement with lanczos scaling\n     - KL-optimal sampling for high-fidelity ICEdit restoration (helps preserve facial features)\n   - If using 4x magnification models, consider changing the scaling factor to 0.5 to prevent artifacts in large ICEdit results\n4. FluxGuidance in ICEdit:\n   - The ICEdit workflow has multiple FluxGuidance nodes in different sections, controlling guidance strength\n   - Style Conversion FluxGuidance: Controls how closely the ICEdit style transfer follows your instructions\n   - Image Editing FluxGuidance: Right side shows a value of 4.0, balancing quality and ICEdit precision\n   - Higher values make ICEdit follow instructions more precisely but may affect image quality\n   - Lower values provide more creative freedom but may result in less precise ICEdit results\n5. Redux Integration with ICEdit (Optional):\n   - The ICEdit workflow includes Redux style fine-tuning capabilities, but it's optional\n   - When using Redux for ICEdit style transfer, you can change to get_condition1, though results may vary\n\n\n### 5. Example ICEdit Editing Tasks\n\nICEdit excels at a variety of editing operations, including:\n\n* **Style Transfers with ICEdit:** \"Convert the image into anime illustration\"\n* **Color Changes using ICEdit:** \"Change the blue shirt to green\"\n* **Clothing Modifications with ICEdit:** \"Change the blue shirt to denim jacket\"\n* **Accessory Addition through ICEdit:** \"Add sunglasses\", \"This girl wears a white watch\"\n* **Accessory Removal with ICEdit:** \"Remove picture on the wall\"\n* **Background Alterations using ICEdit:** \"Girl is on the beach, colorful cloud in the sky\"\n* **Object Manipulation via ICEdit:** \"Hand holding a durian\"\n* **Text/Watermark Operations with ICEdit:** \"Add a blue watermark 'from me' on the wall\", \"Remove the text 'from aha'\"\n* **Multi-element Editing through ICEdit:** \"Convert to Ghibli style\"\n\n### 6. Advanced Tips for Better ICEdit Results\n\n**ICEdit Prompt Engineering:**\n- Be specific and concise in your ICEdit instructions\n- For ICEdit style transfers, include details you want preserved\n- Use clear descriptive language for best ICEdit results\n- When changing backgrounds with ICEdit, be specific about placement\n\n**ICEdit Optimization Tips:**\n- If you encounter unsatisfactory ICEdit results, try changing the seed value (this is critical for success)\n- For complex ICEdit edits, consider breaking them into multiple simpler edit steps\n- Adjust lora_strength to control the intensity of ICEdit effects\n\n**Handling ICEdit Failures:**\n- ICEdit works best with realistic photographs; results may vary with non-photorealistic inputs\n- Object removal tasks with ICEdit have lower success rates than additions or modifications\n- If an ICEdit attempt fails, try rephrasing the instruction or using a different seed\n\n### 7. Credits and Acknowledgements\n\nThis workflow is powered by **ICEdit**, developed by **Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang** from ReLER, CCAI, Zhejiang University and Harvard University. The ComfyUI integration of ICEdit is made possible through community contributions, with special thanks to **judian17** for the [Nunchaku-compatible version of ICEdit](https://github.com/River-Zhang/ICEdit/issues/1#issuecomment-2846568411) and optimization work.\n\nThe workflow uses a specially adapted version of ICEdit-MoE-LoRA where expert layers were weight-averaged for compatibility, enabling high-quality ICEdit processing with minimal computational resources.\n\nFor additional information about ICEdit, please visit:\n- Project page: [ICEdit GitHub](https://github.com/River-Zhang/ICEdit)\n"
    },
    {
        "id": "1223",
        "readme": "## LBM Relighting: Advanced Image Illumination System\n\nLBM Relighting enables realistic and flexible relighting of subject images using lighting information from a separate reference image. The LBM Relighting workflow integrated into ComfyUI lets users combine two images—one for the subject and one for the lighting—to produce a relit composition that reflects the tone, direction, and mood of the lighting input. The LBM Relighting technique preserves the natural characteristics of both images while transforming the lighting conditions.\n\nIdeal for lighting experimentation, photorealistic compositing, and digital look development, LBM Relighting simplifies the process of applying complex lighting setups without the need for 3D rendering or manual adjustments. LBM Relighting provides an efficient and visually accurate way to reimagine subjects under different lighting environments, making it a powerful tool for artists, designers, and visual storytellers who want to experiment with LBM Relighting technology.\n\n---\n\n### Why Use LBM Relighting?\n\nThe LBM Relighting workflow provides:\n\n- Two-input relighting: apply the lighting of one image to the subject in another using LBM Relighting\n- Realistic light synthesis: LBM Relighting preserves shadow, highlight, and directional light cues  \n- Intuitive ComfyUI integration: LBM Relighting offers modular, node-based, and customizable workflows\n- Minimal setup: just load your subject and lighting images into the LBM Relighting system\n- Useful for lighting studies, CG compositing, VFX look-dev, and postproduction workflows with LBM Relighting\n\nLBM Relighting makes it easy to produce new lighting versions of a subject without re-rendering or reshooting, all within the AI compositing environment of ComfyUI. With LBM Relighting, you can achieve professional results in significantly less time.\n\n---\n\n### How to Use LBM Relighting\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1223/readme01.webp\" alt=\"LBM Relighting\" width=\"750\"/>\n</p>\n\n#### Quick Start Steps for LBM Relighting:\n\n1. Upload your **Subject Image** in the `Subject Image` node (this is the image you want to relight with LBM Relighting).  \n2. Upload your **Lighting Image** in the second `Background Image` node (this image provides the lighting conditions for LBM Relighting).  \n3. Adjust any available prompt or strength settings if needed for optimal LBM Relighting results.  \n4. Press **Run** or use **Ctrl/Cmd + Enter** to generate your LBM Relighting output.  \n5. The LBM Relighting result will be saved in the `Outputs` folder through the Image Save node.  \n\n---\n\n\n### 1 - Load LBM Relighting Model\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1223/readme02.webp\" alt=\"LBM Relighting\" width=\"550\"/>\n</p>\n\nSelect the appropriate **LBM Relighting Compatible model and assets** for LBM Relighting generation:\n- `model.safetensors`: Selected LBM Relighting diffusion model loader. You can rename it for your convenience.\n- It should be placed in `ComfyUI/models/diffusion_models` to enable LBM Relighting functionality\n\nDownload Link for LBM Relighting model - https://huggingface.co/jasperai/LBM_relighting/blob/main/model.safetensors\n\n\n\n### 2 - Enter Inputs for LBM Relighting\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1223/readme03.webp\" alt=\"LBM Relighting\" width=\"550\"/>\n</p>\n\n\n#### Subject Image for LBM Relighting\n- Upload the Subject Image, which contains the object or person you want to relight with LBM Relighting.\n- This image will serve as the primary content that receives new lighting based on the reference in the LBM Relighting process.\n\n#### Lighting Reference (Background) Image for LBM Relighting\n- Upload a Lighting Reference Image that represents the desired light conditions for your LBM Relighting output.\n- The lighting pattern, direction, and intensity in this image will be used by LBM Relighting to re-illuminate the subject image.\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1223/readme04.webp\" alt=\"LBM Relighting\" width=\"550\"/>\n</p>\n\nAdjust the subject's mask's expansion or contraction settings to prevent color bleeding when applying LBM Relighting.\n\nOnce both images are uploaded, the LBM Relighting model will process them to generate a relit result, combining the subject with the lighting style of the reference. There are no prompts or advanced configuration required—just drop in the images and run the LBM Relighting workflow.\n\n\n### 4 - Outputs for LBM Relighting\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1223/readme05.webp\" alt=\"LBM Relighting\" width=\"550\"/>\n</p>\n\nThe rendered LBM Relighting results will be saved in the ComfyUI > Output folder. Each run of the LBM Relighting workflow produces a relit image that combines the subject's original appearance with the lighting characteristics of the reference image.\n\nThe LBM Relighting output provides a visually coherent result with the subject naturally illuminated by the new lighting conditions—ideal for artistic rendering, photo adjustments, or creative visual effects that require professional LBM Relighting.\n\n---\n\n## Acknowledgement\n\nThe LBM Relighting workflow and model wrapper were developed by Kijai, offering a straightforward yet powerful solution for image-based relighting within ComfyUI. By using two inputs—a subject image and a lighting reference—the LBM Relighting workflow generates realistic relit outputs that reflect the lighting conditions of the reference. LBM Relighting provides an accessible way for artists and creators to experiment with lighting variations without needing complex setups or 3D environments.\n\nThis LBM Relighting workflow demonstrates how focused tools within ComfyUI can solve specific visual problems with minimal effort. Thanks to Kijai's integration of the LBM Relighting model, users can easily explore relighting as part of larger pipelines or as a standalone enhancement step. We also acknowledge the broader ComfyUI ecosystem, which continues to support modular experimentation and creative flexibility in AI-assisted workflows like LBM Relighting.\n"
    },
    {
        "id": "1224",
        "readme": "### 1. What is the ComfyUI ACE-Step Workflow?\nComfyUI ACE-Step integrates the newly developed ACE-Step music generation foundation model into the ComfyUI environment. Built on a hybrid architecture combining diffusion-based generation with Sana's Deep Compression AutoEncoder (DCAE) and a lightweight linear transformer, ACE-Step enables ultra-fast, high-quality music generation with exceptional control capabilities. This workflow allows users to create original music across diverse genres and styles with simple natural language prompts and lyrics.\n\n### 2. Benefits of ComfyUI ACE-Step:\n* **Unprecedented Speed:** Synthesizes up to 4 minutes of music in just 20 seconds—15× faster than LLM-based alternatives\n* **Musical Coherence:** ACE-Step maintains superior quality across melody, harmony, and rhythm dimensions\n* **Multilingual Support:** Generates music in 19 different languages with exceptional performance in the top 10 languages\n* **Advanced Control:** Enables voice cloning, lyric editing, remixing, and track generation with fine-grained parameters\n* **Creative Flexibility:** Supports diverse music styles, genres, and instruments with various description formats\n* **Seamless Integration:** Plugs directly into ComfyUI workflows for AI-powered audio creation\n\n### 3. How to Use the ComfyUI ACE-Step Workflow\n\n#### 3.1 Generation Methods with ComfyUI ACE-Step\n\n**Example Setup for ACE-Step:**\n1. Prepare inputs:\n   In `TextEncodeAceStepAudio` node: \n   - Add descriptive tags for music style (e.g., \"country rock, folk rock, southern rock, bluegrass, pop\")\n   - Input lyrics with structure tags like [verse], [chorus], [bridge]\n   - Adjust lyrics_strength (1.00 is default)\n2. Configure `KSampler` node parameters:\n   - Adjust steps (50 recommended for ACE-Step)\n   - Set cfg (4.0 is default)\n   - Set denoise value (1.00 is default)\n3. In `EmptyAceStepLatentAudio` node:\n   - Set desired seconds duration (30.0 is default)\n   - Set batch_size\n4. Click `Run` button to run the ACE-Step workflow\n5. In `SaveAudio` node: listen to or save your generated music\n\n##### ACE-Step Core Generation Workflow\n* **Best for:** Creating original music from text descriptions and lyrics\n* **Characteristics:**\n  - Fast generation (15× faster than LLM alternatives)\n  - Strong musical coherence and quality\n  - Flexible duration control\n\n##### ACE-Step Specialized Workflows (LoRA-based)\n\n* **Lyric2Vocal:** ACE-Step model fine-tuned for generating high-quality vocals from lyrics\n* **Text2Samples:** Specialized ACE-Step variant for producing instrumental loops and samples\n* **RapMachine:** Optimized ACE-Step model for rap generation with various styles\n\n#### 3.2 Parameter Reference for ComfyUI ACE-Step\n\n**TextEncodeAceStepAudio Node:**\nThis node processes text inputs to guide ACE-Step music generation.\n- `clip`: Text field for style descriptions, genres, and mood\n- `lyrics`: Text field for song lyrics with optional structure tags\n- `lyrics_strength`: Controls how strongly the lyrics influence generation (default: 1.00)\n\n**KSampler Node:**\nControls the diffusion sampling process in ACE-Step.\n- `seed`: Sets randomization seed for reproducible results\n- `control_after_generate`: Options for seed behavior after generation\n- `steps`: Number of diffusion steps (higher = more refinement)\n- `cfg`: Classifier-free guidance scale (higher = more adherence to prompt)\n- `sampler_name`: Algorithm used for sampling (res_multistep recommended)\n- `scheduler`: Noise schedule type (simple recommended)\n- `denoise`: Controls noise removal level (1.00 is full denoising)\n\n**EmptyAceStepLatentAudio Node:**\nInitializes the audio generation space.\n- `seconds`: Duration of generated audio in seconds\n- `batch_size`: Number of samples to generate simultaneously\n\n**VAEDecodeAudio Node:**\nDecodes latent representations into audible format.\n- `samples`: Input from KSampler\n- `vae`: VAE model used for decoding\n\n**SaveAudio Node:**\nOutputs the final ACE-Step audio result.\n- `filename_prefix`: Prefix for saved audio files\n- `audio`: Player for previewing generated audio\n\n#### 3.3. Advanced Techniques with ComfyUI ACE-Step\n\n**Variations Generation:**\n- Adjust variance parameter to control similarity to original ACE-Step generations\n- Higher variance creates more divergent outputs while preserving core musical elements\n\n**Repainting:**\n- Selectively regenerate specific sections of audio while preserving the rest\n- Useful for fixing problematic segments without changing the entire composition\n\n**Lyric Editing in ACE-Step:**\n- Modify lyrics while maintaining melody, vocal timbre, and accompaniment\n- Supports editing in multiple languages while preserving musical structure\n\n**Voice Cloning:**\n- Preserves vocal characteristics while generating new content with ACE-Step\n- Can be combined with lyric editing for flexible vocal performances\n\n**Style Transfer:**\n- Apply new musical styles to existing compositions\n- Maintains core musical structure while adopting different genre characteristics\n\n#### 3.4. ACE-Step Prompt Tips:\n\n**For General Music:**\n- Be specific about genre, mood, and instrumentation in ACE-Step prompts\n- Example prompts: \"electronic, rock, pop\" or \"funk, pop, soul, melodic\"\n- More detailed prompts: \"dark, death rock, metal, hardcore, electric guitar, powerful, bass, drums, 110 bpm, G major\"\n\n**For Instrumental Music:**\n- Specify instruments and musical characteristics\n- Example prompts: \"saxophone, jazz\" or \"violin, solo, fast tempo\"\n- More detailed prompts: \"sonata, piano, Violin, B Flat Major, allegro\"\n\n**For Multilingual Support:**\n- ACE-Step works best with: English, Chinese, Russian, Spanish, Japanese, German, French, Portuguese, Italian, Korean\n- Non-Latin script languages like Chinese, Japanese, and Korean are well-supported\n\n## More Information about ACE-Step\nFor additional details and development references:\n- Original ACE-Step model by [ACE Studio and StepFun](https://github.com/ace-step/ACE-Step)\n- Model developers: Junmin Gong, Sean Zhao, Sen Wang, Shengyuan Xu, and Joe Guo\n\n### Acknowledgements\nThis workflow is powered by **ACE-Step**, co-developed by **ACE Studio** and **StepFun**. The **ComfyUI ACE-Step integration** enables seamless music generation within the ComfyUI environment. Full credit goes to the original authors for their groundbreaking work on ACE-Step.\n"
    },
    {
        "id": "1225",
        "readme": "### 1. What is the ComfyUI HunyuanCustom Multi-Subject Workflow?\nThe ComfyUI HunyuanCustom Multi-Subject workflow integrates Tencent's advanced HunyuanCustom model into the ComfyUI environment, specifically optimized for dual-subject generation. Built upon the HunyuanVideo framework, this HunyuanCustom workflow focuses on generating videos featuring interactions between two subjects (such as a person and a product) with exceptional identity preservation. The HunyuanCustom workflow, optimized by Kijai, includes several enhancements like FreSca technology and specialized nodes designed to improve HunyuanCustom generation quality, consistency, and efficiency.\n\n### 2. Benefits of ComfyUI HunyuanCustom Multi-Subject Workflow:\n* **Superior Identity Preservation:** HunyuanCustom excels at maintaining consistent identity features of both subjects throughout generated videos.\n* **Natural Subject Interaction:** The HunyuanCustom model enables realistic interactions between a person and a product, ideal for virtual demonstrations.\n* **Enhanced Quality:** Implementation of FreSca technology and optimized nodes significantly improves overall HunyuanCustom video quality and stability.\n* **Product Demonstration Focus:** HunyuanCustom is specifically designed for virtual human advertisements and product showcases.\n* **Performance Optimization:** HunyuanCustom TeaCache implementation delivers approximately 30% faster processing speeds compared to standard workflows.\n\n### 3. How to Use the ComfyUI HunyuanCustom Workflow\n\n#### 3.1 Generation Methods with ComfyUI HunyuanCustom Multi-Subject Workflow\n\n**Example Setup for HunyuanCustom Multi-Subject:**\n1. Prepare inputs:\n   - Upload a reference image of a person (preferably upper-body shot) for best HunyuanCustom identity preservation\n   - Upload a reference image of a product or second subject\n   - Enter simple and clear prompt describing the desired interaction between the person and product\n   - Set negative prompt to avoid unwanted elements in your HunyuanCustom output\n2. Configure the HunyuanCustom nodes with recommended settings (see Parameter Reference)\n3. Click `Run` to run the HunyuanCustom workflow\n4. Retrieve your HunyuanCustom video from the output node\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1225/readme01.webp\" alt=\"HunyuanCustom\" width=\"650\"/>\n\n##### HunyuanCustom Dual-Subject Generation\n* **Best for:** Creating videos featuring person-product interactions with strong HunyuanCustom identity preservation\n* **Characteristics:**\n  - Maintains consistent identity of both the person and product throughout the HunyuanCustom video\n  - Enables natural interaction between the person and product in HunyuanCustom outputs\n  - Achieves realistic demonstrations and product presentations\n\n##### HunyuanCustom Virtual Advertisement\n* **Best for:** Creating product demonstrations and marketing videos with HunyuanCustom\n* **Characteristics:**\n  - Preserves product details and branding elements in HunyuanCustom videos\n  - Shows natural human interaction with the product\n  - Ideal for virtual showrooms and HunyuanCustom demonstrations\n\n#### 3.2 Parameter Reference for ComfyUI HunyuanCustom\n\n**Main HunyuanCustom Node:**\n- `Resolution`: Sets HunyuanCustom output video resolution (recommended: 720×1280 for best quality)\n- `Frames`: Number of frames to generate in HunyuanCustom videos (typically 129 frames for 5-second videos)\n- `Steps`: Number of denoising steps for HunyuanCustom (30 recommended for balance of quality and speed)\n- `CFG Scale`: Controls prompt adherence strength in HunyuanCustom (higher = more prompt-aligned but potentially less natural)\n- `Flow Shift`: Adjusts motion fluidity in HunyuanCustom outputs (recommended value: 13.0)\n\n**FreSca Enhancement Node:**\n- `Enabled`: Toggle the FreSca enhancement on/off for HunyuanCustom\n- `Scale`: Strength of FreSca effect (1.0 recommended for balanced enhancement)\n- `Mode`: Set to \"single blocks\" for optimal hand stability and detail preservation\n\n**ID Enhancement Module:**\n- `Weight`: Controls how strongly the identity features are preserved in HunyuanCustom\n- `Temporal Concat`: Enables improved identity consistency across frames\n\n**SLG (Stability-Latent-Guidance) Node:**\n- `Mode`: Set to \"single blocks\" for best hand detail preservation in HunyuanCustom\n- `Strength`: Controls how strongly the stability guidance is applied to the generation\n\n#### 3.3. Advanced Optimization with ComfyUI HunyuanCustom\n\n**Best Practices for Subject Images:**\n- Use upper-body or shoulder-up portraits for best face quality and consistency in HunyuanCustom\n- Avoid full-body shots as they disperse attention and may result in poorer facial detail\n- For unavoidable full-body requirements, consider post-processing with GPEN face enhancement\n\n**Improving Hand Rendering in HunyuanCustom:**\n- Enable FreSca enhancement and set to \"single blocks\" mode\n- Use SLG nodes with appropriate strength settings in your HunyuanCustom workflow\n- Include specific hand descriptions in your prompt to guide generation\n\n**Performance Enhancement for HunyuanCustom:**\n- Implement TeaCache for approximately 30% faster processing\n- Use appropriate batch sizes based on your GPU capabilities\n- Consider lower resolutions for draft generations and higher resolutions for final HunyuanCustom outputs\n\n**Quality Enhancements:**\n- Add Hunyuan Reward LoRA for improved overall HunyuanCustom quality\n- Use the CFG Zero Star node to enhance generation quality\n- Experiment with different sampling methods for optimal HunyuanCustom results\n\n## More Information about HunyuanCustom\nFor additional details and development references:\n- Original HunyuanCustom model by [Tencent](https://github.com/Tencent/HunyuanCustom)\n- ComfyUI integration of HunyuanCustom by [Kijai](https://github.com/Kijai/ComfyUI-HunyuanVideoWrapper)\n\n### Acknowledgements\nThis workflow is powered by **HunyuanCustom**, developed by **Tencent's Hunyuan team**. The original paper \"HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation\" introduced this powerful framework that emphasizes subject consistency while supporting image, audio, video, and text conditions.\n\nThe **ComfyUI integration** of HunyuanCustom is provided by **Kijai**, including optimizations like FreSca technology, SLG nodes, and TCatch implementation that significantly improve generation quality and efficiency. Full credit goes to the original authors at Tencent for their groundbreaking work on HunyuanCustom and to Kijai for the outstanding ComfyUI implementation and enhancements.\n"
    },
    {
        "id": "1226",
        "readme": "### 1. What is ComfyUI Insert Anything?\n\nComfyUI Insert Anything brings ByteDance's powerful Insert Anything technology to the ComfyUI environment. Built on the Diffusion Transformer (DiT) architecture, this Insert Anything tool enables you to seamlessly transfer elements from reference images to target scenes while maintaining visual harmony and preserving fine details.\n\nThe Insert Anything workflow stands out from traditional image editing tools by allowing natural integration of objects, people, garments, and faces between different images with remarkable identity preservation that makes Insert Anything a versatile solution.\n\n### 2. Key Benefits of Insert Anything\n\n* **All-in-One Solution:** handles multiple tasks (inserting people, objects, clothing, faces) through a single unified model\n* **Dual Control Methods:** works with either mask-guided selections or text-guided descriptions based on your needs\n* **Detail Preservation:** maintains the distinctive features and identity elements from your reference images\n* **Seamless Integration:** achieves natural-looking results where inserted elements blend harmoniously with their surroundings\n* **Quality Output:** generates high-resolution, coherent results across diverse editing scenarios\n* **ComfyUI Compatible:** integrates smoothly with your existing ComfyUI workflows and projects\n\n### 3. How to Use Insert Anything\n\n#### 3.1 Basic Insert Anything Workflow Setup\nStep-by-Step Insert Anything Process:\n1. Prepare your images for Insert Anything:\n   - In the `Source` section: Upload the target image you want Insert Anything to modify (e.g., a racetrack scene)\n   - In the `Reference` section: Upload an image containing the element you want Insert Anything to insert (e.g., a race car)\n   - The Insert Anything system will automatically generate masks using Segment Anything Model (SAM)\n2. Set your prompts in the `CR Prompt Text` node for Insert Anything:\n   - First prompt: Describe what's in your reference image (e.g., \"race car\")\n   - Second prompt: Specify details about the Insert Anything insertion (e.g., \"Race Car yellow and red\")\n3. Optional Insert Anything adjustments:\n   - Fine-tune masking with `Src_mask_option` and `Ref_mask_option` if needed\n   - Adjust threshold parameters for more precise object detection in Insert Anything\n4. Click the `Run` button to process your images with Insert Anything\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1226/readme01.webp\" alt=\"Insert Anything\" width=\"650\"/>\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1226/readme02.webp\" alt=\"Insert Anything\" width=\"650\"/>\n\n#### 3.2 What You Can Create with Insert Anything\n\n* **Product Showcase:** Use Insert Anything to insert products into hand-held poses or contextual scenes\n* **Virtual Try-On:** Let Insert Anything replace clothing items while maintaining natural fit and draping\n* **Identity Transfer:** Apply Insert Anything to swap facial features while preserving natural expressions\n* **Scene Composition:** Utilize Insert Anything to replace large objects (like vehicles) in complex environments\n\n#### 3.3 Understanding Insert Anything Parameters\n\n**Core Insert Anything Components:**\n\n**Insert Anything Preprocessing:**\n- Manages Insert Anything image preparation and automatic masking\n  - `source_image`: Your target image requiring Insert Anything modification\n  - `ref_image`: Your reference image with the element for Insert Anything to insert\n  - `source_mask`/`ref_mask`: Auto-generated or user-provided masks for Insert Anything\n  - `iterations`: Controls Insert Anything mask expansion for better coverage (default: 2)\n\n**Insert Anything Inference:**\n- Executes the actual Insert Anything insertion process\n  - `seed`: Allows reproducible Insert Anything results or variations by changing this value\n\n**Segmentation Tools for Insert Anything:**\n\n**SAM Integration with Insert Anything:**\n- `LayerMask: Load SegmentAnything Models`: Initializes the object detection system for Insert Anything\n- `LayerMask: SegmentAnythingUltra V3`: Controls the Insert Anything masking process\n  - `threshold`: Adjusts mask detection sensitivity for Insert Anything (default: 0.30)\n  - `detail_erode`/`detail_dilate`: Fine-tunes mask boundaries in Insert Anything\n  - `process_detail`: Toggles enhanced processing for complex edges in Insert Anything\n  - `max_megapixels`: Manages memory usage for large Insert Anything images\n\n**Insert Anything Mask Controls:**\n- `Src_mask_option`/`Ref_mask_option`: Select between automatic (sketch) or manual (upload) masking for Insert Anything\n- Default settings work well in most cases with Insert Anything's automatic detection\n\n#### 3.4 Tips for Perfect Insert Anything Results\n\n**Insert Anything Masking Refinement:**\n- Adjust the threshold value (default: 0.30) for more precise Insert Anything automatic masking\n- Increase the `iterations` parameter when you need Insert Anything to expand mask coverage\n- For complex scenarios, try uploading custom masks through the Insert Anything option nodes\n\n**Insert Anything Workflow Optimization:**\n- Experiment with different `seed` values to get variations from Insert Anything\n- Provide high-quality, well-lit images for the best Insert Anything results\n- Match image proportions when possible for more natural Insert Anything integration\n\n**Effective Insert Anything Prompting:**\n- Keep Insert Anything prompts clear and specific about what you're inserting\n- For vehicles, include color and type information in your Insert Anything prompts\n- When using Insert Anything for face swapping, simple terms like \"person\" often work best\n- For clothing insertion, describe the garment type and color in your Insert Anything prompt\n\n### Acknowledgements\n\nThis workflow implements **Insert Anything**, originally developed by **ByteDance** as described in their research paper \"[Insert Anything: Image Insertion via In-Context Editing in DiT](https://arxiv.org/abs/2504.15009)\". The ComfyUI Insert Anything integration provides a user-friendly interface for this technology. Full credit goes to the original authors for their innovative Insert Anything work.\n"
    },
    {
        "id": "1228",
        "readme": "## What is the ComfyUI VACE 14B Workflow?\n\n> **5/24/2025 Update - Model Options:**\n> - **Speed Mode**: FP8E5M2 model + Enable quantization (faster, lower memory)\n> - **Quality Mode**: FP8E4M3 model + Disable quantization (better quality, slower)\n> \n> *Note: Do not enable quantization with FP8E4M3 model as it may cause errors.*\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1228/readme06.webp\" alt=\"ComfyUI VACE\" width=\"650\"/>\n\nComfyUI VACE 14B is a powerful unified framework that integrates various video generation and editing capabilities into a single model. Built on the advanced Wan2.1 foundation and developed by Tongyi Lab at Alibaba Group, the ComfyUI VACE enables creators to perform 5 essential video manipulation tasks without switching between specialized tools. This ComfyUI VACE 14B workflow harnesses the power of the 14B parameter model to provide high-quality video creation and editing capabilities through an intuitive interface.\n\n### 1. Benefits of ComfyUI VACE 14B\n* **All-in-One Solution:** The ComfyUI VACE offers a unified framework for video creation and editing without switching between tools\n* **High-Quality Output:** Generate videos at up to 720p resolution with the powerful VACE 14B model\n* **Flexible Controls:** The ComfyUI VACE workflow provides multiple input modalities including reference images, masks, depth maps, and text prompts\n* **Identity Preservation:** ComfyUI VACE maintains subject identity during transformations and generation when using VACE 14B\n* **Task Composition:** ComfyUI VACE allows you to combine different capabilities for complex transformations through VACE 14B\n\n### 2. Five Key Capabilities in ComfyUI VACE 14B Workflow\n\nThe ComfyUI VACE workflow includes 5 distinct components, each specialized for different video creation and editing tasks powered by VACE 14B:\n\n1. **Reference Image+ControlNet:** ComfyUI VACE lets you swap the main character in your video with one from a reference image, while keeping motion and structure intact thanks to VACE 14B\n2. **Two Images to Video:** With ComfyUI VACE, create a video combining two characters or objects, each taken from a separate reference image through VACE 14B\n3. **First and last frames:** ComfyUI VACE precisely guides video generation by setting both the first and last frames, ensuring fluid motion across frames via VACE 14B\n4. **Video Object Editing:** ComfyUI VACE allows you to selectively modify specific objects within videos while preserving others utilizing VACE 14B\n5. **Video Expansion:** Using ComfyUI VACE, perform spatial outpainting to enlarge the video frame while preserving visual coherence with VACE 14B technology\n\n### 3. Quick Start Guide for ComfyUI VACE 14B\n\n#### 3.1 ComfyUI VACE Reference Image+ControlNet Quick Start\n1. Select the \"Reference Image+ControlNet\" workflow group in ComfyUI VACE\n2. Load your source video in the `Load Video (Upload)` node\n3. Upload a reference image of the new subject in the `Load Image` node\n   - **Tip:** Using white background reference images often results in better subject integration when using VACE 14B\n4. Enter descriptive prompts about the desired scene and style\n5. Use negative prompts to avoid unwanted elements in your ComfyUI VACE output generated by VACE 14B\n6. Recommended resolution: 512x672 recommended for memory efficiency with ComfyUI VACE 14B\n7. Adjust sampling parameters\n8. Click \"Run\" to generate the video with the reference image's style/subject using ComfyUI VACE 14B\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1228/readme01.webp\" alt=\"ComfyUI VACE\" width=\"650\"/>\n\n#### 3.2 ComfyUI VACE Two Images to Video Quick Start\n1. Select the \"Two Images to Video\" workflow group in ComfyUI VACE\n2. Upload your first reference image to the first `Load Image` node\n3. Upload your second reference image to the second `Load Image` node\n4. This ComfyUI VACE method powered by VACE 14B is particularly well-suited for e-commerce applications and is noted for its ability to render fine details effectively, including hands\n5. Enter detailed prompts describing the desired scene, style, and the motion or transition characteristics between the images\n6. Adjust `frame_rate` in the `Video Combine` node to set video smoothness in your ComfyUI VACE output created by VACE 14B\n7. Configure sampling parameters in the `WanVideo Sampler` node\n8. Click \"Run\" to generate the video transition with ComfyUI VACE 14B\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1228/readme02.webp\" alt=\"ComfyUI VACE\" width=\"650\"/>\n\n#### 3.3 ComfyUI VACE First and Last Frames Quick Start\n1. Select the \"First and last frames\" workflow group in the ComfyUI VACE interface\n2. Upload your starting frame to the first `Load Image` node\n3. Upload your ending frame to the second `Load Image` node\n4. Configure the `WanVideo VACE Start To End Frame` node parameters for optimal VACE 14B results\n5. Enter prompts describing the desired motion and transition style for ComfyUI VACE processing\n6. Adjust sampling parameters to fine-tune VACE 14B generation\n7. Click \"Run\" to create a smooth transition video with ComfyUI VACE 14B\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1228/readme03.webp\" alt=\"ComfyUI VACE\" width=\"650\"/>\n\n#### 3.4 ComfyUI VACE Video Object Editing Quick Start\n1. Select the \"Video Object Editing\" workflow group in ComfyUI VACE\n2. Upload your source video using the `Load Video (Upload)` node  \n3. Manually take a screenshot of one frame from your video using any video player  \n4. Upload this frame into the `Points Editor` node via the `bg_image` input — this frame will be used for manual point annotation in ComfyUI VACE for VACE 14B processing\n5. Upload the reference image of the object you want to insert or use for replacement using the `Load Image` node  \n6. In the ComfyUI VACE `Points Editor`, mark areas as follows:  \n   - Green points (Shift + Left Click): mark areas to be modified (e.g. the object to be replaced)  \n   - Red points (Shift + Right Click): mark areas to be preserved (e.g. background or subject)  \n   - Right Click on an existing point to delete it  \n7. The `S2/SAM2 Segmentation` module will generate a mask based on your annotations  \n8. (Optional) Refine the mask using the `Grow Mask With Blur` node in ComfyUI VACE\n9. Enter positive prompts to describe the new object or modification (e.g. \"a traditional red umbrella\")  \n10. Enter negative prompts to exclude unwanted elements from your ComfyUI VACE output generated by VACE 14B\n11. Click \"Run\" to generate the edited video with the object replaced according to your inputs using ComfyUI VACE 14B\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1228/readme04.webp\" alt=\"ComfyUI VACE\" width=\"650\"/>\n\n#### 3.5 ComfyUI VACE Video Expansion Quick Start\n1. Select the \"Video Expansion\" workflow group in ComfyUI VACE\n2. Load your source video in the `Load Video (Upload)` node\n3. Configure the `ImagePad KJ` node with your desired expansion settings\n4. Enter prompts describing what content should appear in the expanded areas for ComfyUI VACE processing through VACE 14B\n5. The ComfyUI VACE enhanced by VACE 14B offers better edge handling for more seamless expansion\n6. Adjust sampling parameters for consistency with the original video\n7. Click \"Run\" to create the expanded video with ComfyUI VACE 14B\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1228/readme05.webp\" alt=\"ComfyUI VACE\" width=\"650\"/>\n\n### 4. Key Parameters for ComfyUI VACE 14B\n\n**Common Sampling Parameters for ComfyUI VACE:**\n- `strength`: 1.0 (default for maximum effect in ComfyUI VACE 14B)\n  - Higher = stronger effect of the VACE 14B algorithm within ComfyUI VACE\n  - Lower = more preservation of original content\n- `guidance`: 7-9 (recommended for optimal ComfyUI VACE results when using VACE 14B)\n  - Higher = more faithful to references/conditions but less creativity\n  - Lower = more creative but may drift from references\n- `steps`: 20-25 (recommended for ComfyUI VACE 14B)\n  - Higher = better quality but slower generation\n  - Lower = faster generation but potentially lower quality\n- `cfg`: 4.0 (recommended for ComfyUI VACE integrated with VACE 14B)\n  - Higher = stronger prompt influence but potentially less natural results\n  - Lower = more natural but may ignore prompt details\n- `seed`: 18 (example for ComfyUI VACE 14B)\n  - Same seed + same settings = same results\n  - Change for different variations in your ComfyUI VACE outputs generated via VACE 14B\n- `shift`: 5.0 (default in ComfyUI VACE 14B)\n  - Higher = more motion variation\n  - Lower = more conservative motion\n\n\n### 5. Advanced Parameters for ComfyUI VACE 14B\n\n**WanVideo VACE:**\n- `num_frames`: 29 (default for reference workflows in ComfyUI VACE 14B)\n  - Higher = longer video but more processing time\n  - Lower = shorter video but faster generation with ComfyUI VACE utilizing VACE 14B\n- `vace_start_percent`: 0.00 (default in ComfyUI VACE when using VACE 14B)\n  - Controls where in the generation process the VACE effect begins\n- `vace_end_percent`: 1.00 (default in ComfyUI VACE 14B)\n  - Controls where in the generation process the VACE effect ends\n\n**WanVideo VACE Start To End Frame:**\n- `num_frames`: 40 (default in ComfyUI VACE 14B)\n  - Higher = longer, smoother transitions but more processing time\n  - Lower = shorter video but faster generation with ComfyUI VACE powered by VACE 14B\n- `empty_frame_level`: 0.50 (default for ComfyUI VACE 14B)\n  - Higher = more creative middle frames\n  - Lower = more direct interpolation\n\n**Video Combine in ComfyUI VACE:**\n- `frame_rate`: 16 (default for ComfyUI VACE with VACE 14B support)\n  - Higher = smoother motion but larger file size\n  - Lower = choppier motion but smaller file size\n- `format`: video/h264-mp4 (default in ComfyUI VACE 14B)\n- `pix_fmt`: yuv420p (default in ComfyUI VACE featuring VACE 14B)\n\n**Experimental Parameters for ComfyUI VACE 14B:**\n- `cfg_zero_star`: true (default in ComfyUI VACE working with VACE 14B)\n  - Experimental parameter that can affect generation quality\n- `fresca_scale_high`: 1.25 (default for ComfyUI VACE 14B)\n  - Controls high-frequency detail preservation\n- `fresca_freq_cutoff`: 20 (default in ComfyUI VACE enhanced by VACE 14B)\n  - Frequency cutoff for detail preservation\n\n### Acknowledgements\nThis ComfyUI VACE 14B workflow is powered by **VACE** (Video All-in-one Creation and Editing), developed by Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu at **Tongyi Lab, Alibaba Group**. The ComfyUI VACE integration utilizes nodes and adaptations by **Kijai** ([WanVideo_comfy](https://huggingface.co/Kijai/WanVideo_comfy)) for implementing VACE 14B. The original research paper for VACE 14B is available at [https://arxiv.org/abs/2503.07598](https://arxiv.org/abs/2503.07598). These ComfyUI VACE workflows featuring VACE 14B can also be run on Ringhub for those without adequate local hardware.\n"
    },
    {
        "id": "1229",
        "readme": "## BAGEL AI: Multimodal Foundation Model for ComfyUI\n\n**BAGEL (BAndwidth-efficient Generalist Expert Learner) AI** is a powerful multimodal foundation model designed for both **image generation** and **vision-language understanding**. Based on a 14B parameter Mixture-of-Transformer-Experts (MoT) architecture—with 7B active at inference—BAGEL AI delivers state-of-the-art performance across text-to-image generation, image editing, and image understanding tasks.\n\nIntegrated directly into **ComfyUI**, BAGEL AI allows creators to generate detailed images from natural language prompts, edit visuals with textual instructions, and perform multimodal tasks like visual Q&A, captioning, and step-by-step reasoning. BAGEL AI combines the quality of diffusion models (like Stable Diffusion 3) with the analytical power of leading VLMs (outperforming models like Qwen2.5-VL and InternVL-2.5).\n\n\n### Why Use BAGEL AI?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1229/readme01.webp\" alt=\"BAGEL AI\" width=\"750\"/>\n</p>\n\nThe BAGEL AI workflow offers:\n- **Text-to-Image Generation**: Create high-quality images from natural language prompts using BAGEL AI  \n- **Image Editing via Text**: Modify existing images using descriptive instructions with BAGEL AI  \n- **Image Understanding**: Perform image captioning, Q&A, and visual analysis tasks in BAGEL AI  \n- **Multimodal Reasoning**: Enable step-by-step explanation or analysis of visual inputs through BAGEL AI  \n- **All-in-One Foundation Model**: Use a single 14B MoT-based architecture for diverse multimodal tasks within BAGEL AI  \n\nWith BAGEL AI, artists, researchers, and developers can explore both the generative and analytical capabilities of multimodal AI using a unified and extensible ComfyUI interface powered by BAGEL AI technology.\n\n\n## 1 - Text-to-Image Generation with BAGEL AI\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1229/readme02.webp\" alt=\"BAGEL AI\" width=\"550\"/>\n</p>\n\n#### Generate Images Using Natural Language Prompts\nBAGEL AI allows you to create high-quality images directly from text inputs. To get started with BAGEL AI:\n1. Enter a detailed **text prompt** into the `Prompt` input node in BAGEL AI.  \n2. Optionally configure parameters like seed, aspect ratio, or decoding steps within BAGEL AI.  \n3. Run the BAGEL AI workflow to generate a new image from the BAGEL model.  \n\nThis BAGEL AI function is ideal for concept art, visual ideation, storytelling, or rapid prototyping using purely natural language descriptions.\n\n\n## 2 - Image Understanding and Visual Q&A with BAGEL AI\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1229/readme03.webp\" alt=\"BAGEL AI\" width=\"550\"/>\n</p>\n\n#### Analyze and Understand Images Using Language\n\nBAGEL AI includes advanced multimodal reasoning and comprehension features, making BAGEL AI ideal for image captioning, analysis, and Q&A:\n1. Upload an **image to analyze** in BAGEL AI.  \n2. Type a **question or prompt** about the image in BAGEL AI (e.g., \"What is the man holding?\", \"Describe this scene.\").  \n3. The BAGEL AI system returns a visual answer or reasoning trace based on the image content.  \nThis BAGEL AI feature is particularly useful for education, content tagging, accessibility workflows, or AI agents needing visual grounding through BAGEL AI capabilities.\n\n\n## 3 - Image Editing with Textual Instructions in BAGEL AI\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1229/readme04.webp\" alt=\"BAGEL AI\" width=\"550\"/>\n</p>\n\n#### Modify Existing Images via Prompt-Based Editing\n\nBAGEL AI also supports prompt-based image editing through its advanced BAGEL AI interface. Here's how to use BAGEL AI:\n1. Upload your **original image** in the BAGEL AI input node.  \n2. Provide a **text instruction** describing the modification you want in BAGEL AI (e.g., \"add a sunset background\", \"make it snow\", etc.).  \n3. Run the node group to apply your desired edits using BAGEL AI processing.  \n\nThis allows artists and designers to non-destructively transform images through simple text without needing manual photo editing, all powered by BAGEL AI technology.\n\n\n## Acknowledgement\n\nThe BAGEL AI workflow for ComfyUI is based on the open-source **BAGEL-7B-MoT** model by **ByteDance Seed**.  \nComfyUI integration and BAGEL AI workflow setup were developed by **neverbiasu**, providing seamless access to image generation, editing, and understanding capabilities within a single unified BAGEL AI interface.\n\nGitHub Repository: [https://github.com/neverbiasu/ComfyUI-BAGEL](https://github.com/neverbiasu/ComfyUI-BAGEL)\n\n\n## BAGEL AI Model Information\n\n- **Model Name**: ComfyUI BAGEL-7B-MoT  \n- **Architecture**: Mixture-of-Transformer-Experts (MoT) optimized for BAGEL AI  \n- **Total Parameters**: 14B (7B Active) in BAGEL AI  \n- **ComfyUI Path**: `models/bagel/ComfyUI-BAGEL-7B-MoT/`  \n- **Automatic Download**: Enabled for BAGEL AI  \n- **Manual Download**: [https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT](https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT)\n\n"
    },
    {
        "id": "1230",
        "readme": "## What is FLUX Dev ControlNet for ComfyUI?\n\n**FLUX Dev ControlNet** is a versatile image generation workflow powered by the **FLUX.1-dev-ControlNet-Union-Pro-2.0** model. This specialized *FLUX Dev ControlNet* workflow supports multiple conditioning modes, including **Canny**, **Depth**, **Pose**, **Soft Edge**, and **Recolor**, allowing artists to guide image generation with enhanced structure, flow, and stylization.\n\nIntegrated into **ComfyUI**, *FLUX Dev ControlNet* workflow enables users to input both a prompt and a reference image to precisely influence the resulting image's composition or pose. It’s especially useful for concept art, character design, and creative prototyping where maintaining structure or gesture is important.\n\n## Why Use FLUX Dev ControlNet?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1230/readme01.webp\" alt=\"FLUX Dev ControlNet\" width=\"750\"/>\n</p>\n\n**FLUX Dev ControlNet** provides:\n\n- **Multi-mode ControlNet**: Use Canny, Depth, Pose, Soft Edge, or Recolor as guidance\n- **Prompt-Based Generation**: Combine structure control with natural language creativity\n- **Works with Other ControlNets**: Can be combined with other ControlNet models for multi-modal influence\n- **Fast, Modular Setup**: Quickly configure prompts, control input, and output settings via grouped nodes in ComfyUI\n\nThis makes *FLUX Dev ControlNet* ideal for workflows that demand both artistic control and creative freedom.\n\n## 1 - FLUX Dev Models\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1230/readme02.webp\" alt=\"FLUX Dev ControlNet\" width=\"550\"/>\n</p>\n\nThe first section handles model loading for your *FLUX Dev ControlNet* setup:\n\n- **FLUX Dev Model**: Loads `FLUX.1-dev-ControlNet-Union-Pro-2.0`\n- **Text Encoders**: CLIP-L and optional T5 variants\n- **VAE**: Standard autoencoder (e.g., `ae.safetensors`)\n\nYou can customize the text encoder or use different VAE options depending on your desired output fidelity with *FLUX Dev ControlNet*.\n\n## 2 - Prompts\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1230/readme03.webp\" alt=\"FLUX Dev ControlNet\" width=\"550\"/>\n</p>\n\nEnter your prompt in this group for *FLUX Dev ControlNet*:\n\n- **Positive Prompt**: Describe what you want to generate (e.g., \"A sci-fi cityscape at dusk\")\n- **Negative Prompt**: Describe what to avoid (e.g., \"blurry, low quality, artifacts\")\n\n## 3 - ControlNet\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1230/readme04.webp\" alt=\"FLUX Dev ControlNet\" width=\"550\"/>\n</p>\n\nThis group lets you control the composition or structure of the output image using a reference in *FLUX Dev ControlNet*:\n\n1. **Upload an image** as a reference input\n2. **Select a preprocessor**: Depth, Canny, Pose, or Recolor\n3. Set **Control Weight**: `0.7` works well for most tasks\n4. Adjust preprocessor parameters if needed (e.g., edge thresholds or depth smoothing)\n\nYou can also stack with another *ControlNet* if required for dual conditioning with *FLUX Dev ControlNet*.\n\n## 4 - KSampler & Output\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1230/readme05.webp\" alt=\"FLUX Dev ControlNet\" width=\"550\"/>\n</p>\n\n- **Sampler Settings**: Choose sampler type (e.g., DPM++, Euler, etc.), steps, and seed.\n- **Output**: Generated image will appear in the output viewer and saved in the ouput folder.\n\nThis stage allows you to fine-tune the final output and easily preview iterations with *FLUX Dev ControlNet*.\n\n\n## Acknowledgement\n\nThe **FLUX Dev ControlNet** workflow uses the open-source **FLUX.1-dev-ControlNet-Union-Pro-2.0** model developed by **Shakker-Labs**.\n\nModel Link: [https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0)\n\nWorkflow setup and design are ComfyUI-native and fully modular.\n\n## Model Information\n\n- **Model Name**: FLUX.1-dev-ControlNet-Union-Pro-2.0\n- **Architecture**: Multi-Condition (Supports Depth, Canny, Pose, Recolor, etc.)\n- **Control Weight Range**: Recommended `0.6–0.8`\n- **Folder Path**: `models/controlnet/FLUX.1-dev-ControlNet-Union-Pro-2.0-fp8.safetensors`\n- **Download URL**: [FLUX.1-dev-ControlNet-Union-Pro-2.0](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0)\n- **Download URL FP8** : [FLUX.1-dev-ControlNet-Union-Pro-2.0(fp8)](https://civitai.com/models/1488208)\n"
    },
    {
        "id": "1231",
        "readme": "### 1. What is the ComfyUI Uni3C Workflow?\n\nThe **Uni3C** workflow integrates the advanced **Uni3C** (Unifying Precisely 3D-Enhanced Camera and Human Motion Controls) model into the *ComfyUI* environment. Developed by **DAMO Academy (Alibaba Group)**, **Uni3C** addresses the fundamental challenge of controllable video generation by unifying camera trajectory control and human motion control within a single 3D-enhanced framework.\n\nBuilt on the *FLUX* diffusion transformer architecture and powered by the *Wan2.1* foundation model, **Uni3C** introduces **PCDController** - a plug-and-play control module that utilizes unprojected point clouds derived from monocular depth estimation. This approach enables precise camera control within **Uni3C** while maintaining the generative capabilities of large-scale video diffusion models. The **Uni3C** system employs SMPL-X character models and global 3D world guidance to achieve spatially consistent video generation across both environmental scenes and human characters.\n\n### 2. Benefits of Uni3C:\n\n* **Unified 3D-Enhanced Framework:** Uni3C simultaneously processes camera trajectories and human motion in a coherent 3D world space.\n* **PCDController Architecture:** With the lightweight 0.95B parameter controller, Uni3C leverages unprojected point clouds from monocular depth estimation, without compromising the 14B parameter base model.\n* **SMPL-X Integration:** Uni3C provides advanced 3D human body model support.\n* **Geometric Prior Utilization:** Uni3C point cloud-based 3D geometric understanding provides robust camera control from single images.\n* **Global 3D World Guidance:** Uni3C ensures rigid transformation alignment between environmental point clouds and SMPL-X characters for spatially consistent video generation.\n\n### 3. How to Use Uni3C Workflow\n\n**Uni3C** operates through **video reference extraction**, which analyzes reference videos to understand both camera movements and human motions, then applies the patterns to generate new videos from the input images. This approach enables precise control with Uni3C without manual parameter adjustment.\n\n#### 3.1 Method 1: Video-Referenced Camera Control\n\n##### Best for: \nExtracting camera movements from reference videos and applying them to new scenes using **Uni3C**.\n\n##### Setup Process:\n1.  **Load Reference Video:** Upload any video with interesting camera movement in the `Load Video (Upload)` node\n    -   **Good Examples:** Videos with camera zooming into subjects, walking footage with natural head movement, cinematic pans, orbital shots\n    -   **Key Point:** Any video with clear camera motion works - from phone recordings to professional cinematography with Uni3C.\n2.  **Load Target Image:** Upload your base image in the `Load Image` node (works with any style: realistic, anime, artwork, AI-generated)\n3.  **Write Your Prompt:** Describe your desired scene in the text prompt area for Uni3C\n4.  **Configure Settings:** The **Uni3C workflow** includes optimized parameters for 4x speed improvement\n5.  **Generate:** Run the workflow to transfer the reference video's camera movement to your scene with Uni3C.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1231/readme01.webp\" alt=\"Uni3C\" width=\"650\"/>\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1231/readme02.webp\" alt=\"Uni3C\" width=\"650\"/>\n\n##### Advanced Tips:\n* **Motion Direction Matching:** Best results occur when the reference video's movement direction aligns with your intended scene composition for Uni3C.\n* **Prompt Coordination:** Write prompts that complement the camera movement style for enhanced coherence with Uni3C.\n* **Reference Quality:** Choose reference videos with stable, clear camera movements for optimal results with Uni3C.\n\n#### 3.2 Method 2: Human Motion Transfer\n\n##### Best for:\nTransferring human motions from reference videos to different characters using Uni3C.\n\n##### Setup Process:\n1.  **Reference Video with Human Motion:** Upload a video containing the human movements you want to transfer\n2.  **Target Character Image:** Load an image of the character you want to animate\n3.  **Motion Control via Prompts:**\n    -   **Preserve Original Motion:** Use general descriptions like \"a woman walking\" to maintain the reference motion with **Uni3C**.\n    -   **Modify Actions:** Be specific to change movements: \"a woman combing her hair\" will alter hand gestures while preserving overall motion flow with **Uni3C**.\n\n##### Key Advantages:\n* **No Skeleton Required:** Unlike traditional motion capture, **Uni3C** understands human movement without complex rigging.\n* **Detail Preservation:** **Uni3C** maintains accessories, hairstyles, and clothing details during motion transfer.\n* **Simultaneous Control:** Both camera movement and human motion are transferred together from the same reference video using **Uni3C**.\n\n**Performance Optimization Architecture:**\nThe **Uni3C workflow** implements several optimization strategies including reduced hidden size from 5120 to 1024 in PCDController, zero-initialized linear projection layers, and injection of camera-controlling features only into the first 20 layers of the base model. Flow matching optimization with reduced sampling steps (10 vs 20+) and adjusted CFG guidance scales provides up to 4x processing speed improvement in **Uni3C** while maintaining generation quality.\n\n#### 3.3 Optimized Performance Settings\n\n**4x Speed Boost Configuration:**\nBased on Uni3C's built-in optimizations, the following settings provide dramatically faster processing:\n\n**WanVideo Sampler Node Settings:**\n* `Steps`: 10 (reduced from default 20+)\n* `CFG`: 1.0-1.5 (optimized for speed-quality balance)\n* `Shift`: 5.0-7.0 (author recommends 7 for best results, 5 for faster processing with Uni3C)\n* `Scheduler`: UniPC (optimized scheduler for Uni3C)\n\n**Key Performance Features:**\n* **AnimateDiff Integration:** Uni3C leverages AnimateDiff optimizations originally designed for text-to-video but effective for image-to-video generation.\n* **Smart Parameter Reduction:** Since image-to-video starts with existing visual content, fewer denoising steps are required compared to text-to-video generation with Uni3C.\n* **Optimized Processing:** Uni3C enables 70-frame videos to complete in ~4-5 minutes (vs ~27 minutes with original settings).\n\n**Quality vs Speed Options:**\n* Maximum Speed: Steps=10, CFG=1.0, Shift=5 → ~4 minutes for 70 frames with Uni3C\n* Balanced: Steps=10, CFG=1.5, Shift=7 → ~5 minutes for 70 frames with Uni3C\n* Higher Quality: Steps=15, CFG=2.0, Shift=7 → ~8-10 minutes for 70 frames with Uni3C\n\n#### 3.4 Workflow Components Understanding\n\n**Reference Video Processing Section:**\n* `Load Video (Upload)`: Accepts MP4, AVI and other standard video formats for motion reference in Uni3C.\n* `WanVideo Encode`: Processes the reference video to extract camera trajectories and motion patterns for Uni3C.\n* `Uni3C ControlNet Loader`: Loads the specialized Uni3C control model for motion understanding.\n\n**Image-to-Video Generation Section:**\n* `Load Image`: Your target image that will be animated with the reference motion by Uni3C.\n* `WanVideo Image/ToVideo Encode`: Converts your static image into a format suitable for video generation by Uni3C.\n* `WanVideo Sampler`: Core generation engine with optimized settings for 4x speed improvement in Uni3C.\n\n**Output Processing:**\n* `WanVideo Decode`: Converts the generated latent video back to viewable format from Uni3C.\n* `Video Combine`: Assembles the final video file with proper frame rate and encoding from Uni3C.\n\n### 4. Advanced Tips and Best Practices\n\n#### Choosing Reference Materials for Uni3C\n* **For Motion Transfer:** Select videos with clear, visible movements where the person stays mostly in frame for Uni3C.\n* **For Camera Control:** Any video with interesting perspective or desired camera movement for Uni3C.\n* **Best Results:** When the reference motion direction matches your intended output direction with Uni3C.\n\n#### Prompt Engineering Best Practices for Uni3C\n* **\"Don't Disturb\" Principle:** For pure motion transfer without character changes, use simple, general prompts with Uni3C.\n* **Specific Action Changes:** Be detailed when you want to modify what the character is doing with Uni3C.\n* **Character Consistency:** Focus prompts on maintaining character appearance with Uni3C.\n\n#### Quality Optimization for Uni3C\n* **Motion Consistency:** Avoid sudden changes in reference videos for smoother results with Uni3C.\n* **Frame Stability:** Ensure reference materials have consistent lighting and framing for Uni3C.\n\n### 5. Acknowledgments\n\nThis workflow is powered by **Uni3C**, developed by **DAMO Academy (Alibaba Group)**, **Fudan University**, and **Hupan Lab**. The *ComfyUI* integration is based on the excellent work by **kijai** ([ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/main/uni3c)), with additional optimizations and workflow design to make this powerful **Uni3C** technology accessible to creators worldwide.\n"
    },
    {
        "id": "1232",
        "readme": "# ComfyUI Phantom: Subject to Video\n**ComfyUI Phantom** is a powerful consistent subjects to video generation model integrated into the ComfyUI workflow environment. This ComfyUI Phantom implementation enables high-quality, identity-consistent video synthesis from one or more reference images, guided by descriptive text prompts within the familiar ComfyUI interface.\n\nBuilt upon advanced text-to-video and image-to-video architectures, *ComfyUI Phantom* specializes in generating human-centric motion while preserving subject identity. Through a unified joint text-image injection approach, ComfyUI Phantom achieves accurate cross-modal alignment—ensuring expressive, frame-consistent outputs that follow the structure and look of the provided references.\n\n## Why Use ComfyUI Phantom?\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1232/readme01.webp\" alt=\"ComfyUI Phantom\" width=\"750\"/>\n</p>\n\n**ComfyUI Phantom** offers:\n- **Reference-Based Generation**: Input one or more reference images to direct subject appearance in ComfyUI Phantom workflows\n- **Prompt + Image Control**: Blend creative text descriptions with image fidelity using ComfyUI Phantom nodes\n- **Identity Preservation**: ComfyUI Phantom maintains subject consistency across frames\n- **Multi-Subject Support**: Generate videos with multiple subjects from reference inputs using ComfyUI Phantom\n- **ComfyUI Integration**: Seamlessly integrates with existing ComfyUI workflows and custom nodes\n- **Ideal for Creators**: Perfect for VTubers, stylized character creators, and narrative video artists using ComfyUI Phantom\n\nWhether you're animating characters or generating reference-driven AI motion, *ComfyUI Phantom* gives you a flexible and powerful toolkit for visual storytelling within the ComfyUI ecosystem.\n\n## 1 - References in ComfyUI Phantom\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1232/readme02.webp\" alt=\"ComfyUI Phantom\" width=\"550\"/>\n</p>\n\nThe first section handles reference uploading for your *ComfyUI Phantom* setup:\n\nLoad your driving reference image here in the ComfyUI Phantom workflow. You can upload max 4 reference images in their respective group. By default 2 are enabled in ComfyUI Phantom, you can enable 2 more by unmuting them.\n\nYou should also enable them in the Image concate multi node to see the comparison update in the compare video output.\n\n## 2 - Resolution and Duration Settings for ComfyUI Phantom\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1232/readme03.webp\" alt=\"ComfyUI Phantom\" width=\"550\"/>\n</p>\n\nEnter your wan 2.1 compatible resolution and duration in frames in these ComfyUI Phantom nodes.\n\n## 3 - Prompts Configuration in ComfyUI Phantom\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1232/readme04.webp\" alt=\"ComfyUI Phantom\" width=\"550\"/>\n</p>\n\nEnter your prompts for **ComfyUI Phantom** video generation:\n- **Positive Prompt**: Describe what you want Phantom to generate which also matches the content of the uploaded reference image\n- **Negative Prompt**: Describe what Phantom should avoid (e.g., \"blurry, low quality, artifacts\")\n\n## 4 - KSampler & Output in ComfyUI Phantom\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1232/readme05.webp\" alt=\"ComfyUI Phantom\" width=\"550\"/>\n</p>\n\n- **Sampler Settings**: Choose sampler type (e.g., DPM++, Euler, etc.), steps, and seed for Phantom generation\n- **Output**: Generated image will appear in the Phantom output viewer and saved in the output folder\n- **Load ComfyUI Phantom Model**: In the Phantom Model selector node, you can choose between the 1.3B or the 14B ComfyUI Phantom model\n\nRendered video will be saved in the outputs folder in your ComfyUI installation.\n\n## ComfyUI Phantom Workflow Benefits\n\n**ComfyUI Phantom** provides several advantages for video generation:\n- **Node-Based Interface**: Leverage ComfyUI's intuitive node system for ComfyUI Phantom workflows\n- **Workflow Customization**: Modify and extend ComfyUI Phantom workflows to suit specific needs\n- **Parameter Control**: Fine-tune ComfyUI Phantom generation with precise parameter adjustments\n- **Batch Processing**: Process multiple reference images efficiently with ComfyUI Phantom\n- **Community Support**: Access shared ComfyUI Phantom workflows and community modifications\n\n### Acknowledgement\nComfyUI Phantom is built on top of the **Wan 2.1** video generation model using the **Wan Video Wrapper** node system in ComfyUI. The core nodes and architecture were developed by [kijai](https://github.com/kijai/ComfyUI-WanVideoWrapper), enabling reference-based, ID-preserving video synthesis within ComfyUI. This ComfyUI Phantom workflow would not be possible without the foundational work behind **Wan 2.1** and the custom ComfyUI tools that power it.\n\n## ComfyUI Phantom Model Information\n- **Source** - [Original Phantom Repo](https://github.com/Phantom-video/Phantom)\n- **ComfyUI Implementation**: https://huggingface.co/Kijai/WanVideo_comfy/tree/main\n- **Model Used in Workflow** : https://civitai.com/models/1651125?modelVersionId=1878555\n- **Architecture**: Multi-Input Reference for ComfyUI Phantom\n- **Model Location**: `comfyui/models/diffusion_models`\n- **ComfyUI Compatibility**: Fully integrated with ComfyUI workflow system\n"
    },
    {
        "id": "1233",
        "readme": "# Self Forcing: Autoregressive Keyframe-to-Video Generation\n\n**Self Forcing** is an advanced keyframe-driven video generation model. Self Forcing enables smooth, high-quality video synthesis by generating motion between a start and end keyframe, guided by descriptive text prompts.\n\nBuilt upon autoregressive video diffusion architectures with KV caching, *Self Forcing* excels at generating temporally consistent, identity-preserving motion across frames. The Self Forcing joint keyframe-text approach allows for fluid transitions, while maintaining subject structure and style throughout the generated video.\n\n## Why Use Self Forcing?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1233/readme01.webp\" alt=\"Self Forcing\" width=\"750\"/>\n</p>\n\n**Self Forcing** offers:\n- **Keyframe-Based Generation**: Self Forcing uses start and end reference images to control appearance and motion  \n- **Prompt + Keyframe Control**: Self Forcing blends creative text descriptions with reference structure  \n- **Autoregressive Motion**: Self Forcing provides smooth, temporally consistent transitions between frames  \n- **Identity Preservation**: Self Forcing maintains subject fidelity across generated sequences  \n- **Ideal for Streamlined Video Creation**: Self Forcing is perfect for character-driven storytelling, cinematic animation, and concept video synthesis\n\nWhether you're generating animations, cinematic sequences, or identity-consistent AI videos, *Self Forcing* gives you full creative control while ensuring smooth and realistic motion with Self Forcing technology.\n\n## Input Images\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1233/readme02.webp\" alt=\"Self Forcing\" width=\"550\"/>\n</p>\n\nIn this section, you will upload your **Start Keyframe** and **End Keyframe** images for Self Forcing. These two images define the beginning and ending appearance of your Self Forcing generated video.\n- Upload both reference images using the provided Load Image nodes for Self Forcing.\n- Use optional **Resizing and Cropping nodes** to adjust your images for optimal Self Forcing alignment and aspect ratio.\n- Properly aligned and well-cropped keyframes improve Self Forcing motion consistency throughout the generated sequence.\n\n## Video Duration\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1233/readme03.webp\" alt=\"Self Forcing\" width=\"550\"/>\n</p>\n\nSet the **total number of frames** your Self Forcing video will generate.\n- Longer frame counts allow for more gradual, fluid transitions between keyframes in Self Forcing.\n- Shorter frame counts result in quicker Self Forcing transitions.\n- Typical Self Forcing range: 16–48 frames depending on desired length and motion complexity.\n\n\n## Model\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1233/readme04.webp\" alt=\"Self Forcing\" width=\"550\"/>\n</p>\n\nThis group loads the **Self Forcing autoregressive video diffusion model**. The Self Forcing workflow automatically selects the correct model version for you.\n- Self Forcing is built on autoregressive rollout with KV caching.\n- Self Forcing ensures stable, temporally coherent motion generation.\n- Self Forcing allows real-time inference on high-end GPUs like RTX 4090.\n\n## Prompts\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1233/readme05.webp\" alt=\"Self Forcing\" width=\"550\"/>\n</p>\n\nIn this section, you can enter your **Text Prompt** to guide the Self Forcing generation.\n- Combine prompts with your keyframes to influence the Self Forcing style, background, or motion context.\n- Use descriptive and clear language to maximize Self Forcing creative control.\n- Negative prompts can also be used to suppress unwanted elements in Self Forcing.\n\n## Outputs\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1233/readme06.webp\" alt=\"Self Forcing\" width=\"550\"/>\n</p>\n\nOnce Self Forcing generation is complete:\n- Your Self Forcing video will be saved automatically in the `Comfyui > output` folder inside your ComfyUI directory.\n- Self Forcing files are stored as video clips (MP4 or image sequences depending on configuration).\n\n\n## Acknowledgement\n\nThis workflow uses the **Self Forcing** model developed by **guandeh**.  \nThe Self Forcing workflow integrates **Wan Video Wrapper** nodes by **kijai** to enable seamless Self Forcing video generation inside ComfyUI.  \nFull credit goes to both authors for their original Self Forcing model development and integration work.\n\nGitHub Repository: [https://github.com/guandeh17/Self-Forcing](https://github.com/guandeh17/Self-Forcing)\n"
    },
    {
        "id": "1234",
        "readme": "### 1. What is the ComfyUI PMRF Workflow?\nThe ComfyUI PMRF workflow integrates the revolutionary Posterior-Mean Rectified Flow algorithm into the ComfyUI environment for photo-realistic face restoration. Based on cutting-edge research from Technion—Israel Institute of Technology (ICLR 2025), ComfyUI PMRF addresses the fundamental challenge in image restoration: achieving minimal distortion while maintaining perfect perceptual quality. Unlike traditional methods that rely on posterior sampling or GAN-based approaches, ComfyUI PMRF approximates the mathematically optimal estimator that minimizes Mean Squared Error (MSE) under a perfect perceptual quality constraint.\n\n### 2. Benefits of ComfyUI PMRF:\n\n> **⚡ UNPRECEDENTED SPEED PERFORMANCE ⚡**\n> \n> This ComfyUI PMRF workflow is **ULTRA-FAST** - delivering results in mere seconds! At 1.29 seconds for 2x upscaling, ComfyUI PMRF is faster than ANY upscale workflow currently available on RunComfy platform. While other methods take minutes, ComfyUI PMRF completes face restoration in the time it takes to blink!\n> \n> *For other upscale workflows, please see the bottom of this page*\n\n* **Ultra Fast Processing:** ComfyUI PMRF achieves 2x face upscaling (512×682 to 1024×1364) in just 1.29 seconds on RTX 4090, compared to minutes required by traditional SD upscaling methods\n* **Low VRAM Requirements:** ComfyUI PMRF operates efficiently with only 3.3GB VRAM, significantly lower than competing solutions (DifFBIR requires 8GB, Topaz PhotoAI needs 20GB)\n* **Superior Detail Preservation:** ComfyUI PMRF's advanced posterior-mean rectified flow algorithm maintains natural facial features while eliminating blur and noise artifacts\n* **Memory Issue Fixed:** This ComfyUI PMRF version resolves the 1GB VRAM occupation bug present in the original PMRF release\n* **Mathematically Optimal:** ComfyUI PMRF provably approximates the theoretically optimal estimator for photo-realistic restoration tasks\n\n### 3. How to Use the ComfyUI PMRF Workflow\n\n#### 3.1 Generation Methods with ComfyUI PMRF\n**Example Setup for ComfyUI PMRF Face Restoration:**\n1. Prepare inputs:\n   In `Load Image` node: \n   - Upload your degraded/blurry face image\n   - Ensure image is properly aligned and cropped to focus on the face\n2. Configure ComfyUI PMRF node:\n   - Set `num_steps` (25 for speed, 100 for maximum quality)\n   - Set `scale` (2.0 for 2x upscaling, adjust as needed)\n3. Click `Queue Prompt` button to run the ComfyUI PMRF workflow\n4. In `Save Image`: get your enhanced face restoration output\n\n#### 3.2 Parameter Reference for ComfyUI PMRF\n**ComfyUI PMRF Core Node:**\nThis node performs the posterior-mean rectified flow restoration process.\n- `scale`: Upscaling factor for the output image (2.0 = 2x larger, 1.5 = 1.5x larger, etc.).\n- `num_steps`: Number of rectified flow iterations.\n- `seed`: Random seed for reproducible results.\n- `control_after_generate`: Determines seed behavior for batch processing (randomize/fixed).\n- `interpolation`: Resampling method used during upscaling process (lanczos4 recommended for best quality).\n\n#### 3.3 Advanced Optimization with ComfyUI PMRF\n\n**Understanding the Scale Parameter in ComfyUI PMRF:**\nThe `scale` parameter controls the upscaling factor - it's a multiplier for your image dimensions. To calculate the correct scale value for ComfyUI PMRF:\n\n**Scale Calculation Formula:**\n```\nscale = Target Resolution ÷ Input Resolution\n```\n\n**Practical Examples for ComfyUI PMRF:**\n- **For 4K output (3840×2160):** If your input is 1920×1080, use `scale: 2.0` (3840÷1920=2.0)\n- **For 4K output from 1280×720:** Use `scale: 3.0` (3840÷1280=3.0)  \n- **For 2K output (2560×1440) from 1280×720:** Use `scale: 2.0` (2560÷1280=2.0)\n- **For custom sizes:** Always divide your target width by input width to get the scale value\n\n> **💡 Pro Tip: Iterative Enhancement**\n> \n> For heavily degraded images where single-pass results aren't satisfactory, you can use **iterative processing**: Take the ComfyUI PMRF output and feed it back as input for another round of restoration. This multi-pass approach can achieve even better results for extremely challenging images.\n\n## Technical Background\n\n### How ComfyUI PMRF Works\nThink of image restoration like fixing a blurry photograph. Traditional methods either make the image too smooth (losing important details) or add weird artifacts when trying to make it look natural. ComfyUI PMRF solves this by using a two-step approach: first, it creates the \"best guess\" of what the clear image should look like, then it uses advanced mathematics to transport this guess to look perfectly natural - like the difference between a rough sketch and a finished painting.\n\n### The Science Behind ComfyUI PMRF\nComfyUI PMRF addresses the fundamental \"distortion-perception tradeoff\" in image restoration. The key insight is that the optimal way to restore images isn't to guess randomly (like most AI methods do), but to follow a mathematically proven path. ComfyUI PMRF first predicts the \"posterior mean\" (the statistically best guess), then uses \"rectified flow\" to optimally transport this prediction to the natural image distribution. This ensures both minimal error and maximum visual quality.\n\n## More Information about ComfyUI PMRF\nFor additional details and development references:\n- PMRF original research by [Technion—Israel Institute of Technology](https://github.com/ohayonguy/PMRF)\n- Paper: \"Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration\" (ICLR 2025)\n- Project Page: [https://pmrf-ml.github.io/](https://pmrf-ml.github.io/)\n- Online Demo: [https://huggingface.co/spaces/ohayonguy/PMRF](https://huggingface.co/spaces/ohayonguy/PMRF)\n\n### Acknowledgements\nThis ComfyUI PMRF workflow is powered by **PMRF (Posterior-Mean Rectified Flow)**, developed by **Guy Ohayon, Tomer Michaeli, and Michael Elad** from **Technion—Israel Institute of Technology**. The research was published at **ICLR 2025**.\n\nThe **ComfyUI PMRF integration** includes bug fixes for memory issues in the original implementation. Full credit goes to the original authors for their groundbreaking work in photo-realistic image restoration.\n"
    },
    {
        "id": "1235",
        "readme": "### 1. What is the ComfyUI Wan FusionX+NAG Workflow?\nThe ComfyUI Wan FusionX workflow represents a revolutionary breakthrough in video generation technology, combining the power of the legendary Wan FusionX model with Normalized Attention Guidance (NAG) to deliver cinema-grade video quality. This workflow integrates ByteDance's advanced video generation capabilities through a unified FusionX fusion model that consolidates multiple specialized models into one powerful solution.\n\nWan FusionX is not just another video model - it's a carefully crafted FusionX fusion that integrates the best capabilities from the WAN ecosystem:\n- **CausVid** - Causal motion modeling for superior scene flow and dramatic speed improvements\n- **AccVideo** - Enhanced temporal alignment and realism with fast generation\n- **MoviiGen1.1** - Cinema-quality smoothness and lighting effects\n- **MPS Reward LoRA** - Fine-tuned for motion dynamics and detail enhancement\n- **Custom LoRAs** - Optimized for texture clarity and fine details\n\nCombined with NAG (Normalized Attention Guidance), this Wan FusionX workflow provides unprecedented control over video generation through dual prompt guidance (positive and negative), ensuring outputs that precisely match your creative vision.\n\n### 2. Benefits of ComfyUI Wan FusionX+NAG\n* **Legendary Quality:** Wan FusionX represents the pinnacle of WAN model evolution, delivering cinema-grade video quality that surpasses traditional WAN 2.1 models\n* **6-Step Generation:** Achieve high-quality results in just 6 sampling steps, dramatically reducing generation time while maintaining superior quality\n* **Dual Prompt Control:** NAG enables precise control through both positive and negative prompts, ensuring outputs that exactly match your vision\n* **All-in-One Solution:** Replace multiple gigabytes of separate models with this single, comprehensive FusionX solution\n* **Enhanced Prompt Adherence:** Superior text-to-video alignment compared to standard WAN models\n* **Complete Workflow Suite:** Includes both Wan FusionX core functionality and integrated VACE 5-in-1 capabilities\n* **Memory Efficient:** Optimized for consumer hardware while delivering professional results\n\n### 3. Wan FusionX vs WAN 2.1: The Generational Leap\n\nWan FusionX represents a significant evolution beyond standard WAN 2.1 models:\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1235/readme01.webp\" alt=\"Wan FusionX\" width=\"750\"/>\n\n### 4. How to Use the ComfyUI Wan FusionX+NAG Workflow\nThis Wan FusionX workflow consists of two main groups:\n1. **Wan FusionX+NAG I2V/T2V** - Core Wan FusionX functionality with NAG enhancement\n2. **VACE Wan FusionX+NAG** - Integrated VACE 5-in-1 capabilities with FusionX\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1235/readme02.webp\" alt=\"Wan FusionX\" width=\"750\"/>\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1235/readme03.webp\" alt=\"Wan FusionX\" width=\"750\"/>\n\n#### 4.1 Wan FusionX+NAG Text-to-Video (T2V) Generation\n**Quick Start Steps for FusionX T2V:**\n1. Configure the `WanVideo Apply NAG` node with your prompts:\n   - **Positive Prompt:** Detailed description of your desired video scene\n   - **Negative Prompt:** Elements to avoid (Chinese prompts work best for optimal Wan FusionX results)\n2. Set key FusionX parameters in `WanVideo Sampler`:\n   - **CFG:** Must be set to 1.0 (critical for Wan FusionX)\n   - **Steps:** 6-10 steps recommended for FusionX\n   - **Shift:** Start with 1 for 1024x576, or 2 for 1080x720\n   - **Sampler:** Use uni_pc for optimal results\n3. Click \"Run\" to generate your FusionX video\n\n#### 4.2 Wan FusionX+NAG Image-to-Video (I2V) Generation\n**Quick Start Steps for FusionX I2V:**\n1. Upload your reference image in the `Load Image` node\n2. Configure the same NAG and FusionX sampling parameters as T2V\n3. Recommended FusionX settings for I2V:\n   - **Shift:** 2 for optimal motion\n   - **Frames:** 121 with 24 FPS for 50% speed boost\n   - **CFG:** Keep at 1.0 for Wan FusionX compatibility\n4. Enter descriptive prompts about desired motion and style\n5. Generate your FusionX image-to-video result\n\n#### 4.3 Key Parameter Guidelines for Wan FusionX\n**Critical FusionX Settings:**\n- **CFG:** MUST be 1.0 - higher values produce poor results with FusionX\n- **Shift:** Resolution-dependent (1 for 1024x576, 2 for 1080x720)\n- **Sampler:** uni_pc recommended for Wan FusionX\n- **Steps:** 6-10 for optimal FusionX speed/quality balance\n- **TeaCache:** Not recommended due to low step count\n\n**Advanced FusionX Parameters:**\n- **Lower Shift (1-2):** More realistic effects with Wan FusionX\n- **Higher Shift (3-9):** More stylized, artistic looks\n- **Frame Rate:** 24 FPS recommended for smooth motion\n\n### 5. VACE Integration with Wan FusionX\nFor the comprehensive VACE 5-in-1 functionality (reference-based subject replacement, dual-image video generation, first/last frame control, object modification, and video expansion), this workflow integrates the complete VACE system with Wan FusionX as the base model.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1235/readme04.webp\" alt=\"Wan FusionX\" width=\"650\"/>\n\n**Key Differences from Standard VACE:**\n- **Model Selection:** Use Wan FusionX variants in the VACE model selector\n- **Enhanced Quality:** Wan FusionX provides superior baseline quality for all VACE operations\n- **Enhanced Reference Image + ControlNet:** This FusionX workflow includes additional OpenPose control for more precise subject replacement guidance. The workflow features a switch system allowing you to choose between:\n  - **Depth Control:** Standard depth-based guidance for spatial understanding\n  - **OpenPose Control:** Advanced pose-based character guidance for accurate body positioning and movement\n  \n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1235/readme05.webp\" alt=\"Wan FusionX\" width=\"650\"/>\n\n\nFor detailed VACE usage instructions, refer to the comprehensive VACE 14B workflow documentation: [ComfyUI VACE 14B All-in-One Video Creation & Editing Workflow](https://www.runcomfy.com/comfyui-workflows/vace-14b-all-in-one-video-creation-editing-workflow-comfyui)\n\n\n### Acknowledgements\nThis workflow is powered by **Wan FusionX**, a FusionX fusion model created by **vrgamedevgirl84** that integrates CausVid, AccVideo, MoviiGen1.1, and MPS Reward LoRA technologies. The **NAG (Normalized Attention Guidance)** implementation is provided by **Kijai** through the WanVideoWrapper nodes. The **VACE integration** utilizes the comprehensive video editing framework developed by **Tongyi Lab at Alibaba Group**. \n\nSpecial recognition goes to the research teams behind **CausVid**, **AccVideo**, **MoviiGen**, and **MPS LoRA** for their foundational contributions that make this unified FusionX video generation solution possible.\n\n**Model Sources:**\n- Wan FusionX Models: [Hugging Face - vrgamedevgirl84](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX)\n- ComfyUI Integration: [Kijai/ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)\n- VACE Framework: [Original Research Paper](https://arxiv.org/abs/2503.07598)\n"
    },
    {
        "id": "1236",
        "readme": "# DreamO: Unified Multi-Task Image Customization Framework\n\n**DreamO** is a powerful reference-driven image generation workflow that supports a variety of visual customization tasks, including identity preservation, style transfer, virtual try-on, and multi-condition blending.\n\nBuilt upon a unified architecture, **DreamO** enables flexible image synthesis from one to three input references, guided by natural language prompts. Whether you're working with characters, garments, objects, or facial identities, **DreamO** delivers high-fidelity outputs while preserving structure and stylistic consistency across tasks.\n\n## Why Use DreamO?\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1236/readme01.webp\" alt=\"DreamO\" width=\"750\"/>\n</p>\n\nThe **DreamO** framework offers:\n- **Multi-Task Support**: **DreamO** supports IP, ID, Try-On, Style, or Multi-Condition modes  \n- **1–3 Reference Images**: **DreamO** uses one, two, or three input images to guide generation  \n- **Task-Specific Control**: **DreamO** applies targeted customization like facial identity, clothes, or appearance style  \n- **Text + Image Fusion**: **DreamO** combines visual references with prompt-driven creativity  \n- **Perfect for Creators**: **DreamO** is ideal for character design, virtual try-on demos, product prototyping, and more\n\n**DreamO** empowers artists, developers, and designers with a comprehensive toolkit for controlled image synthesis—flexible, modular, and creatively scalable.\n\n## DreamO Models\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1236/readme02.webp\" alt=\"DreamO\" width=\"550\"/>\n</p>\n\nThis group loads all the required **DreamO** models automatically when you first run the **DreamO** workflow. No manual setup is required.\nThe **DreamO** models are hosted on the official repository:  \n[https://github.com/ToTheBeginning/ComfyUI-DreamO](https://github.com/ToTheBeginning/ComfyUI-DreamO)\n\n## DreamO Input Images\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1236/readme03.webp\" alt=\"DreamO\" width=\"550\"/>\n</p>\n\nIn this section, you upload **1 to 3 reference images** and select the **task** you want **DreamO** to perform:\n- IP (Identity + Pose)\n- ID (Facial Identity only)\n- Style\n\nYou can also set the **render resolution** to define the **DreamO** output image size. Adjust this based on your desired quality or use-case. Proper image resolution improves alignment and preserves fine details in **DreamO** output.\n\n## DreamO Prompts\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1236/readme04.webp\" alt=\"DreamO\" width=\"550\"/>\n</p>\n\nEnter your **main text prompt** here to control the appearance, context, or style of the **DreamO** output image:\n- Write detailed, expressive prompts to guide the **DreamO** generation direction  \n- You can combine visual references and language to shape clothing, accessories, emotion, or setting in **DreamO**  \n- Use **negative prompts** to avoid unwanted features or artifacts in **DreamO** outputs\n\nPrompt guidance is essential to achieving fine-grained results in all **DreamO** tasks.\n\n## DreamO KSampler + Output\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1236/readme05.webp\" alt=\"DreamO\" width=\"550\"/>\n</p>\n\nIn this **DreamO** section, you can:\n- Configure **sampling parameters** like steps, sampler type (Euler, DPM++), and guidance scale for **DreamO**  \n- Trigger the **DreamO** generation process  \n- View and compare the **DreamO** output results\n\nAll **DreamO** generated images are automatically saved in your `ComfyUI > output` folder.\n\n**DreamO** renders a **side-by-side comparison** showing how the reference and prompts influence the final result—helping you iterate quickly and creatively with **DreamO**.\n\n## Acknowledgement\n\nThis workflow uses the **DreamO** model developed by **ToTheBeginning**.  \nAll credits go to them for building this unified, multi-task image customization architecture and enabling seamless multi-input workflows in ComfyUI with **DreamO**.  \n**DreamO** GitHub Repository: [https://github.com/ToTheBeginning/ComfyUI-DreamO](https://github.com/ToTheBeginning/ComfyUI-DreamO)\n\n**DreamO** brings together years of research into identity, structure, and style preservation across tasks like IP-Adapter, InstantID, InstantStyle, and Try-On generation. **DreamO** is a versatile and production-ready toolkit for creators looking to combine reference-driven control with deep model fidelity—all inside ComfyUI with **DreamO**.\n"
    },
    {
        "id": "1237",
        "readme": "> **⚠️ Important Note: This ComfyUI MultiTalk implementation currently supports SINGLE-PERSON generation only. Multi-person conversational features will be coming soon.**\n\n### 1. What is MultiTalk?\n\nMultiTalk is a revolutionary framework for audio-driven multi-person conversational video generation developed by MeiGen-AI. Unlike traditional talking head generation methods that only animate facial movements, MultiTalk technology can generate realistic videos of people speaking, singing, and interacting while maintaining perfect lip synchronization with audio input. MultiTalk transforms static photos into dynamic speaking videos by making the person speak or sing exactly what you want them to say.\n\n### 2. How MultiTalk Works\n\nMultiTalk leverages advanced AI technology to understand both audio signals and visual information. The ComfyUI MultiTalk implementation combines **MultiTalk + Wan2.1 + Uni3C** for optimal results:\n\n**Audio Analysis:** MultiTalk uses a powerful audio encoder (Wav2Vec) to understand the nuances of speech, including rhythm, tone, and pronunciation patterns.\n\n**Visual Understanding:** Built on the robust Wan2.1 video diffusion model (you can visit our [Wan2.1 workflow](https://www.runcomfy.com/comfyui-workflows/wan-2-1-workflow-in-comfyui-text-image-to-video-generation) for t2v/i2v eneration), MultiTalk understands human anatomy, facial expressions, and body movements.\n\n**Camera Control:** MultiTalk with Uni3C controlnet enables subtle camera movements and scene control, making the video more dynamic and professional-looking. Check out our [Uni3C workflow](https://www.runcomfy.com/comfyui-workflows/uni3c-comfyui-workflow-video-referenced-camera-motion-transfer) for creating beautiful camera motion transfer.\n\n**Perfect Synchronization:** Through sophisticated attention mechanisms, MultiTalk learns to perfectly align lip movements with audio while maintaining natural facial expressions and body language.\n\n**Instruction Following:** Unlike simpler methods, MultiTalk can follow text prompts to control the scene, pose, and overall behavior while maintaining audio synchronization.\n\n### 3. Benefits of ComfyUI MultiTalk\n\n* **High-Quality Lip Sync:** MultiTalk achieves millisecond-level precision in lip synchronization, especially impressive for singing scenarios\n* **Versatile Content Creation:** MultiTalk supports both speaking and singing generation with various character types including cartoon characters\n* **Flexible Resolution:** MultiTalk generates videos in 480P or 720P at arbitrary aspect ratios\n* **Long Video Support:** MultiTalk creates videos up to 15 seconds in length\n* **Instruction Following:** MultiTalk controls character actions and scene settings through text prompts\n\n### 4. How to Use the ComfyUI MultiTalk Workflow\n\n#### Step-by-Step MultiTalk Usage Guide\n\n**Step 1: Prepare Your MultiTalk Inputs**\n1. **Upload Reference Image:** Click \"choose file to upload\" in Load Image node\n   - Use clear, front-facing photos for best MultiTalk results\n   - Image will be automatically resized to optimal dimensions (832px recommended)\n2. **Upload Audio File:** Click \"choose file to upload\" in LoadAudio node\n   - MultiTalk supports various audio formats (WAV, MP3, etc.)\n   - Clear speech/singing works best with MultiTalk\n   - For creating custom songs, consider using our [Ace-Step music generation workflow](https://www.runcomfy.com/comfyui-workflows/ace-step-music-generation-model-in-comfyui-ai-audio-creation), which produces high-quality music with synchronized lyrics.\n3. **Write Text Prompt:** Describe your desired scene in the text encode nodes for MultiTalk generation\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1237/readme01.webp\" alt=\"MultiTalk\" width=\"650\"/>\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1237/readme02.webp\" alt=\"MultiTalk\" width=\"650\"/>\n\n**Step 2: Configure MultiTalk Generation Settings**\n1. **Sampling Steps:** 20-40 steps (higher = better MultiTalk quality, slower generation)\n2. **Audio Scale:** Keep at 1.0 for optimal MultiTalk lip sync\n3. **Embed Cond Scale:** 2.0 for balanced MultiTalk audio conditioning\n4. **Camera Control:** Enable Uni3C for subtle movements, or disable for static MultiTalk shots\n\n**Step 3: Optional MultiTalk Enhancements**\n1. **LoRA Acceleration:** Enable for faster MultiTalk generation with minimal quality loss\n2. **Video Enhancement:** Use enhance nodes for MultiTalk post-processing improvements\n3. **Negative Prompts:** Add unwanted elements to avoid in MultiTalk output (blurry, distorted, etc.)\n\n**Step 4: Generate with MultiTalk**\n1. Queue the prompt and wait for MultiTalk generation\n2. Monitor VRAM usage (48GB recommended for MultiTalk)\n3. MultiTalk generation time: 7-15 minutes depending on settings and hardware\n\n\n### 5. Acknowledgements\n\n**Original Research:** MultiTalk is developed by **MeiGen-AI** with collaboration from leading researchers in the field. The original paper \"Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation\" presents the groundbreaking research behind this technology.\n\n**ComfyUI Integration:** The ComfyUI implementation is provided by **Kijai** through the ComfyUI-WanVideoWrapper repository, making this advanced technology accessible to the broader creative community.\n\n**Base Technology:** Built upon the Wan2.1 video diffusion model and incorporates audio processing techniques from Wav2Vec, representing a synthesis of cutting-edge AI research.\n\n### 6. Links and Resources\n\n* **Original Research:** [MeiGen-AI MultiTalk Repository](https://github.com/MeiGen-AI/MultiTalk)\n* **Project Page:** [https://meigen-ai.github.io/multi-talk/](https://meigen-ai.github.io/multi-talk/)\n* **ComfyUI Integration:** [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/multitalk)\n"
    },
    {
        "id": "1238",
        "readme": "## What is SeedVR2?\n\nSeedVR2 is a revolutionary one-step video restoration model developed by ByteDance Seed, built on a Diffusion Transformer architecture. Unlike traditional diffusion models that require dozens of inference steps, SeedVR2 achieves **single-step generation**, delivering over 4x faster video restoration while maintaining or even exceeding the quality of multi-step models.\n\nThink of SeedVR2 as a super-intelligent \"video repair specialist\" that can instantly transform blurry, low-resolution videos and images into crisp, high-quality results with just one pass—no need for the repeated \"polishing\" that other tools require.\n\n## Benefits of ComfyUI SeedVR2\n\n* **Lightning-Fast Processing:** SeedVR2 achieves one-step restoration, eliminating the need for time-consuming multi-step inference\n* **Exceptional Enhancement Quality:** SeedVR2 delivers stunning results for both image and video restoration tasks\n* **Dual-Mode Capability:** SeedVR2 handles both image and video restoration within a single unified workflow\n* **Adaptive Resolution Support:** SeedVR2 processes arbitrary input sizes without resolution constraints\n* **Superior Detail Recovery:** SeedVR2 excels at recovering fine details like text, facial features, and architectural elements\n* **Memory Efficient:** The SeedVR2 3B model provides excellent quality-to-performance ratio\n\n## How to Use ComfyUI SeedVR2 Workflow\n\n### Understanding the SeedVR2 Dual Workflow Structure\n\nThe SeedVR2 workflow consists of **two distinct processing paths**:\n\n1. **Upper Path - SeedVR2 Image Enhancement Workflow**\n2. **Lower Path - SeedVR2 Video Enhancement Workflow**\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1238/readme01.webp\" alt=\"SeedVR2\" width=\"650\"/>\n\nBoth SeedVR2 workflows share the same core SeedVR2 technology and can deliver impressive restoration results across different media types.\n\n### SeedVR2 Image Enhancement Workflow (Upper Path)\n\n**SeedVR2 Setup Process:**\n1. **Load Image Node:** Upload your blurry or low-resolution image for SeedVR2 processing\n2. **SeedVR2 Video Upscaler Node:** Configure your SeedVR2 enhancement settings\n   - `new_width`: Set target width\n   - `cfg_scale`: Use 1.01 for subtle SeedVR2 enhancement or higher values for more dramatic changes\n   - `batch_size`: Keep at 1 for single image SeedVR2 processing\n3. **LayerUtility Nodes:** The workflow includes comparison tools to evaluate SeedVR2 before/after results\n4. **Save Image:** Export your SeedVR2 enhanced result\n\n### SeedVR2 Video Enhancement Workflow (Lower Path)\n\n**SeedVR2 Video Setup Process:**\n1. **Load Video Node:** Upload your video file for SeedVR2 processing\n   - Set appropriate `force_rate`, `custom_width`, and `custom_height` for SeedVR2\n   - Configure `frame_load_cap` to limit SeedVR2 processing for testing\n   - Use `select_every_nth` to skip frames if needed for faster SeedVR2 processing\n2. **SeedVR2 Video Upscaler:** Use identical SeedVR2 settings as image workflow\n3. **Video Combine Node:** Compile SeedVR2 processed frames back into video format\n   - `frame_rate`: Match your input video's framerate for SeedVR2 output\n   - `format`: Choose appropriate output format (mp4 recommended for SeedVR2)\n   - `crf`: Control compression quality (19 is a good balance for SeedVR2)\n\n### SeedVR2 Parameter Optimization Guide\n\n**Essential SeedVR2 Settings for Best Results:**\n\n**SeedVR2 Model Selection:**\n- **seedvr2_ema_3b_fp16.safetensors**: Best overall SeedVR2 choice (requires 18GB+ VRAM)\n- **seedvr2_ema_3b_fp8_e4m3fn.safetensors**: Lower memory SeedVR2 alternative with slightly reduced quality\n- **seedvr2_ema_7b_fp16.safetensors**: Larger SeedVR2 model for potentially better quality (higher VRAM requirements)\n- **seedvr2_ema_7b_fp8_e4m3fn.safetensors**: SeedVR2 7B model with reduced precision\n- Currently avoid SeedVR2 7B models due to reported vertical stripe artifacts\n\n**SeedVR2 Quality Controls:**\n- `cfg_scale`: 1.01-1.5 for subtle SeedVR2 enhancement, 2.0-3.0 for aggressive SeedVR2 restoration\n- `seed`: Experiment with different values—each produces unique SeedVR2 detail reconstruction\n- `preserve_vram`: Always enable unless you have abundant GPU memory for SeedVR2\n\n## More Information about SeedVR2\n\nFor additional SeedVR2 technical details and research background:\n- **Original SeedVR2 research** by [ByteDance Seed Team](https://github.com/ByteDance-Seed/SeedVR)\n- **ComfyUI SeedVR2 implementation** by [NumZ](https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler)\n- **SeedVR2 project homepage** with demos and comparisons: [SeedVR2 Website](https://iceclear.github.io/projects/seedvr2/)\n\n### Acknowledgements\n\nThis workflow is powered by **SeedVR2**, developed by the **ByteDance Seed research team**. The ComfyUI SeedVR2 integration is provided by **NumZ**, enabling seamless one-step video and image restoration within the ComfyUI ecosystem. Special recognition goes to the original authors for their groundbreaking work in making high-quality SeedVR2 diffusion-based restoration both fast and accessible.\n"
    },
    {
        "id": "1239",
        "readme": "## Important Note - 7/4/2025 Update\n\nWe've updated the FLUX Kontext Dev workflow with **two new group functionalities** that significantly expand the editing capabilities:\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1239/readme02.webp\" alt=\"Flux Kontext Dev\" width=\"750\"/>\n\n### New Group 1: \"Edit specified area\" - Precise Region Editing\nThis group enables **targeted editing of specific areas** in your image using visual annotation. Perfect for making precise modifications to particular regions without affecting the rest of the image.\n\n**How to use:**\n1. **Upload and Annotate:** Click on \"IMG\" to upload your image\n2. **Create Selection:** Select the fourth rectangular shape button in the \"Shapes\" toolbar \n3. **Mark Area:** Click \"Stroke\" to choose your annotation color and draw rectangles around areas you want to edit\n4. **Generate:** Use your text prompt to describe what changes you want in the marked areas\n\nThis workflow is ideal for making surgical edits - like changing the color of a specific object, adding elements to particular locations, or modifying facial features while keeping everything else untouched.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1239/readme03.webp\" alt=\"Flux Kontext Dev\" width=\"750\"/>\n\n\n### New Group 2: \"Creative Scribble-to-Image Editing \" - Scribble-to-Image Editing  \nThis group combines **reference image input with freehand scribble guidance** to create more intuitive and creative editing workflows.\n\n**How to use:**\n1. **Upload Reference Image:** Use the `Load Image` node to upload your original image that you want to edit/transform\n2. **Create Scribble Guide:** Click \"Edit\" to jump to the painting page\n3. **Set Canvas:** Click \"New image\" in the upper right corner and set your canvas size to match your reference image\n4. **Draw Your Vision:** Start drawing/scribbling your guide marks using different colors to indicate different elements or changes you want to make\n   - Use different colors to represent different objects, areas, or modifications\n   - The scribble acts as a visual guide to tell FLUX Kontext Dev where and what to change\n5. **Confirm Changes:** After completing your scribble artwork, click the dark green circular button on the outer side of the upper right corner of the canvas to highlight it\n6. **Verify Update:** Switch back to the ComfyUI page to check if the scribble image has been updated successfully - if updated, you can close the painting page\n7. **Generate:** Run the workflow with your text prompt describing how the scribble should guide the transformation\n\nThis workflow excels at intuitive edits where you can literally \"paint\" your ideas onto a canvas to guide the AI, offering more organic and creative control than rigid rectangular selections.\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1239/readme04.webp\" alt=\"Flux Kontext Dev\" width=\"750\"/>\n\nBoth groups work seamlessly with the existing FLUX Kontext Dev workflow and can be used independently or combined for complex multi-stage editing projects.\n\n\n### 1. What is FLUX Kontext Dev?\n\nFLUX Kontext Dev is a breakthrough 12-billion parameter multimodal image editing model developed by Black Forest Labs. Built on the rectified flow transformer architecture, this open-source FLUX Kontext Dev model supports simultaneous text and image input, intelligently understanding image context and performing precise editing operations. Unlike proprietary editing tools, FLUX Kontext Dev runs entirely on local machines, providing researchers, developers, and creators with unprecedented flexibility for AI-powered image editing through FLUX Kontext Dev technology.\n\nThink of FLUX Kontext Dev as having a professional photo editor that understands both what you're showing it and what you're telling it to do. Just like how you might tell a human editor \"transform this kitchen into a cozy library but keep the same lighting mood,\" FLUX Kontext Dev can understand complex editing instructions while preserving the essential characteristics of your image through advanced FLUX Kontext Dev processing.\n\n### 2. How FLUX Kontext Dev Works\n\nFLUX Kontext Dev uses a sophisticated process called flow matching combined with guidance distillation. In simple terms, imagine FLUX Kontext Dev as an artist who can see your original image and read your editing instructions simultaneously. The FLUX Kontext Dev model first understands the content, composition, and style of your input image, then gradually transforms it according to your text prompt while maintaining crucial elements like character features, object relationships, and overall composition through FLUX Kontext Dev algorithms.\n\nThe \"flow matching\" technique in FLUX Kontext Dev allows the model to create a smooth transformation path from your original image to the edited result, ensuring that changes feel natural and coherent. The guidance distillation training makes FLUX Kontext Dev more efficient, requiring fewer computational steps to achieve high-quality results compared to traditional diffusion models, making FLUX Kontext Dev the ideal choice for professional image editing workflows.\n\n### 3. Key features and Benefits of FLUX Kontext Dev\n\n**Character Consistency with FLUX Kontext Dev:** FLUX Kontext Dev excels at preserving unique elements across multiple scenes and environments. Whether you're editing a person's appearance or modifying objects, FLUX Kontext Dev maintains recognizable features and characteristics throughout the editing process.\n\n**Precise Local and Global Editing:** The FLUX Kontext Dev model can make targeted modifications to specific parts of an image without affecting other areas, or apply comprehensive changes across the entire image when needed through FLUX Kontext Dev's advanced processing capabilities.\n\n**Style Transfer and Reference:** Generate novel scenes while preserving the unique style of reference images using FLUX Kontext Dev, allowing for consistent aesthetic transformations through FLUX Kontext Dev's style understanding.\n\n**Iterative Editing Capability:** FLUX Kontext Dev's robust consistency allows users to refine images through multiple successive edits with minimal visual drift, enabling complex multi-step editing workflows powered by FLUX Kontext Dev technology.\n\n**Interactive Speed:** FLUX Kontext Dev is optimized for minimal latency in both image generation and editing, making FLUX Kontext Dev suitable for real-time creative workflows.\n\n**Open-Source Flexibility:** Unlike proprietary solutions, FLUX Kontext Dev provides complete local control, enabling custom integrations and modifications through the FLUX Kontext Dev framework.\n\n### 4. How to Use FLUX Kontext Dev in ComfyUI\n\n#### 4.1 Basic FLUX Kontext Dev Workflow Setup\n\n**Step 1 - Upload Images for FLUX Kontext Dev:**\n1. **Image Input Options for FLUX Kontext Dev:**\n   * **Single Image:** Use `Load Image (from Outputs)` for basic FLUX Kontext Dev editing\n   * **Multiple Images:** Use `Image Stitch` node to combine images, then connect to the FLUX Kontext Dev workflow\n2. **About FLUX Kontext Dev Edit:** The FLUX Kontext Dev workflow includes helpful notes - make sure to read them for optimal FLUX Kontext Dev guidance\n3. **Image Scaling:** The `FluxKontextImageScale` node automatically handles image preprocessing for FLUX Kontext Dev\n\n**Step 2 - FLUX Kontext Dev Prompt Configuration:**\n1. **CLIP Text Encode (Positive Prompt) for FLUX Kontext Dev:** Write your editing instruction in English\n   * Example: \"Transform the room into a cyberpunk nightclub with neon lighting effects and holographic displays\"\n2. **Preview Image:** Monitor your input image in the FLUX Kontext Dev preview section\n3. **Reference Settings:** The FLUX Kontext Dev workflow includes various notes for parameter guidance - please read them carefully for optimal FLUX Kontext Dev results\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1239/readme01.webp\" alt=\"Flux Kontext Dev\" width=\"750\"/>\n\n**Step 3 - Generate with FLUX Kontext Dev:**\n1. **FLUX Kontext Dev Sampling Settings:** Adjust KSampler parameters:\n   * `steps`: 20 (default for FLUX Kontext Dev)\n   * `cfg`: 1.0 (guidance scale for FLUX Kontext Dev)\n   * `sampler_name`: euler (recommended for FLUX Kontext Dev)\n   * `denoise`: 1.00 (FLUX Kontext Dev processing strength)\n2. **Execute FLUX Kontext Dev:** Click `Run`\n3. **Save FLUX Kontext Dev Results:** View and save your edited image in the `Save Image` node\n\n**Important Note:** The FLUX Kontext Dev workflow contains helpful notes and guidance throughout - make sure to read these FLUX Kontext Dev notes as you work through the process for optimal results.\n\n#### 4.2 FLUX Kontext Dev Editing Techniques and Applications\n\n**Basic Object Modifications with FLUX Kontext Dev**\n* **Color Changes:** \"Transform the bicycle to metallic silver finish\" using FLUX Kontext Dev\n* **Clothing Updates:** \"Change the jacket to a leather bomber style\" with FLUX Kontext Dev processing\n* **Object Replacement:** \"Replace the laptop with an antique typewriter\" through FLUX Kontext Dev\n\n**Style Transfer and Artistic Transformation using FLUX Kontext Dev**\n* **Art Style Conversion:** \"Convert to impressionist painting style with soft brushstrokes and pastel colors\" via FLUX Kontext Dev\n* **Historical Periods:** \"Transform to medieval manuscript illumination style\" with FLUX Kontext Dev\n* **Artistic Mediums:** \"Convert to watercolor painting with flowing pigment effects and paper texture\" using FLUX Kontext Dev\n\n**Environmental and Background Changes with FLUX Kontext Dev**\n* **Scene Modification:** \"Transform the setting to a futuristic space station while maintaining the subject's pose\" through FLUX Kontext Dev\n* **Lighting Adjustments:** \"Change to dramatic evening scene with warm candlelight ambiance\" using FLUX Kontext Dev\n* **Weather Effects:** \"Add heavy snowfall and winter atmosphere while preserving the original composition\" via FLUX Kontext Dev\n\n**Character and Portrait Editing with FLUX Kontext Dev**\n* **Age Transformation:** \"Make the person appear 15 years younger while keeping their distinctive features\" using FLUX Kontext Dev\n* **Expression Changes:** \"Modify the expression to show surprise and wonder\" with FLUX Kontext Dev\n* **Hairstyle Modifications:** \"Change to long braided hair with decorative ribbons\" through FLUX Kontext Dev\n\n**Text and Graphics Integration using FLUX Kontext Dev**\n* **Adding Text:** \"Insert bold graffiti-style text 'REBEL' on the brick wall\" with FLUX Kontext Dev\n* **Logo Placement:** \"Add a retro diner logo to the window while maintaining glass reflections\" using FLUX Kontext Dev\n* **Sign Modifications:** \"Change 'ENTRANCE' to 'EXIT' on the doorway sign\" through FLUX Kontext Dev\n\n**Advanced FLUX Kontext Dev Editing Scenarios**\n* **Multi-Image Combination:** Use the `Image Stitch` node to combine multiple images before FLUX Kontext Dev editing\n* **Iterative Refinement:** Perform multiple successive edits using the `Load Image(from output)` node with FLUX Kontext Dev\n* **Perspective Changes:** \"Shift the viewpoint to bird's eye view from directly above\" using FLUX Kontext Dev\n\n#### 4.3 FLUX Kontext Dev Parameter Control and Optimization\n\n**Guidance Scale for FLUX Kontext Dev:** Controls the strength of text prompt adherence in FLUX Kontext Dev\n* Higher values (3.0-5.0): Stronger adherence to prompt, more dramatic changes with FLUX Kontext Dev\n* Lower values (1.5-2.5): Subtler changes, better preservation of original image using FLUX Kontext Dev\n\n**FLUX Kontext Dev Sampling Settings:**\n* **Steps:** Typically 20-50 steps for high quality FLUX Kontext Dev results\n* **Denoise:** Controls how much of the original image is preserved in FLUX Kontext Dev (0.3-0.8 for editing)\n\n**FLUX Kontext Dev Model Variants:**\n* **Standard FLUX Kontext Dev:** Balanced performance for most editing tasks\n* **FP8/FP4 Variants:** Optimized FLUX Kontext Dev for speed and memory efficiency on compatible hardware\n\n### 5. Prompt Engineering for Optimal FLUX Kontext Dev Results\n\n#### 5.1 Best Practices for FLUX Kontext Dev\n\n**Be Specific and Clear with FLUX Kontext Dev**\n* Wrong: \"Make it more interesting\"\n* Correct: \"Transform the garden into a magical fairy tale setting while keeping the same flower arrangements\" using FLUX Kontext Dev\n\n**Use Preservation Language with FLUX Kontext Dev**\n* Include phrases like \"while maintaining,\" \"preserving,\" or \"keeping the same\" to specify what should remain unchanged in FLUX Kontext Dev\n* Example: \"Convert to art deco style while maintaining the original subject positioning\" with FLUX Kontext Dev\n\n**Break Complex Edits into Steps for FLUX Kontext Dev**\n* Instead of \"Turn the person into a space explorer on Mars,\" use multiple FLUX Kontext Dev steps:\n  1. \"Change the outfit to futuristic space gear while preserving body proportions\" using FLUX Kontext Dev\n  2. \"Transform the background to Martian landscape with red terrain\" with FLUX Kontext Dev\n\n#### 5.2 Common FLUX Kontext Dev Editing Templates\n\n**Object Modification Template for FLUX Kontext Dev:**\n\"Transform [specific object] into [new object/state], while keeping [elements to preserve] identical\" using FLUX Kontext Dev\n\n**Style Transfer Template for FLUX Kontext Dev:**\n\"Convert to [specific style with details], while maintaining [composition/character/lighting] exactly as original\" with FLUX Kontext Dev\n\n**Background Replacement Template for FLUX Kontext Dev:**\n\"Replace the background with [detailed description], keep the main subject in identical position and stance\" using FLUX Kontext Dev\n\n**Character Editing Template for FLUX Kontext Dev:**\n\"Alter [specific feature] to [new state], while preserving [other facial features/expression/pose] unchanged\" with FLUX Kontext Dev\n\n### 6. Advanced FLUX Kontext Dev Workflow Techniques\n\n**Multi-Round Iterative Editing with FLUX Kontext Dev**\nUse the `Load Image(from output)` node to chain multiple FLUX Kontext Dev editing operations:\n1. First FLUX Kontext Dev edit: Initial style transformation\n2. Second FLUX Kontext Dev edit: Adjust specific details\n3. Third FLUX Kontext Dev edit: Modify or enhance elements\n4. Final FLUX Kontext Dev edit: Perfect color and lighting balance\n\n**Multi-Image Processing with FLUX Kontext Dev**\nThe FLUX Kontext Dev workflow supports multiple image input methods:\n* **Image Stitch Node for FLUX Kontext Dev:** Combine multiple images using the `Image Stitch` node before FLUX Kontext Dev editing\n  * Adjust `direction` (right/left/up/down) to control how images are combined for FLUX Kontext Dev\n  * Set `match_image_size` to `true` for consistent sizing in FLUX Kontext Dev\n  * Configure `spacing_width` and `spacing_color` for image separation in FLUX Kontext Dev\n* **Load Image (from Outputs) for FLUX Kontext Dev:** Use this node for iterative FLUX Kontext Dev editing and accessing previously generated results\n* **About FLUX Kontext Dev Edit:** Pay attention to the FLUX Kontext Dev workflow notes that provide helpful guidance for optimal results\n\n**FLUX Kontext Dev Workflow Notes and Guidance**\nThe ComfyUI FLUX Kontext Dev workflow includes built-in notes and instructions throughout the node graph. These FLUX Kontext Dev notes contain valuable tips for:\n* FLUX Kontext Dev parameter optimization suggestions\n* Common FLUX Kontext Dev troubleshooting solutions\n* Best practices for specific FLUX Kontext Dev editing tasks\n* FLUX Kontext Dev model loading requirements and compatibility\n\n**Important:** Always read the FLUX Kontext Dev workflow notes as you progress through each step - they provide context-specific guidance that can significantly improve your FLUX Kontext Dev editing results.\n\n**Batch Processing Considerations for FLUX Kontext Dev**\n* Use consistent prompting templates for similar edits across multiple images with FLUX Kontext Dev\n* Maintain similar guidance scale settings for FLUX Kontext Dev consistency\n* Consider using the group node workflow for streamlined FLUX Kontext Dev batch operations\n\n\n## More Information about FLUX Kontext Dev\n\nFor additional technical details and FLUX Kontext Dev development resources:\n* **Original FLUX Kontext Dev Model Repository:** [Black Forest Labs FLUX](https://github.com/black-forest-labs/flux)\n* **FLUX Kontext Dev Model Weights:** [HuggingFace Repository](https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev)\n* **FLUX Kontext Dev Technical Paper:** [arXiv Technical Report](https://arxiv.org/abs/2506.15742)\n\n### Acknowledgements\n\nThis workflow is powered by **FLUX Kontext Dev**, developed by **Black Forest Labs**. The FLUX Kontext Dev model represents a significant advancement in open-source image editing technology, providing researchers and creators with professional-grade tools for AI-powered image manipulation through FLUX Kontext Dev. Full credit goes to the Black Forest Labs team for their innovative work in developing this breakthrough multimodal editing model, FLUX Kontext Dev.\n"
    },
    {
        "id": "1240",
        "readme": "## What is Hunyuan3D-2.1?\n\nHunyuan3D-2.1 is Tencent's breakthrough 3D asset generation system that turns single images into production-ready 3D models with physically-based rendering (PBR) materials. Building on the strong foundation of Hunyuan3D 2.0, this latest Hunyuan3D-2.1 version delivers significantly improved texture quality with finer surface details and enhanced three-dimensional depth perception.\n\nWhile Hunyuan3D 2.0 already impressed with solid geometry generation, Hunyuan3D-2.1 takes it to the next level with more nuanced material textures and better spatial understanding. The Hunyuan3D-2.1 system creates proper PBR materials with separate albedo, metallic, and roughness maps, making your generated objects respond to lighting realistically - metals shine convincingly, fabrics absorb light naturally, and surfaces have that perfect tactile quality.\n\nFor those familiar with the previous version, you can check out the [Hunyuan3D 2.0 workflow](https://www.runcomfy.com/comfyui-workflows/hunyuan3d-2-workflow-in-comfyui-create-3d-assets-from-images) to see how far Hunyuan3D-2.1 has come.\n\n## Benefits of Hunyuan3D-2.1\n\n* **Enhanced Texture Quality**: Significant improvement over 2.0 with finer surface details and more realistic material representation in Hunyuan3D-2.1\n* **Superior 3D Depth**: Better spatial understanding creates more convincing three-dimensional forms with Hunyuan3D-2.1\n* **True PBR Materials**: Hunyuan3D-2.1 generates albedo, metallic, and roughness maps for photorealistic rendering that actually works in professional pipelines\n* **Production-Ready Output**: Creates watertight meshes and proper UV mapping that you can immediately use in games, films, or design projects\n* **Incredible Detail Capture**: Hunyuan3D-2.1 preserves fine features like individual calculator buttons, fabric textures, and mechanical parts\n* **Zero Texture Seams**: 3D-aware positioning ensures seamless textures from every viewing angle\n\n## Quick Use Guide\n\n### Hunyuan3D-2.1 Basic Workflow\n\n1. **Prepare your image**: Ensure clean background and clear subject - this is critical for best Hunyuan3D-2.1 results\n2. Load your reference image using `Hunyuan 3D 2.1 Load Image with Transparency`\n3. Preview your Hunyuan3D-2.1 result in the 3D viewer\n\n### Hunyuan3D-2.1 Essential Settings\n\n**For Shape Generation:**\n- `steps`: 25 (sweet spot for quality vs speed in Hunyuan3D-2.1)\n- `guidance_scale`: 7.5 (how closely it follows your image)\n- Keep `control_after_generate` on \"fixed\" for consistency\n\n**For Texturing:**\n- `view_size`: 768 (good quality without killing your VRAM)\n- `texture_size`: 1024 (high-res textures for Hunyuan3D-2.1)\n- `guidance_scale`: 3.0 (texture alignment strength)\n\n## Advanced Tips for Hunyuan3D-2.1\n\n**Image Preparation is Everything**: Clean backgrounds and clear subjects are absolutely critical for optimal Hunyuan3D-2.1 performance. The model performs dramatically better with well-prepared input images - remove backgrounds, center your subject, and ensure good lighting on the main object.\n\n**Mesh Quality Control**: The `Post Process Trimesh` node is your friend in Hunyuan3D-2.1. Enable `remove_floaters` and `reduce_faces` for cleaner results. Adjust `max_facenum` based on your needs - lower for real-time applications, higher for high-quality renders.\n\n**Texture Resolution Balance**: The `octree_resolution` in the VAE Decoder controls mesh detail level in Hunyuan3D-2.1. 256 is the default, but you can push it higher for more geometric detail if you have the compute power.\n\n## More Information\n\n**Research & Development**:\n- **Original Paper**: [Hunyuan3D-2.1 Technical Report](https://arxiv.org/abs/2506.15442)\n- **Official Repository**: [Tencent Hunyuan3D-2.1](https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1)\n- **ComfyUI Integration**: [ComfyUI-Hunyuan3d-2-1](https://github.com/visualbruno/ComfyUI-Hunyuan3d-2-1)\n\n### Acknowledgements\n\nThis workflow is powered by **Hunyuan3D-2.1**, developed by the **Tencent Hunyuan3D Team**. The **Hunyuan3D-2.1 ComfyUI implementation** is created by **visualbruno** and the open-source community, bringing cutting-edge Hunyuan3D-2.1 technology to creators worldwide. Special thanks to the research team for making this fully open-source - democratizing access to production-quality 3D asset generation through Hunyuan3D-2.1.\n"
    },
    {
        "id": "1241",
        "readme": "## What is the Flux Kontext Character Turnaround Sheet LoRA?\n\nThis workflow transforms a single character illustration into a complete turnaround sheet with five distinct poses using the Flux Kontext Character Turnaround Sheet LoRA. Think of it as having a 3D scanner for your 2D characters - feed it one image, and get a comprehensive view package that shows your character from every angle. The Flux Kontext Character Turnaround Sheet LoRA specializes in maintaining character consistency while generating front, profile, 3/4, and back views that look like they came from the same model sheet.\n\nCharacter turnaround sheets are the backbone of professional animation and 3D modeling pipelines. Instead of manually drawing each pose or struggling with inconsistent AI generations, this Flux Kontext Character Turnaround Sheet LoRA workflow delivers clean, evenly-spaced character views on a pure white background - exactly what you need for reference material or feeding into 3D reconstruction tools like Rodin or Tripo.\n\n## How the Flux Kontext Character Turnaround Sheet LoRA Generation Works\n\nThis workflow uses the specialized Flux Kontext Character Turnaround Sheet LoRA trained on turnaround sheet data to understand how characters look from different angles. The Flux Kontext Character Turnaround Sheet LoRA reconstructs your character from five viewpoints while keeping proportions and details consistent - like having an artist rotate your character in 3D space and sketch each angle perfectly. The Flux Kontext Character Turnaround Sheet LoRA's training data ensures seamless transitions between poses while maintaining character integrity.\n\n## How to Use the Flux Kontext Character Turnaround Sheet LoRA Workflow\n\n### Basic Generation Process\n\n**Step 1: Input Your Character**\nLoad your character image in the `Load Image` node. The Flux Kontext Character Turnaround Sheet LoRA performs best with full-body shots of illustrated or stylized characters rather than real photos or tight headshots.\n\n**Step 2: Set Your Prompt**\nIn the `CLIP Text Encode (Positive Prompt)` node, keep the trigger phrase that activates the Flux Kontext Character Turnaround Sheet LoRA properly.\n\n**Step 3: Generate Your Turnaround**\nHit `Run` and the Flux Kontext Character Turnaround Sheet LoRA output shows up in the `Save Image` node as a horizontal sheet.\n\n## Acknowledgements\n\nThis workflow features the **Flux Kontext Character Turnaround Sheet LoRA** created by **reverentelusarca**. The Flux Kontext Character Turnaround Sheet LoRA was trained using Ostris AI Toolkit and represents a specialized approach to multi-view character generation. You can find the original model on [Civitai](https://civitai.com/models/1753109/flux-kontext-character-turnaround-sheet-lora). Full credit goes to the original Flux Kontext Character Turnaround Sheet LoRA creator for developing this powerful tool that bridges the gap between 2D character art and 3D production pipelines.\n"
    },
    {
        "id": "1242",
        "readme": "## What is Flux Kontext 360 Degree LoRA?\n\nFlux Kontext 360 Degree LoRA is a specialized model that transforms regular images into seamless 360-degree panoramic views. Think of it as having a magic lens that can take any photo and stretch it into a full wraparound world—perfect for VR environments, immersive backgrounds, or creating those cool spherical images you see in virtual tours.\n\nThe Flux Kontext 360 Degree LoRA model has been specifically trained on before/after image pairs using the Ostris AI Toolkit, learning how to intelligently fill in the missing parts of an image to create a complete 360-degree experience while maintaining visual coherence across the seams.\n\n## Key Benefits of Flux Kontext 360 Degree LoRA\n\n* **Seamless Edge Blending:** Flux Kontext 360 Degree LoRA creates panoramic images where left and right edges connect naturally\n* **Simple Input Process:** Just upload any image and let the Kontext LoRA work its magic\n* **VR-Ready Output:** Generate content perfect for virtual environments and immersive experiences with Flux Kontext 360 Degree LoRA\n* **Flexible Quality Control:** Raw outputs can be enhanced significantly with upscaling techniques\n\n## Quick Start Guide for Flux Kontext 360 Degree LoRA\n\n**Basic Setup:**\n1. **Load your image** in the `Load Image` node\n2. **Set dimensions** to 2:1 ratio (recommended: 1536x768 pixels)\n3. **Write your prompt** describing/enhancing your input image (e.g., \"ocean waves, stormy weather\" for a sea photo)\n4. **Click Queue Prompt** and let the Flux Kontext 360 Degree LoRA workflow transform your image into 360-degree panorama\n\n## Understanding the Flux Kontext 360 Degree LoRA Workflow\n\n**Stage 1 - Base Generation:**\nThe core generation happens with standard Flux nodes plus the Kontext 360 Degree LoRA:\n- `Load Diffusion Model` + `Load LoRa`: Loads Flux with the Kontext 360 Degree LoRA enhancement\n- `CLIPTextEncode`: Processes your prompt to guide the image transformation\n- `FluxGuidance`: Controls how closely the output follows your prompt (default: 2.5)\n- `KSamplerSelect` + `SamplerCustomAdvanced`: Generates the initial 360-degree image with Flux Kontext LoRA\n\n**Stage 2 - Crop and Enhance:**\nMultiple `Crop Image` nodes prepare different sections for processing:\n- Creates overlapping sections to ensure seamless blending\n- Each crop gets processed through `ComfyUI_CustomNodes` for refinement\n\n**Stage 3 - Upscale and Polish:**\n- `Ultimate SD Upscale`: Tiles the image for high-resolution output without losing quality\n- `Inpaint` section: Automatically fixes any remaining seam issues using mask-based editing\n- Final `Paste By Mask` operations blend everything together\n\n## Pro Tips for Flux Kontext 360 Degree LoRA\n\n**Dimension Settings:**\n- **Always use 2:1 aspect ratio** for first generation with Flux Kontext 360 Degree LoRA (1536x768, 2048x1024, etc.)\n- Avoid square or portrait formats—they don't work well for 360-degree content\n\n**FluxGuidance Tuning:**\n- **Default 2.5** works for most scenes with Kontext LoRA\n- **Lower (1.5-2.0)** for more creative, loose interpretations\n- **Higher (3.0-4.0)** for stricter prompt adherence, but watch for artifacts\n\n**Ultimate SD Upscale Optimization:**\n- **Tile size 1536x1280** balances quality and VRAM usage\n- **Enable \"Half Tile\" overlap** to minimize visible seam lines\n- **2x upscale** is usually sufficient; 4x for exhibition-quality output\n\n**Prompt Writing for Flux Kontext 360 Degree LoRA:**\n- **Describe your input image** to help the Kontext LoRA model understand: \"stormy ocean\" for a sea photo\n- **Add enhancement keywords**: \"wide panoramic view, seamless, immersive\"\n- **Include style directions**: \"cinematic lighting\" or \"natural landscape\"\n\n## More Information\n\n* **Original Model:** [Civitai Model Page](https://civitai.com/models/682349?modelVersionId=1958732)\n* **Training Framework:** Ostris AI Toolkit  \n* **Model Type:** LoRA for Flux diffusion models\n* **License:** Check model page for current license terms\n\n## Acknowledgements\n\nThis workflow showcases the **Flux Kontext 360 Degree LoRA** model, designed for transforming standard images into immersive panoramic experiences. The Flux Kontext 360 Degree LoRA model demonstrates how specialized LoRA training can extend Flux diffusion models into new creative territories while maintaining the accessibility and power of the Flux ecosystem."
    },
    {
        "id": "1243",
        "readme": "### 1. What is the FLUX Kontext OmniConsistency LoRA Workflow?\nThe FLUX Kontext OmniConsistency LoRA workflow brings the power of artistic style transfer to ComfyUI through a clever integration of the Kontext model with OmniConsistency LoRA. Think of it as having a master artist who can instantly reimagine your photos in 22 different art styles - from Studio Ghibli's dreamy watercolors to LEGO's blocky charm - while keeping your subject perfectly recognizable.\n\nThis FLUX Kontext OmniConsistency LoRA workflow leverages a LoRA-adapted version of the 12B-parameter Kontext rectified flow transformer, fine-tuned specifically for multi-style artistic transformation. What makes FLUX Kontext OmniConsistency LoRA special? You don't need 22 different models for 22 different styles. One FLUX Kontext OmniConsistency LoRA workflow, one LoRA, endless creative possibilities.\n\n### 2. Key Features and Benefits of FLUX Kontext OmniConsistency LoRA\n**22 Artistic Styles**: FLUX Kontext OmniConsistency LoRA empowers users to convert images into 22 unique art styles, from 3D Chibi to Pop Art, using intuitive text prompts with FLUX Kontext OmniConsistency LoRA.\n\n**Content-Preserving Conversion**: By combining the original Kontext model with LoRA fine-tuning, FLUX Kontext OmniConsistency LoRA ensures transformed images retain the subject's layout and identity throughout the FLUX Kontext OmniConsistency LoRA process.\n\n**LoRA-Based Fine-Tuning**: FLUX Kontext OmniConsistency LoRA leverages Hugging Face-hosted LoRA modules to integrate seamlessly with pre-trained models for faster and memory-efficient customization in FLUX Kontext OmniConsistency LoRA workflows.\n\n### 3. How to Use FLUX Kontext OmniConsistency LoRA in ComfyUI\n#### FLUX Kontext OmniConsistency LoRA Basic Workflow\n\n**Load your image**\n- Use the **LoadImage** node to import the base image you want to transform with FLUX Kontext OmniConsistency LoRA.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1243/readme01.webp\" alt=\"FLUX Kontext OmniConsistency LoRA\" width=\"650\"/>\n\n**Set your style prompt**\n- Select your desired style using the Integer node (1-22) in FLUX Kontext OmniConsistency LoRA:\n  - Simply change the number to switch between styles in FLUX Kontext OmniConsistency LoRA\n  - Each number corresponds to a different artistic style\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1243/readme03.webp\" alt=\"FLUX Kontext OmniConsistency LoRA\" width=\"650\"/>\n\n**Run the workflow**\n- Hit `run` to execute FLUX Kontext OmniConsistency LoRA.\n\n**View and save output**\n- Check results in the **PreviewImage** node and collect final FLUX Kontext OmniConsistency LoRA images from the **SaveImage** output.\n\n#### Essential Settings for FLUX Kontext OmniConsistency LoRA\n\n**LoRA Strength** (in `Load LoRA` node for FLUX Kontext OmniConsistency LoRA):\n* Default: 1.0\n* Lower (0.7-0.9): More subtle style transfer, preserves more original details in FLUX Kontext OmniConsistency LoRA\n* Higher (1.1-1.3): Stronger stylization, more dramatic transformation with FLUX Kontext OmniConsistency LoRA\n\n**FluxGuidance** (Style Enhancement in FLUX Kontext OmniConsistency LoRA):\n* Default: 2.5\n* Range: 2.0-2.8 for best balance in FLUX Kontext OmniConsistency LoRA\n* Higher values = stronger style effects\n* Too high (>3.0) may distort details\n\n**KSampler Settings for FLUX Kontext OmniConsistency LoRA**:\n* **Steps**: 20 (default)\n* More steps = better quality but slower FLUX Kontext OmniConsistency LoRA generation\n  \n### 4. Acknowledgement\nThis FLUX Kontext OmniConsistency LoRA workflow integrates the Kontext_OmniConsistency_LoRA model developed by @svjack, which builds on the research foundation of Omni-Consistency and fine-tuning techniques tailored for image-generation applications. Special recognition to @svjack for both the original FLUX Kontext OmniConsistency LoRA model development and its seamless ComfyUI integration, enabling efficient and customizable AI workflows. All credit goes to them for their ongoing contributions to enhancing consistency in FLUX Kontext OmniConsistency LoRA text-to-image generation frameworks.\n\n### 5. More Resources About FLUX Kontext OmniConsistency LoRA\nExplore technical resources and documentation related to FLUX Kontext OmniConsistency LoRA:\n* **GitHub Repository** – FLUX Kontext OmniConsistency LoRA workflow definition and integration files for ComfyUI. [Kontext OmniConsistency LoRA.json](https://github.com/comfyonline/comfyonline_workflow/blob/main/Kontext%20OmniConsistency%20Lora.json)\n* **HuggingFace Weights** – Pretrained FLUX Kontext OmniConsistency LoRA weights compatible with ComfyUI for multi-style artistic transformation. [Kontext_OmniConsistency_LoRA](https://huggingface.co/svjack/Kontext_OmniConsistency_LoRA)\n"
    },
    {
        "id": "1244",
        "readme": "## What is FLUX Kontext LoRA?\n\n**FLUX Kontext LoRA** is a powerful image editing workflow that transforms your photos using AI-driven style transfer. This FLUX Kontext LoRA workflow comes with **13 pre-loaded artistic styles**, plus the ability to **upload your own custom LoRA files** for unlimited creative possibilities.\n\nBuilt on the FLUX Kontext model with curated LoRA adapters, FLUX Kontext LoRA acts like specialized art filters that understand context and intelligently apply artistic transformations while respecting your image's lighting, perspective, and subject matter.\n\n## 13 Pre-loaded LoRA Styles in FLUX Kontext LoRA\n\nThis FLUX Kontext LoRA workflow comes with 13 carefully curated LoRA styles, each optimized for specific artistic transformations:\n\n**Realistic Enhancement**\n- **RealisticKontextLoRA** - Natural photo enhancement and portrait refinement\n\n**Animation & Character Styles**\n- **Ghibli Style** - Studio Ghibli's signature animation aesthetic\n- **Dragon Ball Style** - Classic manga and anime character styling\n- **FlatAnimteStyleV1** - Clean, flat animation aesthetics\n\n**Digital Art & Effects**\n- **Kontext Pixel Art** - Retro gaming and pixel-perfect styling\n- **UnFlux** - Advanced flux-based artistic transformation\n- **Glass Prism** - Crystalline and reflective surface effects\n- **360 Degree** - Panoramic HDR environment mapping\n\n**Traditional & Cultural**\n- **GuoHuaKontextLoRA** - Classical ink wash and brush painting\n\n**Creative & Functional**\n- **Mech Anything** - Futuristic mechanical transformations\n- **Fluffy Kontext Dev LoRA** - Soft textures and adorable character styling\n- **Kontext Three-view sketch** - Technical drawing and concept art\n- **Kontext_change_clothes** - Clothing transformation while preserving pose\n\n## Key Features and Benefits of FLUX Kontext LoRA\n\n**13 Built-in LoRA Styles**: FLUX Kontext LoRA includes curated artistic styles from Ghibli to pixel art, mech designs, and traditional painting.\n\n**Custom LoRA Support**: Upload your own LoRA files through the Assets panel to extend FLUX Kontext LoRA's creative possibilities.\n\n**Multi-LoRA Mixing**: FLUX Kontext LoRA allows combining multiple styles simultaneously with adjustable strengths for unique hybrid aesthetics.\n\n**Smart Image Processing**: FLUX Kontext LoRA automatically optimizes input dimensions for best quality while maintaining aspect ratios.\n\n**Batch Generation**: Generate multiple variations at once using FLUX Kontext LoRA's batch_size parameter.\n\n**Contextual Understanding**: FLUX Kontext LoRA applies artistic transformations that respect your image's lighting, perspective, and subject matter.\n\n## How to Use FLUX Kontext LoRA\n\n### Quick Start with FLUX Kontext LoRA (5 Steps)\n\n**Step 1: Upload Your Image**  \nClick on the \"Load Image\" node and upload the photo you want to transform with FLUX Kontext LoRA.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1244/readme01.webp\" alt=\"FLUX Kontext LoRA\" width=\"550\"/>\n\n**Step 2: Choose Your LoRA Style**  \nIn the \"Power Lora Loader (rgthree)\" node, toggle on one of the 13 available FLUX Kontext LoRA styles. Each LoRA is clearly labeled.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1244/readme02.webp\" alt=\"FLUX Kontext LoRA\" width=\"550\"/>\n\n**Step 3: Check the Style Guide**  \nReference the FLUX Kontext LoRA list above for the exact prompt each style needs. For example, \"Glass Prism\" works best with \"make this object a glass prism with reflections\".\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1244/readme03.webp\" alt=\"FLUX Kontext LoRA\" width=\"550\"/>\n\n**Step 4: Write Your Prompt**  \nIn the \"CLIP Text Encode\" node, enter the recommended prompt for your chosen FLUX Kontext LoRA style. Keep it simple and match the style's requirements.\n\n**Step 5: Generate**  \nHit `run` and watch FLUX Kontext LoRA transform your image. Results appear in the \"Save Image\" node on the right.\n\n### Essential FLUX Kontext LoRA Settings\n\n**Power Lora Loader (rgthree)**  \nYour FLUX Kontext LoRA style control center with 13 pre-loaded options:\n- Toggle individual FLUX Kontext LoRA styles on/off with the blue switches\n- Adjust \"Strength\" (default 1.00) to control FLUX Kontext LoRA style intensity\n- Use \"Toggle All\" to quickly disable all FLUX Kontext LoRA styles\n- Mix multiple FLUX Kontext LoRA styles by enabling several at different strengths\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1244/readme04.webp\" alt=\"FLUX Kontext LoRA\" width=\"550\"/>\n\n### Advanced FLUX Kontext LoRA Tips\n\n**Custom LoRA Upload for FLUX Kontext LoRA**:  \nNavigate to Assets → ComfyUI → models → loras folder. Either paste a download URL or upload your own LoRA files directly to this folder. They'll appear in the FLUX Kontext LoRA Power Lora Loader dropdown after refresh.\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1244/readme05.webp\" alt=\"FLUX Kontext LoRA\" width=\"750\"/>\n\n**FLUX Kontext LoRA Mixing Strategies**:  \n- Combine complementary FLUX Kontext LoRA styles (e.g., Ghibli + Fluffy for dreamy textures)\n- Use lower strengths (0.5-0.8) when mixing multiple FLUX Kontext LoRA styles\n- Pixel art + realistic FLUX Kontext LoRA styles can create interesting retro-modern hybrids\n\n**Batch Generation with FLUX Kontext LoRA**:  \n- Increase batch_size to 2-4 for quick FLUX Kontext LoRA variations\n- Higher batch sizes use more memory but save time with FLUX Kontext LoRA\n- Each batch uses the same prompt but different random seeds in FLUX Kontext LoRA\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1244/readme06.webp\" alt=\"FLUX Kontext LoRA\" width=\"550\"/>\n\n**FLUX Kontext LoRA Quality Control**:  \n- If FLUX Kontext LoRA results look over-stylized, reduce LoRA strength to 0.7-0.8\n- For subtle FLUX Kontext LoRA effects, try strength values around 0.5\n- Experiment with different samplers if default FLUX Kontext LoRA results aren't optimal\n\n## Acknowledgements\n\nThis workflow is powered by the **FLUX Kontext model** and **[rgthree's Power Lora Loader](https://www.mimicpc.com/workflows/flux-kontext-image-editing-with-lora)**. Credit to the original developers and the community contributors who created the specialized LoRA adapters for FLUX Kontext LoRA."
    },
    {
        "id": "1245",
        "readme": "## What is Consistent-Face-3x3-Generator?\n**Consistent-Face-3x3-Generator** is a structured face generation workflow designed to produce a **3×3 grid** of the **same character** rendered across **nine different poses**. The Consistent-Face-3x3-Generator workflow uses the FLUX base model along with the **Flux Depth LoRA** to ensure facial consistency while enabling expressive pose variety.\n\nThis Consistent-Face-3x3-Generator workflow is perfect for creating pose reference sheets, dataset generation, or consistency testing for stylized characters.\n\n## Why Use Consistent-Face-3x3-Generator?\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1245/readme01.webp\" alt=\"Consistent-Face-3x3-Generator\" width=\"750\"/>\n</p>\n\n**Consistent-Face-3x3-Generator** offers:\n- **Consistent Identity**: Preserves the character's face across 9 outputs in the Consistent-Face-3x3-Generator grid\n- **Pose Variation**: Each cell in the Consistent-Face-3x3-Generator's 3×3 grid shows a unique head pose  \n- **Depth-Aware Guidance**: Uses Flux Depth LoRA for 3D-aware consistency  \n- **All-in-One Grid Output**: Consistent-Face-3x3-Generator outputs a unified 3×3 visual presentation  \n- **Ideal for Stylized Workflows**: Perfect for anime, toon, and concept character pipelines\n\nWhether you're generating stylized portraits or testing identity consistency, the *Consistent-Face-3x3-Generator* is optimized for clarity, control, and batch visual output.\n\n\n## Base\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1245/readme02.webp\" alt=\"Consistent-Face-3x3-Generator base\" width=\"550\"/>\n</p>\n\nThis group in Consistent-Face-3x3-Generator loads all the core models and handles initial generation:\n- Load the **FLUX model** and **Flux Depth LoRA** for Consistent-Face-3x3-Generator\n- Enter your **main prompt** describing the character and style\n- Set **CFG, Steps, Sampler**, and other KSampler parameters\n- The Consistent-Face-3x3-Generator result is a **single 3×3 base image** containing all 9 variations\n\nThis is your starting grid for further upscaling and detailing in the Consistent-Face-3x3-Generator workflow.\n\n\n## SD Upscaler\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1245/readme03.webp\" alt=\"Consistent-Face-3x3-Generator upscaler\" width=\"550\"/>\n</p>\n\nThis stage in Consistent-Face-3x3-Generator uses **Stable Diffusion Upscale** to enhance each image in the grid:\n- The Consistent-Face-3x3-Generator's 3×3 grid is split internally\n- Each image is processed one by one for cleaner high-res output\n- Helps preserve pose clarity while boosting resolution\n\nGreat for preparing final Consistent-Face-3x3-Generator outputs suitable for publishing or design references.\n\n\n## Splitter Group and Refiner\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1245/readme04.webp\" alt=\"Consistent-Face-3x3-Generator splitter\" width=\"550\"/>\n</p>\n\nThis Consistent-Face-3x3-Generator group:\n- **Splits** the single grid image into 9 separate face crops\n- Sends each image into a **refiner model** to clean up details\n- Keeps the identity consistent while enhancing features\n\nThe Consistent-Face-3x3-Generator prepares each portrait for high-detail enhancement in the next step.\n\n\n## Adetailer\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1245/readme05.webp\" alt=\"Consistent-Face-3x3-Generator adetailer\" width=\"550\"/>\n</p>\n\nIn the Consistent-Face-3x3-Generator workflow:\n- Applies **face-specific detailing** to each cropped image\n- Enhances skin texture, eye sharpness, and facial structure\n- Ideal for stylized or photorealistic finishing based on prompt\n\nThe Consistent-Face-3x3-Generator's Adetailer is aimed at improving the facial realism and expression clarity across poses.\n\n\n## Chin Detailer + Output\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1245/readme06.webp\" alt=\"Consistent-Face-3x3-Generator output\" width=\"550\"/>\n</p>\n\nFinal touch-up and output for Consistent-Face-3x3-Generator:\n- **Fixes and adjusts chin/jaw structure** if needed\n- Saves final upscaled and refined portraits from Consistent-Face-3x3-Generator to your local directory\n- Output path: `ComfyUI/output/YYYY-MM-DD/`\n\nEach image in the Consistent-Face-3x3-Generator set is saved individually for easy access and review.\n\n\n## Acknowledgement\n\nThe Consistent-Face-3x3-Generator workflow was developed by **tenofas** and is based on the **FLUX** image generation model and its **Flux Depth LoRA**. Full credit goes to tenofas for designing a reference-consistent, multi-pose portrait generator inside ComfyUI.  \nOriginal Workflow: [tenofas on OpenArt](https://openart.ai/workflows/tenofas/consistent-face-3x3-generator/z23yGwwWBpM5AWj4PT0Q)\n\nThe Consistent-Face-3x3-Generator workflow makes identity-stable image generation easier and more modular, letting artists and developers explore structured visual variation at scale.\n"
    },
    {
        "id": "1246",
        "readme": "## What is the FLUX Kontext Presets ComfyUI Workflow?\n\nThe FLUX Kontext presets ComfyUI workflow brings Black Forest Labs' revolutionary preset system directly into ComfyUI, transforming how you edit and reimagine images with simple style selections. Think of it as having a creative AI assistant that instantly understands complex editing instructions - just pick a FLUX Kontext preset like \"Pixel Art\" or \"Movie Poster\" and watch your images transform without writing detailed prompts.\n\nThis FLUX Kontext presets workflow replicates the official BFL preset system with over 15 built-in styles, including but not limited to: Teleport, Move Camera, Relight, Product Photo, Zoom, Colorize, Remove Text, Cartoonify, Movie Poster, Haircut, Bodybuilder, and Interior Design. The magic of FLUX Kontext presets happens through intelligent prompt generation that converts your simple preset choice into sophisticated Kontext-compatible instructions.\n\n## Key Features and Benefits of FLUX Kontext Presets\n\n**One-Click Style Transformation**: Select from 15+ FLUX Kontext presets to instantly transform images - no prompt engineering required.\n\n**Multiple LLM Options**: Choose between free Janus model or premium OpenAI/Gemini for enhanced FLUX Kontext presets creative results.\n\n**Official BFL Preset Replication**: Authentic FLUX Kontext preset behaviors matching the official Kontext system exactly.\n\n**Customizable Presets**: Edit or add your own FLUX Kontext presets by modifying the kontextpresets.py file.\n\n**Seamless Integration**: FLUX Kontext presets work perfectly with existing Kontext workflows for immediate creative exploration.\n\n## How to Use FLUX Kontext Presets in ComfyUI\n\n### **Getting Started with FLUX Kontext Presets**\n**Load your source image**\n* Use the **Load Image** node to upload your photo for FLUX Kontext presets\n* Any image works - portraits, landscapes, products, or scenes\n* The FLUX Kontext presets system adapts to your content automatically\n\n**Choose your FLUX Kontext preset style**\n* Click the **Kontext Presets** dropdown menu\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1246/readme04.webp\" alt=\"FLUX Kontext Preset\" width=\"750\"/>\n* Select from FLUX Kontext presets like \"Pixel Art\", \"Cartoonify\", \"Movie Poster\", etc.\n* Each FLUX Kontext preset triggers specific transformation behaviors\n\n**Generate and wait**\n* Simply click `Run` after selecting your FLUX Kontext preset\n* The workflow handles all prompt generation automatically\n* Watch as your image transforms based on the selected FLUX Kontext preset style\n* No manual prompting needed - the FLUX Kontext preset does all the work!\n\n### **Available FLUX Kontext Preset Styles**\n**Image Utilities**\n* **Move Camera**: Reveal new angles and perspectives with FLUX Kontext presets\n* **Relight**: Professional lighting adjustments using FLUX Kontext presets\n* **Product Photo**: Transform into commercial-quality shots\n* **Zoom**: Dynamic zoom effects on subjects\n* **Colorize**: Add vibrant colors to any image\n\n**Creative Transformations**\n* **Cartoonify**: Convert photos to cartoon style with FLUX Kontext presets\n* **Movie Poster**: Create dramatic film poster designs  \n* **Pixel Art**: Retro 8-bit pixelated conversions\n* **Haircut**: Change hairstyles naturally\n* **Bodybuilder**: Athletic body transformations\n\n**Removal & Editing**\n* **Remove Text**: Clean text from images using FLUX Kontext presets\n* **Remove Furniture**: Clear spaces in interiors\n* **Remove Anything**: Smart object removal\n* **Interior Design**: Redesign room aesthetics\n\n### **Advanced FLUX Kontext Presets Settings**\n**LLM Model Selection**\n* **Janus** (Free): Default model for FLUX Kontext presets, no setup needed\n* **OpenAI/Gemini**: Enhanced creativity options (requires credits via Settings → User, payments go to ComfyUI.org)\n* Choose based on your quality needs and preferences\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1246/readme07.webp\" alt=\"FLUX Kontext Preset\" width=\"750\"/>\n\n**Custom FLUX Kontext Preset Creation**\n* Edit `kontextpresets.py` in the custom_nodes folder\n* Add your own FLUX Kontext preset prompts following the existing format\n* Restart ComfyUI to load new FLUX Kontext presets\n* Share custom FLUX Kontext presets with the community\n\n## Acknowledgement\n\nThis FLUX Kontext presets ComfyUI workflow is a community implementation of Black Forest Labs' official Kontext preset system. Special thanks to the original workflow creator for reverse-engineering the FLUX Kontext preset behaviors and making them accessible in ComfyUI. The workflow faithfully replicates BFL's innovative approach to AI-powered image editing through intelligent FLUX Kontext presets prompt generation.\n\n## More Resources About FLUX Kontext Presets\n\nExplore additional resources for the FLUX Kontext presets system:\n* **Official BFL Presets** – Try the original FLUX Kontext presets system at [FLUX Playground](https://playground.bfl.ai/image/edit)\n* **Custom Preset Sharing** – Community FLUX Kontext presets and modifications on Reddit [r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/)\n* **Workflow Download** – Get the latest FLUX Kontext presets version with updates [Google Drive](https://drive.google.com/drive/folders/1V9xmzrS2Y9lUurFnhOHj4nOSnRFFTK74)\n"
    },
    {
        "id": "1247",
        "readme": "## What is the OmniGen2 ComfyUI Workflow?\n\nThe OmniGen2 ComfyUI workflow brings unified multimodal generation to your fingertips, combining text-to-image synthesis and instruction-based image editing in a single, powerful framework. Think of it as having a creative AI assistant that not only generates stunning images from your text descriptions but also understands and executes complex editing commands with remarkable precision.\n\nThis workflow leverages a sophisticated 7B parameter model built on the Qwen 2.5 VL foundation, featuring a unique dual-path Transformer architecture. What makes this model special is its decoupled design - using separate pathways for text and image generation, allowing it to maintain exceptional language understanding while delivering high-fidelity visual outputs that stay true to your creative vision.\n\n## Key Features and Benefits of OmniGen2\n\n**Dual Generation Modes**: OmniGen2 creates new images from text or edits existing ones with natural language commands through the intuitive interface.\n\n**Advanced Architecture**: The OmniGen2 dual-path design separates text and image processing for optimal performance.\n\n**Compositional Understanding**: OmniGen2 handles complex multi-element prompts with exceptional accuracy in every generation.\n\n**Precise Image Editing**: Make targeted changes while preserving the rest of your image perfectly using OmniGen2 advanced algorithms.\n\n**Multimodal Reflection**: OmniGen2 self-analyzes and refines outputs for improved results.\n\n## How to Use OmniGen2 in ComfyUI\n\n### **OmniGen2 Text-to-Image Workflow**\n\n**Set your image dimensions**\n* Use the **EmptySD3LatentImage** node to define output size for OmniGen2:\n  * Adjust width and height based on your OmniGen2 needs\n  * Keep batch_size at 1 for single image generation\n\n**Craft your text prompt**\n* In the **CLIP Text Encode (Prompt)** nodes for OmniGen2:\n  * Write detailed, descriptive prompts in the first encoder\n  * Leave the second encoder empty or add negative prompts\n  * OmniGen2 excels with complex compositional descriptions\n\n**Generate and save**\n* Hit `Run` to create your OmniGen2 image\n* The **VAE Decode** converts latents to final image\n* **Save Image** automatically saves your OmniGen2 creation to output folder\n\n### **OmniGen2 Image Editing Workflow**\n\n**Upload your source image**\n* Use the **Load Image** node to import the image you want to edit with OmniGen2\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1247/readme03.webp\" alt=\"OmniGen2\" width=\"650\"/>\n\n**Write your editing instruction**\n* In the **CLIP Text Encode (Prompt)** node for OmniGen2:\n  * Describe what changes you want clearly and specifically\n  * Examples: \"Transform character's hair color into natural silver-white\", \"Add aviator sunglasses\"\n  * Natural language commands work perfectly with OmniGen2\n\n**Configure OmniGen2 editing parameters**\n* **Scale Image to Total Pixels** node:\n  * **upscale_method**: area (maintains quality during resizing)\n  * **megapixels**: 2.00 (controls total pixel count)\n    * This resizes your image to approximately 2 million pixels total\n    * For example: would scale a 1920x1080 image to maintain ~2MP\n    * Higher values = more detail but slower processing\n    * Lower values = faster generation but less detail\n    * 2.00 is optimal for editing capabilities\n    <img src=\"https://cdn.runcomfy.net/workflow_assets/1247/readme01.webp\" alt=\"OmniGen2\" width=\"650\"/>\n* **VAE Encode** converts your scaled image to latent space\n\n**Optional: Enable second image input**\n* The purple (bypassed) nodes allow multi-image operations:\n  * Press Ctrl+B to toggle bypass mode\n  * Upload a second image for style transfer or object insertion\n  * Perfect for tasks like \"combine elements from image 1 and image 2\"\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1247/readme02.webp\" alt=\"OmniGen2\" width=\"650\"/>\n\n**Generate edited result**\n* Execute the OmniGen2 workflow to see your edits applied\n* Results maintain high fidelity while following instructions precisely\n\n\n## Acknowledgments\n\nThis ComfyUI workflow integrates the groundbreaking OmniGen2 model developed by researchers at Beijing Academy of Artificial Intelligence. Special recognition goes to the team for creating this unified multimodal generation system that pushes the boundaries of what's possible with a 7B parameter model. The architecture represents a significant advancement in balancing model efficiency with generation quality.\n\n## More Resources About OmniGen2\n\nOmniGen2 is released under open-source licensing, making it freely available for both research and commercial applications. For more information about OmniGen2:\n\n* **GitHub Repository** - Official implementation and model architecture details: [VectorSpaceLab/OmniGen2](https://github.com/VectorSpaceLab/OmniGen2)\n* **Project Page** - Comprehensive overview with demos and technical insights: [Official Page](https://vectorspacelab.github.io/OmniGen2/)\n* **ComfyUI Examples** - Step-by-step tutorials and additional workflows: [ComfyUI Examples](https://comfyanonymous.github.io/ComfyUI_examples/omnigen/)\n"
    },
    {
        "id": "1248",
        "readme": "## What is the Cosmos-Predict2 ComfyUI Workflow?\n\nThe Cosmos-Predict2 ComfyUI workflow brings NVIDIA's next-generation physical world foundation model to your fingertips, enabling both high-quality text-to-image generation and innovative video-to-world transformation. Think of it as having a digital crystal ball that can not only create stunning images from text descriptions but also predict and generate realistic video sequences that follow the laws of physics.\n\nThis Cosmos-Predict2 workflow leverages a sophisticated 2B-parameter foundation model specifically designed for physical AI scenarios. What makes Cosmos-Predict2 special? It doesn't just generate pretty pictures - it understands physics, environmental interactions, and realistic dynamics, making Cosmos-Predict2 perfect for industrial simulation, autonomous driving scenarios, urban planning, and scientific research applications.\n\n## Key Features and Benefits of Cosmos-Predict2\n\n**Dual Generation Modes**: Cosmos-Predict2 supports both text-to-image generation for creating static visuals and video-to-world transformation for dynamic scene prediction, all within a single Cosmos-Predict2 ComfyUI workflow.\n\n**Physical Accuracy**: Unlike standard image generators, Cosmos-Predict2 maintains exceptional physical accuracy and environmental interactivity, ensuring Cosmos-Predict2 generated content follows real-world physics and dynamics.\n\n**Professional Applications**: Cosmos-Predict2 is designed for serious use cases including industrial simulation, autonomous driving development, urban planning visualization, and scientific research where accuracy matters most.\n\n**Flexible Video Control**: The Cosmos-Predict2 video generation component includes optional first and last frame control, allowing precise direction over temporal sequences and scene transitions within the Cosmos-Predict2 workflow.\n\n## How to Use Cosmos-Predict2 in ComfyUI\n### **Cosmos-Predict2 Text-to-Image Workflow**\n**Set your image dimensions**\n* Use the **EmptySD3LatentImage** node to define output size for your Cosmos-Predict2 generation:\n  * Default: 1024x1024 pixels\n  * Adjust width and height based on your Cosmos-Predict2 requirements\n  * Keep batch_size at 1 for single image generation\n**Craft your text prompt**\n* In the **CLIP Text Encode (Prompt)** node for Cosmos-Predict2:\n  * Write detailed, descriptive prompts for best Cosmos-Predict2 results\n  * Cosmos-Predict2 excels with physical world descriptions\n  * Include environmental details and spatial relationships in your Cosmos-Predict2 prompts\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1248/readme01.webp\" alt=\"Cosmos-Predict2\" width=\"750\"/>\n* **Generate and save**\n  * Hit `Run` to create your Cosmos-Predict2 image, which saves automatically to the output directory.\n\n### **Cosmos-Predict2 Video-to-World Workflow**\n* **Upload your input image**\n  * Use the **Load Image** node to import your starting frame for Cosmos-Predict2 video generation.\n* **Configure video parameters**\n  * In the **CosmosPredict2ImageToVideoLatent** node:\n    * **Width/Height**: Set to 848x480 for optimal Cosmos-Predict2 performance\n    * **Length**: 33 frames for ~2 second Cosmos-Predict2 videos at 16fps\n    * **Batch_size**: Keep at 1 for Cosmos-Predict2 processing\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1248/readme02.webp\" alt=\"Cosmos-Predict2\" width=\"750\"/>\n* **Optional frame control**\n  * enable the bypassed nodes (Ctrl+B) for first and last frame control in Cosmos-Predict2:\n    * Upload additional images to guide Cosmos-Predict2 video start and end points\n    * Perfect for creating specific narrative sequences with Cosmos-Predict2\n* **Run video generation**\n  * Execute the Cosmos-Predict2 workflow to create physics-aware video sequences that maintain temporal consistency.\n\n**Essential Settings for Cosmos-Predict2**\n* **KSampler Configuration for Cosmos-Predict2**:\n  * **Steps**: 35 (default for Cosmos-Predict2 quality balance)\n  * **CFG**: 4.0 for proper Cosmos-Predict2 guidance strength\n  * **Sampler**: euler (recommended for Cosmos-Predict2)\n  * **Scheduler**: karras for smooth Cosmos-Predict2 generation\n* **Cosmos-Predict2 Video Generation Settings**:\n  * **FPS**: 16 frames per second (optimal for Cosmos-Predict2)\n  * **Format**: Auto-detects best codec for your Cosmos-Predict2 system\n  * Lower frame counts = faster Cosmos-Predict2 generation, higher = smoother motion\n\n## Acknowledgement\n\nThis Cosmos-Predict2 ComfyUI workflow integrates NVIDIA's Cosmos-Predict2 foundation model, a breakthrough in physical world AI generation. Special recognition to the NVIDIA research team for developing this advanced Cosmos-Predict2 physical simulation model and to the ComfyUI community for enabling seamless Cosmos-Predict2 integration. The Cosmos-Predict2 model weights and technical implementation follow NVIDIA's official Cosmos-Predict2 specifications, ensuring authentic performance for professional applications.\n\n## More Resources About Cosmos-Predict2\n\nExplore technical resources and documentation related to Cosmos-Predict2:\n* **GitHub Repository** – Official Cosmos-Predict2 implementation and model files. [Cosmos-predict2](https://github.com/nvidia-cosmos/cosmos-predict2)\n* **HuggingFace Hub** – Pre-trained Cosmos-Predict2 model weights and documentation for ComfyUI integration. [Cosmos-Predict2](https://huggingface.co/collections/nvidia/cosmos-predict2-68028efc052239369a0f2959)\n"
    },
    {
        "id": "1249",
        "readme": "## What is the Push-In Camera - A Motion LoRA for Wan 2.1 ComfyUI Workflow?\n\nThe Push-In Camera - A Motion LoRA for Wan 2.1 ComfyUI workflow transforms static AI-generated videos into dynamic, cinematic sequences with realistic drone-style push-in camera movements. Think of the Push-In Camera - A Motion LoRA for Wan 2.1 as adding a professional cinematographer to your AI video generation toolkit - one that knows exactly how to create that dramatic zoom effect you see in documentaries and films using the Push-In Camera - A Motion LoRA for Wan 2.1 technology.\n\nThis Push-In Camera - A Motion LoRA for Wan 2.1 workflow addresses a common frustration with the Wan 2.1 I2V 720p model: while it produces stunning image quality, the motion can sometimes feel static or \"slideshow-like.\" By integrating this carefully trained Push-In Camera - A Motion LoRA for Wan 2.1, your videos gain natural camera dynamics that breathe life into every frame, creating that coveted \"from wide shot to intimate detail\" movement that makes viewers lean in with Push-In Camera - A Motion LoRA for Wan 2.1 effects.\n\n## Key Features and Benefits of Push-In Camera - A Motion LoRA for Wan 2.1\n\n**Cinematic Motion Enhancement**: The Push-In Camera - A Motion LoRA for Wan 2.1 adds professional-grade push-in camera movement to transform flat sequences into dynamic visual narratives.\n\n**Extensive Training Foundation**: Push-In Camera - A Motion LoRA for Wan 2.1 developed through 40+ iterations using 100 real-world video clips for authentic camera physics.\n\n**Universal Style Compatibility**: Push-In Camera - A Motion LoRA for Wan 2.1 works seamlessly across all visual styles - nature, products, artistic sequences, and more.\n\n**AI-Powered Prompt Generation**: Integrated OpenAI chat node helps craft optimal Push-In Camera - A Motion LoRA for Wan 2.1 prompts automatically.\n\n**One-Click Integration**: Pre-configured Push-In Camera - A Motion LoRA for Wan 2.1 workflow with all nodes connected - just load and run.\n\n## How to Use Push-In Camera - A Motion LoRA for Wan 2.1 in ComfyUI\n\n### **Getting Started with Push-In Camera - A Motion LoRA for Wan 2.1**\n\n**Input your source image for Push-In Camera - A Motion LoRA for Wan 2.1**\n* Use the **Load Image** node to select your starting frame for Push-In Camera - A Motion LoRA for Wan 2.1\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1249/readme04.webp\" alt=\"Push-In Camera - A Motion LoRA for Wan 2.1\" width=\"750\"/>\n* Best Push-In Camera - A Motion LoRA for Wan 2.1 results with images having clear depth layers\n* The Push-In Camera - A Motion LoRA for Wan 2.1 workflow includes an upscaling option for enhanced quality\n\n**Generate your Push-In Camera - A Motion LoRA for Wan 2.1 prompt**\n* Option 1: Write directly in the positive prompt field with `Push-in camera` trigger for Push-In Camera - A Motion LoRA for Wan 2.1\n* Option 2: Use the **OpenAI Chat** node for AI-assisted Push-In Camera - A Motion LoRA for Wan 2.1 prompt creation (requires credits in Settings → User, payments go to ComfyUI.org)\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1249/readme05.webp\" alt=\"Push-In Camera - A Motion LoRA for Wan 2.1\" width=\"750\"/>\n* The chat node can expand simple ideas into detailed Push-In Camera - A Motion LoRA for Wan 2.1 descriptions\n\n**Run the Push-In Camera - A Motion LoRA for Wan 2.1 generation**\n* Click `Run` to process your Push-In Camera - A Motion LoRA for Wan 2.1 cinematic sequence\n* Monitor Push-In Camera - A Motion LoRA for Wan 2.1 progress in the preview window\n* Push-In Camera - A Motion LoRA for Wan 2.1 videos save automatically to your output folder\n\n### **Advanced Push-In Camera - A Motion LoRA for Wan 2.1 Settings**\n**Push-In Camera - A Motion LoRA for Wan 2.1 LoRA Strength Control**\n* Adjust in the **LoraLoaderModelOnly** node for Push-In Camera - A Motion LoRA for Wan 2.1 (default: 1.0)\n* Lower values (0.5-0.7): Subtle Push-In Camera - A Motion LoRA for Wan 2.1 movement\n* Higher values (0.8-1.0): Dramatic Push-In Camera - A Motion LoRA for Wan 2.1 effects\n\n**WanImageToVideo Parameters for Push-In Camera - A Motion LoRA for Wan 2.1**\n* **Length**: 81 frames (~3 seconds at standard fps) for Push-In Camera - A Motion LoRA for Wan 2.1\n* **Batch size**: Keep at 1 for Push-In Camera - A Motion LoRA for Wan 2.1 stability\n\n## Acknowledgement\n\nThis Push-In Camera - A Motion LoRA for Wan 2.1 ComfyUI workflow integrates a meticulously trained LoRA developed specifically for the Wan 2.1 I2V 720p (14B) model. Special recognition goes to the creator who invested over 40 iterations refining this Push-In Camera - A Motion LoRA for Wan 2.1 effect, training on 100 real-world video clips to achieve authentic Push-In Camera - A Motion LoRA for Wan 2.1 cinematic movement. The Push-In Camera - A Motion LoRA for Wan 2.1 workflow implementation leverages ComfyUI's flexibility and OpenAI's language capabilities to make professional Push-In Camera - A Motion LoRA for Wan 2.1 effects accessible to all creators.\n\n## More Resources About Push-In Camera - A Motion LoRA for Wan 2.1\n\nExplore additional resources and documentation for the Push-In Camera - A Motion LoRA for Wan 2.1 LoRA:\n* **Motion LoRA Model** – The specialized Push-In Camera - A Motion LoRA for Wan 2.1 trained for cinematic effects [Motion-Lora-Camera-Push-In-Wan-14B-720p-I2V](https://huggingface.co/lovis93/Motion-Lora-Camera-Push-In-Wan-14B-720p-I2V)\n* **Wan 2.1 Base Model** – The foundation model powering Push-In Camera - A Motion LoRA for Wan 2.1 generation [Wan2.1-I2V-14B-720P](https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P)\n"
    },
    {
        "id": "1250",
        "readme": "## HiDream E1.1 - Image Editing Model\n\nHiDream E1.1 is a powerful open-source text-and-image-based image editing model developed by HiDream-ai, extending the capabilities of the original HiDream-I1. With natural language as its primary control method, this HiDream E1.1 ComfyUI workflow enables intuitive image editing—such as object removal, style change, and content replacement—directly on existing images. Whether you're refining AI-generated content, modifying real photos, or crafting creative edits, HiDream E1.1 brings an unprecedented level of interactivity and precision to visual workflows.\n\n\n### Why Use HiDream E1.1 - Image Editing Workflow?\n\nThe **HiDream E1.1** workflow offers a flexible, fast, and identity-stable setup inside ComfyUI:\n\n* Maintain **consistent character identity** using HiDream E1.1's reference images and guided prompt system\n* Intuitive input and high control over visual results using HiDream E1.1's InstructPix2Pix conditioning\n* Lightweight HiDream E1.1 setup with fast inference times suitable for animation pipelines\n\nFrom storyboards to social media avatars, **HiDream E1.1** ensures your characters stay visually coherent across edits.\n\n\n### How to Use the HiDream E1.1 Workflow in ComfyUI\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1250/readme01.webp\" alt=\"HiDream E1.1\" width=\"750\"/>\n</p>\n\n#### HiDream E1.1 Workflow Overview\n\n* `Load Reference Image` : Provide reference images for HiDream E1.1 to maintain visual consistency\n* `Enter Editing Prompt Text`: Describe the desired style, setting, pose, and output look for HiDream E1.1\n* `Set Generation Settings`: Define resolution, steps, and CFG scale for HiDream E1.1 processing\n* `Generate`: Run the HiDream E1.1 inference and get edited output\n* `Save Image`: Final HiDream E1.1 image is saved to your output folder for review\n\n\n#### **Quick Start Steps with HiDream E1.1:**\n\n1. Upload **reference images** into the HiDream E1.1 image loader nodes\n2. Add your **positive prompt** for HiDream E1.1 editing, styling, pose, and scene\n3. Set the **desired resolution**, sampler, and inference steps for HiDream E1.1\n4. Click `Run` to generate HiDream E1.1 edited output\n5. Download from the Save node to continue or iterate with HiDream E1.1\n\n\n\n### Load Models\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1250/readme02.webp\" alt=\"HiDream E1.1\" width=\"550\"/>\n</p>\n\n* Here you Load the HiDream E1.1 model - Full or FP8 version.\n* HiDream E1.1 models are loaded automatically please give 2-3 mins when running for the first time for the models to download. \n\n\n### Load Image For References\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1250/readme03.webp\" alt=\"HiDream E1.1\" width=\"550\"/>\n</p>\n\nUse the **Load Image Node** to upload image for HiDream E1.1 InstructPix2Pix conditioning.\n\n* Choose clean, well-lit photos for optimal HiDream E1.1 results.\n* Upload your reference image for HiDream E1.1 editing.\n  \n>  These inputs help the HiDream E1.1 model learn and retain facial and other features during generation.\n\n\n### Prompts for HiDream E1.1\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1250/readme04.webp\" alt=\"HiDream E1.1\" width=\"550\"/>\n</p>\n\nThe prompt guides HiDream E1.1 editing of the **object, style, pose, and environment** for the final edited output.\n**Positive Prompt for HiDream E1.1:**\n  ```\n  Change T-shirt color from red to blue. Don't change any other thing\n  ```\n**Negative Prompt for HiDream E1.1 (Optional):**\n  ```\n  blurry, lowres, bad anatomy, extra limbs, distorted face, open mouth, text, watermark\n  ```\n\n>  Use detailed, stylistic descriptors for best HiDream E1.1 results. Combine scene, mood, and character traits.\n\n\n## 4 - Set Generation IP2P Sampler Parameters\n\n<p align=\"center\">\n <img src=\"https://cdn.runcomfy.net/workflow_assets/1250/readme05.webp\" alt=\"HiDream E1.1\" width=\"550\"/>\n</p>\n\nAdjust the HiDream E1.1 Sampler, Seed, Scheduler, Steps and CFG based on your desired visual interest.\n\nAll HiDream E1.1 generated images are automatically saved in your `ComfyUI > output` folder\n\n\n\n## Acknowledgement\n\nThis HiDream E1.1 ComfyUI workflow integrates HiDream-ai's advanced image editing foundation model, a breakthrough in text-guided visual transformation. Special recognition to the HiDream-ai research team for developing this sophisticated HiDream E1.1 editing model and to the ComfyUI community for enabling seamless HiDream E1.1 integration. The HiDream E1.1 model weights and technical implementation follow HiDream-ai's official specifications, ensuring authentic performance for professional applications.\n\n## More Resources About HiDream E1.1\n\nExplore technical resources and documentation related to HiDream E1.1:\n* **HuggingFace Hub** – Official HiDream E1.1 model weights and documentation for ComfyUI integration. [HiDream E1.1](https://huggingface.co/HiDream-ai/HiDream-E1-1)\n* **GitHub Repository** – HiDream E1.1 implementation, source code, and technical details. [HiDream-E1](https://github.com/HiDream-ai/HiDream-E1)\n"
    },
    {
        "id": "1252",
        "readme": "## What is the InstantCharacter ComfyUI Workflow?\n\n**InstantCharacter** is a personalized character image generation workflow for **ComfyUI**, built on top of the **FLUX Diffusion Transformer (DiT)**. Think of InstantCharacter as enabling high-fidelity character rendering from a single reference image and a text prompt—while preserving facial identity, pose adaptability, and creative styling through the InstantCharacter framework.\n\nWhether you're generating stylized portraits, sequential keyframes, or imaginative poses in diverse environments, **InstantCharacter** gives creators precise control with unmatched consistency. The InstantCharacter workflow brings Tencent's breakthrough personalization technology to ComfyUI, combining identity preservation with creative flexibility.\n\n## Key Features and Benefits of InstantCharacter\n**Character Identity Preservation**: InstantCharacter maintains character appearance while flexibly creating high-quality images based on text instructions. The InstantCharacter workflow achieves excellent balance between identity consistency and text controllability.\n\n**Advanced Feature Extraction**: InstantCharacter utilizes SigLIP for fine-grained identity details and DINOv2 for robust background-resistant features. This dual encoder approach provides comprehensive character information through InstantCharacter's multi-level extraction.\n\n**Scalable DiT Architecture**: Built specifically for Diffusion Transformers, InstantCharacter employs full Transformer architecture for efficient FLUX interaction. InstantCharacter uses timestep-aware Q-former and cross-attention mechanisms for superior performance.\n\n**Style Compatibility**: InstantCharacter demonstrates excellent compatibility with style LoRAs across anime, realistic, and artistic styles. The InstantCharacter framework maintains core identity features while adapting to diverse visual styles.\n\n## How to Use InstantCharacter in ComfyUI\n\n### **Load Model**\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1252/readme02.webp\" alt=\"InstantCharacter Model Loading\" width=\"550\"/>\n</p>\n\nThe **InstantCharacter DiT model** and required encoders are automatically loaded:\n\n- **InstantCharacter DiT FLUX Adapter** initializes automatically for optimal performance\n- FLUX model downloads to `models/diffuser` directory for InstantCharacter processing\n- Image encoder downloads to `models/clipvision` for InstantCharacter identity extraction\n- InstantCharacter IP-adapter requires manual download to `models/ipadapter` from: [InstantCharacter IP-Adapter](https://huggingface.co/tencent/InstantCharacter/blob/main/instantcharacter_ip-adapter.bin)\n\n**System Requirements for InstantCharacter**: 45GB VRAM required, or 24GB VRAM with CPU offloading enabled for InstantCharacter workflows.\n\n### **Input Reference**\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1252/readme03.webp\" alt=\"InstantCharacter Reference Input\" width=\"550\"/>\n</p>\n\nUpload a **single reference image** of your character for InstantCharacter processing:\n- **Portrait** or **full body** images work best with InstantCharacter analysis\n- Keep background clutter minimal to improve InstantCharacter's identity encoding accuracy\n- This reference anchors character identity across all InstantCharacter generations\n\nThe InstantCharacter model extracts multi-level features:\n- Low-level facial and clothing details through InstantCharacter's SigLIP encoder\n- Region-level and patch-based structural features via InstantCharacter's DINOv2 system\n\n### **Sampler + Prompt**\n\n<p align=\"center\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1252/readme04.webp\" alt=\"InstantCharacter Sampling\" width=\"550\"/>\n</p>\n\nGuide your InstantCharacter generation with detailed prompts and sampling options\n- **Positive Prompt for InstantCharacter**\n  - Describe scenes, poses, styles, or expressions for InstantCharacter to interpret\n  - InstantCharacter excels with complex action-oriented prompts\n  - Example: `\"a character in sci-fi armor walking in a neon-lit corridor, cinematic lighting\"`\n- **Negative Prompt for InstantCharacter**\n  - Filter unwanted traits from InstantCharacter output\n  - Example: `\"blurry, extra limbs, bad anatomy, watermark\"`\n- **InstantCharacter Sampler Settings**\n  - Choose samplers like **Euler** or **DPM++** optimized for InstantCharacter\n  - Adjust **steps**, **CFG**, and **resolution** for InstantCharacter performance\n  - All InstantCharacter outputs save automatically to ComfyUI output folder\n\nProper prompt design maximizes both **identity alignment** and **scene accuracy** in InstantCharacter workflows.\n\n## Acknowledgement\n\nThis InstantCharacter ComfyUI workflow integrates the **InstantCharacter** model developed by **Tencent** and implemented by **jax-explorer**. Special recognition to the Tencent research team for developing InstantCharacter's advanced personalization system and to the ComfyUI community for enabling seamless InstantCharacter integration.\n\n## More Resources About InstantCharacter\n\nExplore technical resources and documentation related to InstantCharacter:\n* **GitHub Repository** – Official InstantCharacter implementation and model files. [ComfyUI-InstantCharacter](https://github.com/jax-explorer/ComfyUI-InstantCharacter)\n"
    },
    {
        "id": "1253",
        "readme": "## What is the Wan 2.2 ComfyUI Workflow?\n\nThe Wan 2.2 ComfyUI workflow represents the next evolution of multimodal AI generation, featuring three breakthrough capabilities: cinematic-grade aesthetic control, complex dynamic motion generation, and real-world semantic accuracy. Building upon Wan 2.1's foundation, Wan 2.2 transforms text descriptions or static images into professional-grade video content with unprecedented precision and motion quality.\n\nWan 2.2 excels in cross-modal creation, seamlessly converting static images into dynamic scenes while maintaining style consistency across formats. This Wan 2.2 workflow empowers creators from illustrators to game developers with intelligent creative assistance and expanded preset templates for professional AI-powered content generation.\n\n## Key Features and Benefits of Wan 2.2\n\n**Cinematic-Grade Aesthetic Control**: Wan 2.2 integrates professional filmmaking elements—lighting, color grading, and camera language—into the generation model. Control visual aesthetics precisely through multi-dimensional keywords for refined Wan 2.2 artistic expression.\n\n**Complex Dynamic Motion Generation**: Wan 2.2 delivers precise control over human gestures, athletic movements, and facial expressions with fluid motion sequences. Natural detail rendering ensures stable, high-quality Wan 2.2 generation across all motion types.\n\n**Real-World Semantic Accuracy**: Wan 2.2's enhanced instruction following excels in multi-object generation and complex spatial relationships. Transform detailed prompts into realistic scenes that faithfully represent real-world dynamics with Wan 2.2.\n\n**Cross-Modal Creation Capabilities**: Wan 2.2 effortlessly converts static images into dynamic scenes while ensuring style consistency across different formats.\n\n**Intelligent Creative Assistance**: Wan 2.2 provides real-time generation previews, expanded preset templates, and fine-tuned LoRA models for flexible workflows.\n\n## How to Use Wan 2.2 in ComfyUI\n\n### **Wan 2.2 5B Hybrid Version (Recommended)**\nThe versatile 5B model supports both text-to-video and image-to-video in one workflow:\n\n**Text-to-Video Mode (Default)**:\n* Set video dimensions in the **Wan22ImageToVideoLatent** node:\n  * Width: 1280, Height: 704 for widescreen Wan 2.2 output\n  * Length: 121 frames for longer videos\n  * Batch_size: Keep at 1 for single generation\n* Write comprehensive prompts in the **CLIP Text Encode (Positive Prompt)** node:\n  * Basic Formula: Subject + Scene + Motion\n  * Example: \"a young woman in traditional dress dancing gracefully in a sunlit garden, soft golden hour lighting, camera slowly circling\"\n\n**Image-to-Video Mode**:\n* Enable the **Load Image** node (use Ctrl+B to unbypass the purple node)\n* Upload your base image for Wan 2.2 transformation\n* Focus on motion descriptions in your prompts\n\n### **Wan 2.2 14B I2V - Premium Image-to-Video**\nFor maximum image-to-video quality *(Requires 2XLarge machine or higher, otherwise OOM issues may occur)*:\n\n* Upload your image in the **Load Image** node\n* Configure video settings in **WanImageToVideo** node:\n  * Width: 1280, Height: 720 for HD output\n  * Length: 121 frames for smooth Wan 2.2 motion\n* Use motion-focused prompts in **CLIP Text Encode** nodes\n* This 14B model provides superior detail preservation and motion quality\n\n### **Wan 2.2 14B T2V - Pure Text-to-Video**\nFor dedicated text-to-video generation *(Requires 2XLarge machine or higher, otherwise OOM issues may occur)*:\n\n* Set parameters in **EmptyHunyuanLatentVideo** node:\n  * Width: 1280, Height: 704 recommended\n  * Length: 121 frames, Batch_size: 1\n* Craft detailed cinematic prompts emphasizing:\n  * Environmental details and lighting\n  * Camera movements and angles\n  * Complex motion sequences for Wan 2.2 realism\n\n**Essential Wan 2.2 Settings**:\n* **KSampler Configuration**:\n  * Steps: 20 (optimized for Wan 2.2)\n  * CFG: 3.5 (balanced guidance)\n  * Sampler: \"euler\" (stable generation)\n  * Scheduler: \"simple\" for consistent results\n* **Video Output**:\n  * FPS: 24 frames per second for cinematic Wan 2.2 quality\n  * Auto codec selection for best compatibility\n\n## Acknowledgement\n\nThis Wan 2.2 ComfyUI workflow integrates Alibaba's latest Wan 2.2 multimodal generative model, representing a significant advancement in AI video generation technology. Special recognition to the Alibaba Wan Team for developing this breakthrough Wan 2.2 system and the ComfyUI community for enabling seamless Wan 2.2 integration. The Wan 2.2 implementation maintains full compatibility with professional workflows while delivering unprecedented creative control.\n\n## More Resources About Wan 2.2\n\nFor the latest updates and technical resources about Wan 2.2:\n\n* **Official Documentation** – Complete Wan 2.2 setup guide and technical specifications. [ComfyUI Wan 2.2 Docs](https://docs.comfy.org/tutorials/video/wan/wan2_2)\n* **GitHub Repository** – Official Wan 2.2 source code and model implementation. [Wan 2.2 GitHub](https://github.com/Wan-Video/Wan2.2)\n"
    },
    {
        "id": "1254",
        "readme": "## What is the Wan 2.2 + Lightx2v V2 ComfyUI Workflow?\n\nThe Wan 2.2 + Lightx2v V2 ComfyUI workflow represents a breakthrough in ultra-fast text-to-video and image-to-video generation, combining Alibaba's powerful Wan 2.2 14B model with the revolutionary Lightx2v V2 LoRA distillation technology. This workflow delivers professional-quality video generation at lightning speed - up to 4x faster than standard Wan 2.2 workflows while maintaining excellent visual quality.\n\nWan 2.2 + Lightx2v V2 uses a two-stage sampling process: a high-noise model that establishes the basic video structure, followed by a low-noise model that adds fine details and refinements. The Lightx2v V2 component is a specialized distillation LoRA that compresses this Wan 2.2 process dramatically - reducing the standard 40 steps (20+20) to just 8-14 steps while using low CFG settings and LCM sampling for maximum speed.\n\nThink of Wan 2.2 + Lightx2v V2 as having a turbocharged video generator that maintains the quality of the original two-stage Wan 2.2 process but cuts generation time by up to 75%. Perfect for rapid prototyping, content creation, and professional video workflows where speed matters.\n\n## Key Features and Benefits of Wan 2.2 + Lightx2v V2\n\n**Ultra-Fast Generation with Flexible Steps**: The Wan 2.2 + Lightx2v V2 workflow reduces standard 40-step sampling to just 8-14 steps using distillation techniques. Choose 8 steps (maximum speed), 12 steps (balanced), or 14 steps (optimal quality) in your Wan 2.2 + Lightx2v V2 setup.\n\n**Advanced LoRA Acceleration Options**: Offers full acceleration (both stages) for maximum speed or half-acceleration (refinement only) for better prompt control while maintaining significant performance gains.\n\n**Optimized for Lower Hardware Requirements**: The Wan 2.2 + Lightx2v V2 combination reduces VRAM usage and computational cost through LCM sampling with low CFG settings, making high-quality video generation more accessible.\n\n**Professional Quality Output**: Maintains the high-quality video generation and motion coherence of the original Wan 2.2 despite the dramatic speed improvements through advanced Lightx2v V2 distillation technology.\n\n## How to Use Wan 2.2 + Lightx2v V2 in ComfyUI\n\n### **Text-to-Video Generation Workflow**\n\n**Configure your Wan 2.2 + Lightx2v V2 video parameters**\n* Use the **EmptyHunyuanLatentVideo** node to set your video dimensions:\n  * Width: 704 pixels (optimized for Wan 2.2 + Lightx2v V2)\n  * Height: 544 pixels \n  * Length: 81 frames for ~3.4 second videos at 24fps\n  * Batch_size: Keep at 1 for optimal Wan 2.2 + Lightx2v V2 performance\n\n**Craft your text prompt for Wan 2.2 + Lightx2v V2**\n* In the **CLIP Text Encode (Positive Prompt)** node:\n  * Write detailed, descriptive prompts for best results\n  * Include motion descriptions and camera movements\n  * Specify lighting, mood, and visual style preferences for your Wan 2.2 + Lightx2v V2 generation\n\n**Essential Wan 2.2 + Lightx2v V2 KSampler Settings (Flexible Configuration)**\n* **High Noise KSampler (Wan 2.2 Foundation Stage)**:\n  * **Steps**: 4-7 (adjust based on Wan 2.2 + Lightx2v V2 speed vs quality needs)\n  * **CFG**: 1.0-3.5 (lower for speed, higher for prompt control in Wan 2.2 + Lightx2v V2)\n  * **Sampler**: lcm (for Lightx2v V2 acceleration) or euler (for quality retention)\n  * **Scheduler**: simple\n* **Low Noise KSampler (Wan 2.2 + Lightx2v V2 Refinement Stage)**:\n  * **Steps**: 8-14 total (adjust based on desired Wan 2.2 + Lightx2v V2 quality level)\n  * **CFG**: 1.0-3.5 (coordinate with foundation stage settings)\n  * **Sampler**: lcm (recommended for Lightx2v V2 refinement acceleration)\n  * **Scheduler**: simple\n  * **Important**: Set the High Noise KSampler's end_at_step and Low Noise KSampler's start_at_step to half your total steps (e.g., both set to 4 for 8 total steps, or 6 for 12 total steps) to ensure proper Wan 2.2 + Lightx2v V2 stage handoff.\n\n**Wan 2.2 + Lightx2v V2 Parameter Guidelines**\n* **Total Steps**: 8-14 steps work well for Wan 2.2 + Lightx2v V2 - adjust based on your speed vs quality priorities\n* **CFG Values**: 1.0-3.5 range for Wan 2.2 + Lightx2v V2 - lower values for faster generation, higher for better prompt following\n* **Mix and Match**: Combine different Wan 2.2 + Lightx2v V2 settings to find your optimal balance\n\n### **Universal LoRA Acceleration Settings (Both T2V and I2V)**\n\n**Wan 2.2 + Lightx2v V2 LoRA Configuration Options**\n* **LoraLoaderModelOnly** nodes control the Wan 2.2 + Lightx2v V2 acceleration level:\n  * **T2V LoRA**: `lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank64_bf16.safetensors`\n  * **I2V LoRA**: `lightx2v_I2V_14B_480p_cfg_step_distill_rank128_bf16.safetensors`\n  * **Full Acceleration**: Apply Lightx2v V2 LoRA to both high-noise (foundation) and low-noise (refinement) models with strength 1.0\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1254/readme01.webp\" alt=\"Wan 2.2 + Lightx2v V2\" width=\"750\"/>\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1254/readme07.webp\" alt=\"Wan 2.2 + Lightx2v V2\" width=\"450\"/>\n  * **Half Acceleration**: Apply Lightx2v V2 LoRA only to the low-noise (refinement) model with strength 1.0 - better prompt control for Wan 2.2 + Lightx2v V2\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1254/readme02.webp\" alt=\"Wan 2.2 + Lightx2v V2\" width=\"750\"/>\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1254/readme05.webp\" alt=\"Wan 2.2 + Lightx2v V2\" width=\"300\"/>\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1254/readme06.webp\" alt=\"Wan 2.2 + Lightx2v V2\" width=\"300\"/>\n\n**Wan 2.2 + Lightx2v V2 Speed vs Quality Comparison**\n* **Maximum Speed Setup**: LCM sampler, CFG 1.0, 8 steps total, Lightx2v V2 LoRA on both Wan 2.2 stages\n* **Balanced Setup**: Mixed samplers, CFG 1.0-3.5, 12 steps total, Lightx2v V2 LoRA on refinement only for Wan 2.2  \n* **Quality Priority Setup**: Euler + LCM, CFG 3.5, 14 steps total, selective Lightx2v V2 usage with Wan 2.2\n* **Custom Configuration**: Adjust steps (8-14), CFG (1.0-3.5), and Lightx2v V2 LoRA placement based on your specific Wan 2.2 + Lightx2v V2 needs\n\n### **Image-to-Video Generation Workflow**\n\n**Upload your input image for Wan 2.2 + Lightx2v V2**\n* Use the **LoadImage** node to import your starting frame\n\n**Configure your Wan 2.2 + Lightx2v V2 image-to-video parameters**\n* The I2V workflow shares the same KSampler configurations and Lightx2v V2 acceleration options\n\n## Acknowledgement\n\nThis Wan 2.2 + Lightx2v V2 ComfyUI workflow integrates Alibaba's Wan 2.2 foundation model with the innovative Lightx2v V2 LoRA distillation technology. Special recognition to Alibaba's Tongyi Wanxiang research team for developing the Wan 2.2 model series and to the community developers who created the Lightx2v V2 acceleration techniques. The Wan 2.2 + Lightx2v V2 workflow demonstrates how advanced distillation methods can dramatically improve generation speed while preserving visual quality for professional applications.\n\n## More Resources About Wan 2.2 + Lightx2v V2\n\nExplore additional Wan 2.2 + Lightx2v V2 resources and documentation:\n* **HuggingFace Model Hub** – Pre-trained Wan 2.2 model weights and Lightx2v V2 LoRA files. [Wan 2.2 Models](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged)\n* **Lightx2v V2 LoRA Files** – Distillation LoRA models for ultra-fast Wan 2.2 + Lightx2v V2 generation. [Lightx2v Collection](https://huggingface.co/lightx2v)\n\n"
    },
    {
        "id": "1255",
        "readme": "## What is the Flux1-Krea-dev ComfyUI Workflow?\n\nThe Flux1-Krea-dev ComfyUI workflow brings you the most advanced open-source **Flux Krea Dev** model available today, developed through a collaboration between Black Forest Labs and Krea. Think of it as having a professional photographer's eye combined with an artist's aesthetic sense - this **Flux Krea Dev** model doesn't just generate images, it creates visually stunning artwork that avoids the typical \"AI look\" that plagues many generation models.\n\nThis **Flux Krea Dev** workflow leverages the cutting-edge Flux.1-Krea-dev architecture, specifically engineered to produce images with exceptional realism and natural detail representation. What makes this **Flux Krea Dev** model special? It maintains perfect highlight control, delivers outstanding image quality, and provides a unique aesthetic style that sets it apart from standard **Flux** implementations. The **Flux Krea Dev** technology represents a breakthrough in AI image generation, combining Flux's robust foundation with Krea's artistic refinements.\n\n## Key Features and Benefits of Flux1-Krea-dev\n\n**Superior Aesthetic Quality**: This **Flux Krea Dev** model creates images with exceptional realism and distinctive aesthetics that avoid the typical \"AI look,\" making your **Flux Krea Dev** creations more authentic and professional.\n\n**Fully Compatible Architecture**: Built on the proven **Flux** foundation, **Flux Krea Dev** maintains full compatibility with existing Flux.1 [dev] workflows while delivering enhanced performance.\n\n**Professional-Grade Output**: Perfect for commercial projects and professional presentations where **Flux Krea Dev** image quality matters most.\n\n## How to Use Flux1-Krea-dev in ComfyUI\n\nThis **Flux Krea Dev** workflow is designed for simplicity - just modify your prompt and generate stunning **Flux Krea Dev** images!\n\n### **Write your Flux Krea Dev prompt**\n* In the **CLIP Text Encode (Prompt)** node, replace the default text with your creative description\n* This **Flux Krea Dev** model excels with detailed, realistic prompts that leverage the **Flux Krea Dev** aesthetic capabilities\n* Include specific details about subjects, lighting, composition, and style for optimal **Flux Krea Dev** output\n* The **Flux Krea Dev** model responds particularly well to artistic and cinematic descriptions\n\n### **Optional: Adjust image size**\n* The **EmptySD3LatentImage** node is preset to 1024x1024 pixels for optimal **Flux Krea Dev** performance\n* Change width/height if you need different **Flux Krea Dev** output dimensions\n* **Flux Krea Dev** works best with standard aspect ratios like 1:1, 16:9, or 4:3\n\n### **Generate your Flux Krea Dev image**\n* Hit `Run` or use `Ctrl+Enter` to start **Flux Krea Dev** generation\n* **Flux Krea Dev** images save automatically with the prefix \"flux_krea\"\n* Generation time varies based on complexity, but **Flux Krea Dev** typically delivers results quickly\n\n## Acknowledgement\n\nThis **Flux Krea Dev** workflow integrates the Flux1-Krea-dev model, a breakthrough collaboration between Black Forest Labs and Krea. Special recognition to the Black Forest Labs team for developing this advanced **Flux** architecture and to Krea for their artistic vision in fine-tuning this **Flux Krea Dev** model. The **Flux Krea Dev** model weights and implementation follow the official Flux.1-Krea-dev specifications, ensuring authentic **Flux Krea Dev** performance for professional applications.\n\nThe **Flux Krea Dev** project represents a significant advancement in open-source AI image generation, combining technical excellence with artistic sophistication.\n\n## More Resources About Flux1-Krea-dev\n\nExplore technical resources and documentation related to **Flux Krea Dev**:\n* **Official Announcement** – Black Forest Labs announcement about **Flux Krea Dev** collaboration and features. [BFL Flux1-Krea-dev](https://bfl.ai/announcements/flux-1-krea-dev)\n* **HuggingFace Repository** – Official **Flux Krea Dev** model weights and documentation. [black-forest-labs/FLUX.1-Krea-dev](https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev)\n* **ComfyUI Blog** – Detailed guide on **Flux Krea Dev** integration and usage. [Flux1-Krea-dev on ComfyUI](https://blog.comfy.org/p/flux1-krea-dev-lands-on-comfyui-on)"
    },
    {
        "id": "1256",
        "readme": "## What is the Put It Here Kontext ComfyUI Workflow?\n\nThe Put It Here Kontext ComfyUI workflow brings intelligent object placement to your image editing arsenal, enabling seamless integration of any white-background object into realistic scenes. Think of it as having a smart placement assistant that understands lighting, shadows, and perspective - automatically making your objects look like they naturally belong in their new environment.\n\nThis Put It Here Kontext workflow leverages advanced AI models specifically designed for contextual object insertion. What makes Put It Here Kontext special? It doesn't just paste objects randomly - it understands spatial relationships, surface interactions, and environmental lighting, making Put It Here Kontext perfect for e-commerce photography, product visualization, and creative compositing where realism matters.\n\n## Key Features and Benefits of Put It Here Kontext\n\n**Multiple Operation Modes**: Two core Put It Here Kontext workflows - single image mode for quick placement and background+foreground mode for advanced positioning control with Put It Here Kontext technology.\n\n**Intelligent Placement**: Put It Here Kontext maintains realistic lighting and shadow integration for natural-looking object placement results.\n\n**Interactive Positioning**: Manual positioning control through canvas interface for precise Put It Here Kontext object placement and adjustment capabilities.\n\n**Professional Quality Output**: Put It Here Kontext delivers studio-quality compositing results suitable for commercial applications and professional workflows.\n\n## How to Use Put It Here Kontext in ComfyUI\n\n### **Put It Here Kontext Single Image Workflow**\n* **Upload your composite image**: Use the **Load Image** node to upload an image that already contains your white-background object overlaid on the target scene for Put It Here Kontext processing\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1256/readme01.webp\" alt=\"Put It Here Kontext\" width=\"550\"/>\n* **Switch to Single Image mode**: In the **Set_Single image** node, change the Constant value to \"Single image\" to activate Put It Here Kontext single image processing\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1256/readme02.webp\" alt=\"Put It Here Kontext\" width=\"550\"/>\n* **Add your Put It Here Kontext prompt**: In the **Text** node, enter \"put it here\" + description of the desired final appearance:\n  * Example: \"put it here, a black sports watch naturally resting on a wrist among tropical plants\"\n  * Keep prompts simple and descriptive for best Put It Here Kontext results\n* **Generate seamless integration**: Hit `Run` and Put It Here Kontext will blend the white-background object naturally into the scene with realistic lighting\n\n### **Put It Here Kontext Background + Foreground Workflow**\nThis advanced Put It Here Kontext mode requires a two-step process for maximum control over object placement:\n\n**Step 1: Upload and Initial Put It Here Kontext Processing**\n* **Load background scene**: Upload your target image using the **Load Image** node in the \"Background Image\" group for Put It Here Kontext processing\n* **Load foreground object**: Upload your white-background object using the **Load Image** node in the \"Foreground\" group for Put It Here Kontext integration\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1256/readme04.webp\" alt=\"Put It Here Kontext\" width=\"750\"/>\n* **Switch to Multi Image mode**: In the **Set_Single image** node, change the Constant value to \"Multi image\" to enable Put It Here Kontext advanced mode\n* **Run first processing**: Execute the workflow - Put It Here Kontext will generate an initial placement preview with contextual analysis\n\n**Step 2: Position and Finalize Put It Here Kontext Results**\n* **Crop dialog interaction**: When the crop dialog appears, click \"Apply\" to proceed with Put It Here Kontext positioning\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1256/readme05.webp\" alt=\"Put It Here Kontext\" width=\"550\"/>\n* **Adjust object position**: Use the canvas interface to drag your object to the exact desired location for optimal Put It Here Kontext placement\n* **Lock position**: Click the lock icon on the image upload area to secure your positioning for Put It Here Kontext processing\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1256/readme06.webp\" alt=\"Put It Here Kontext\" width=\"550\"/>\n* **Final processing**: Run the Put It Here Kontext workflow again for the final, positioned result with full contextual integration\n\n### **Put It Here Kontext Image Preparation Tools**\n* **Background Removal**: Use the **Load Image** node in the \"Remove Background\" section to upload any image for Put It Here Kontext preparation\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1256/readme07.webp\" alt=\"Put It Here Kontext\" width=\"650\"/>\n* **Automatic processing**: The **Image Rembg (Remove Background)** node processes your image using the u2net model for clean background removal compatible with Put It Here Kontext\n* **White background generation**: Creates perfect white-background images ready for Put It Here Kontext workflows\n\n**Essential Settings for Put It Here Kontext**\n* **Model Configuration for Put It Here Kontext**:\n  * Choose either **UNet** or **Nunchaku** models - use one and disable the other for optimal Put It Here Kontext performance\n  * Select appropriate mode: \"Single Image\" or \"Multiple Images\" based on your Put It Here Kontext workflow requirements\n* **Put It Here Kontext Quality Settings**:\n  * **Scale Image to Total Pixels**: Increase megapixels value for higher resolution Put It Here Kontext output\n  * **Upscale method**: Lanczos provides optimal quality for Put It Here Kontext processing results\n* **Object Requirements for Put It Here Kontext**:\n  * White background with clean white borders essential for Put It Here Kontext accuracy\n  * For transparent objects: Add \"transparent\" keyword to your Put It Here Kontext prompt for better integration\n\n## Acknowledgement\n\nThis Put It Here Kontext ComfyUI workflow integrates the innovative \"Put It Here\" contextual object placement model. Special recognition to the original developers for creating this breakthrough approach to intelligent object insertion and to the ComfyUI community for enabling seamless Put It Here Kontext integration across different workflow configurations.\n\n## More Resources About Put It Here Kontext\n\nExplore technical resources and community content related to Put It Here Kontext development and implementation:\n\n* **Model Repository**: [Put It Here Kontext v0.1](https://civitai.com/models/1791091/put-it-herekontextv01nunchaku?modelVersionId=2026901)\n\n"
    },
    {
        "id": "1257",
        "readme": "## What is the Flux Kontext Pulid ComfyUI Workflow?\n\n**Flux Kontext Pulid** is a reference-based character generation workflow inside **ComfyUI** that combines the **FLUX Diffusion Transformer (DiT)** with the **Kontext Adapter** and **Pulid identity control**. This Flux Kontext Pulid setup allows creators to generate high-fidelity character images from a single face reference while maintaining identity consistency across styles, poses, and scenes.\n\nWhether you're building stylized character renders or testing identity stability under prompt variation, **Flux Kontext Pulid** delivers creative flexibility and structure-aware consistency—all powered by cutting-edge DiT architecture within the Flux Kontext Pulid workflow.\n\n## Key Features and Benefits of Flux Kontext Pulid\n\n**Single Reference Identity Control**: Use one face image to guide all Flux Kontext Pulid generations\n\n**Kontext Attention Adapter**: Context-aware personalization using prompt + visual alignment in Flux Kontext Pulid\n\n**Pulid Nodes**: Precise identity preservation through weight and timeline control in Flux Kontext Pulid\n\n**DiT Transformer Base**: Built on FLUX for high-fidelity and expressive character output in Flux Kontext Pulid\n\n**Compatible with LoRAs and Stylized Prompts**: Great for anime, realism, concept design, or portrait workflows with Flux Kontext Pulid\n\nThis Flux Kontext Pulid workflow is especially useful for AI artists looking to test facial consistency across multiple renders without retraining.\n\n## How to Use Flux Kontext Pulid in ComfyUI\n\n### **Main Group**\n<p align=\"left\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1257/readme02.webp\" alt=\"Flux Kontext Pulid\" width=\"550\"/>\n</p>\n\nThis is the **Main Sampling Group** that controls your base image generation using the reference and prompt in Flux Kontext Pulid.\n\n**Load Image Here**\n<p align=\"left\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1257/readme03.webp\" alt=\"Flux Kontext Pulid\" width=\"550\"/>\n</p>\n\n* Upload a **face reference image** into the `Load Image` node for Flux Kontext Pulid\n* This image drives the Pulid identity encoding and DiT personalization in Flux Kontext Pulid\n\n**Enter Prompts Here**\n<p align=\"left\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1257/readme04.webp\" alt=\"Flux Kontext Pulid\" width=\"550\"/>\n</p>\n\n* Add your **positive and negative prompts** to shape the pose, scene, clothing, and expression\n* Use descriptive language like `\"a female character in a red coat, standing in snowfall\"`  \n* Add negatives like `\"extra hands, distorted face, blurry\"`\n\nYou can also adjust the **Sampler settings** here:\n* Change the sampler type (Euler, DPM++, etc.)\n* Modify steps, CFG scale, resolution, or noise schedule\n* This gives you more control over image fidelity and style response\n\n**Pulid Control**\n<p align=\"left\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1257/readme05.webp\" alt=\"Flux Kontext Pulid\" width=\"550\"/>\n</p>\n\nBoth this group and the Highres group have **Pulid control nodes** for Flux Kontext Pulid.  \nHere, you can set:\n* `Weight`: Controls how strongly the identity is preserved in Flux Kontext Pulid\n* `Start At` and `End At`: Determines when identity influence is applied during Flux Kontext Pulid sampling\n\nThis allows you to fade or blend identity effects across the Flux Kontext Pulid sampling timeline.\n\n### **Highres Group**\n<p align=\"left\">\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1257/readme06.webp\" alt=\"Flux Kontext Pulid\" width=\"550\"/>\n</p>\n\nThis group handles **upscaling** and **final detail enhancement** of the base image.\n\n* The **Redux Sampler** in this group performs second-pass generation at higher resolution\n* Adjust sampler settings (steps, sampler, resolution) here to fine-tune upscale quality\n* You can scale the image output (e.g., 1.5x, 2x) based on memory and sharpness needs\n* The final output is saved automatically to your `ComfyUI > output` folder\n\nMake sure to mirror Pulid weights in this group to maintain character identity during upscale.\n\n## Acknowledgement\n\nThis Flux Kontext Pulid ComfyUI workflow integrates FLUX Diffusion Transformer technology with Kontext and Pulid identity systems. Special thanks to the original developers and the ComfyUI community for making this workflow accessible.\n\n## More Resources About Flux Kontext Pulid\n\nExplore technical resources and documentation related to Flux Kontext Pulid:\n* **Original Author** – Complete implementation and usage examples. [Flux Kontext Pulid Workflow](https://openart.ai/workflows/aitold/flux-kontext-pulid/KONIFxqdDS98VCHNBoc6)\n"
    },
    {
        "id": "1258",
        "readme": "## What is the Wan 2.2 Image Generation ComfyUI Workflow?\n\nThe Wan 2.2 Image Generation workflow brings you the power of Alibaba's breakthrough video foundation model, specifically adapted for creating stunning photorealistic images. Think of Wan 2.2 Image Generation as having a Hollywood-level cinematographer in your pocket. This isn't just another image generator, it's a model originally designed for high-end video production that excels at understanding lighting, composition, and realistic human features.\n\nThis complete Wan 2.2 Image Generation workflow pack gives you two distinct approaches in one file: MoE Mix-Mode for absolute maximum quality using dual model architecture, and Low-Only Mode with integrated upscaling for faster generation. Wan 2.2's video heritage means it understands temporal consistency and realistic motion blur effects, creating images with cinematic depth that traditional image models often miss.\n\n## Key Features and Benefits of Wan 2.2 Image Generation\n\n**Dual Workflow Architecture**: Choose between MoE Mix-Mode for ultimate quality or Low-Only Mode with built-in 4x Ultrasharp upscaling for your Wan 2.2 Image Generation needs.\n\n**Cinematic Quality**: Wan 2.2's video model foundation delivers exceptional photorealism with professional lighting and natural textures in every image generation.\n\n**Flexible VRAM Options**: Supports both full precision models and GGUF quantized versions for systems with less than 24GB VRAM for seamless Wan 2.2 Image Generation.\n\n**Professional Portrait Excellence**: Wan 2.2 Image Generation excels at human subjects with Hollywood-quality portraits and sophisticated lighting effects.\n\n## How to Use Wan 2.2 Image Generation in ComfyUI\n\n### Wan 2.2 Image Generation MoE Mix-Mode Workflow\n* **Enable the upper workflow group**\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1258/readme01.webp\" alt=\"Wan 2.2 Image Generation\" width=\"750\"/>\n  * This is the \"MoE Mix-Mode\" section in your Wan 2.2 Image Generation workflow\n  * Disable the \"Low-Only Mode\" group by right-clicking and selecting \"Bypass\"\n* **Configure your image dimensions**\n  * Use the **Empty Latent by Ratio (WLSH)** node to set your Wan 2.2 Image Generation output size\n* **Write your Wan 2.2 Image Generation prompt**\n  * In the **CLIP Text Encode (Positive Prompt)** node:\n    * Wan 2.2 Image Generation responds exceptionally well to cinematic descriptions\n    * Include lighting details, camera angles, and environmental context for best Wan 2.2 Image Generation results\n* **Advanced Wan 2.2 Image Generation settings**\n  * **KSamplerAdvanced** nodes use dual-stage generation:\n    * First stage: 4 steps with high-noise Wan 2.2 model\n    * Second stage: 4 steps with low-noise Wan 2.2 model for refinement\n  * **LoraLoaderModelOnly** applies LightX2V enhancement at different strengths for each Wan 2.2 Image Generation stage\n\n### Wan 2.2 Image Generation Low-Only Mode Workflow\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1258/readme02.webp\" alt=\"Wan 2.2 Image Generation\" width=\"750\"/>\n* **Enable the lower workflow group**\n  * This is the \"Low-Only Mode\" section in your Wan 2.2 Image Generation workflow\n  * Disable the \"MoE Mix-Mode\" group above\n* **Select your Wan 2.2 Image Generation model version**\n  * **UNETLoader** for full precision Wan 2.2 models (24GB+ VRAM)\n  * **UnetLoaderGGUF** for quantized Wan 2.2 versions (less VRAM)\n* **Configure generation parameters**\n  * **KSamplerAdvanced**: 40 steps at CFG 1.3 for optimal Wan 2.2 Image Generation quality\n  * **EmptyHunyuanLatentVideo**: Set to 1024x1024 for Wan 2.2 Image Generation base resolution\n* **Built-in Wan 2.2 Image Generation upscaling pipeline**\n  * **ImageUpscaleWithModel** uses 4x Ultrasharp for initial upscaling\n  * **ImageScaleBy** reduces to 0.5x for manageable final size\n  * **KSampler** refinement pass at 15 steps with 0.2 denoise strength\n\n### Essential Wan 2.2 Image Generation Settings\n* **Scheduler**: Use \"beta\" for MoE Mix-Mode, \"simple\" for Low-Only Mode\n* **Sampler**: \"euler\" works best across both Wan 2.2 Image Generation configurations\n* **CFG Values**: 1.0 for MoE Mix-Mode, 1.3-2.5 for Low-Only Mode\n* **LoRA Strength**: Smartphone Snapshot Photo Reality at 0.3 strength enhances Wan 2.2 Image Generation realism\n\n## Acknowledgement\n\nThis Wan 2.2 Image Generation ComfyUI workflow adapts Alibaba's Wan 2.2 video foundation model (developed by Tongyi Lab) for image creation. Special recognition to the original workflow creators who developed both the MoE Mix-Mode and Low-Only Mode approaches, and to the ComfyUI community for enabling this creative adaptation of Wan 2.2 for image generation use cases.\n\n## More Resources About Wan 2.2 Image Generation\nExplore technical resources and community discussions for Wan 2.2 Image Generation:\n* MoE Mix-Mode Workflow Source – Wan 2.2 Image Generation workflow implementation and community examples [Wan 2.2 Workflow](https://openart.ai/workflows/onion/wan-22-image-generation/PBUio70nggOW8dg3Ajys)\n* Low-Only Mode Workflow Source – Community feedback and results showcasing Wan 2.2 Image Generation's human image generation capabilities [Community Discussion](https://www.reddit.com/r/StableDiffusion/comments/1mcm7qm/wan_22_human_image_generation_is_very_good_this/)\n\n\n"
    },
    {
        "id": "1259",
        "readme": "## What is the Wan 2.2 FLF2V ComfyUI Workflow?\n\nThe **Wan 2.2 FLF2V** ComfyUI workflow brings next-generation first-last frame video generation to your fingertips, using the advanced **Wan 2.2 FLF2V** model for seamless frame interpolation. This **Wan 2.2 FLF2V** workflow provides a fast, efficient way to create visually coherent videos from just two input images.\n\nCompared to Wan 2.1, the **Wan 2.2 FLF2V** model brings improved fidelity, smoother interpolation, and better control—while maintaining the intuitive ease of use that makes **Wan 2.2 FLF2V** perfect for creators of all skill levels. Simply upload two images and let the **Wan 2.2 FLF2V** model intelligently fill in the frames between.\n\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1259/readme01.webp\" alt=\"Wan 2.2 FLF2V\" width=\"750\"/>\n\n\n## Key Features and Benefits of Wan 2.2 FLF2V\n\n**Upgrade from Wan 2.1 FLF2V**: **Wan 2.2 FLF2V** delivers improved visual fidelity, motion coherence, and faster runtime for enhanced video generation performance.\n\n**Start-to-End Frame Video**: **Wan 2.2 FLF2V** auto-generates intermediate frames between two input images, creating seamless transitions with professional quality.\n\n**Simple, Intuitive Workflow**: Just upload two images and go with **Wan 2.2 FLF2V** - minimal setup required for maximum creative impact.\n\n**Optimized for ComfyUI**: **Wan 2.2 FLF2V** offers fully native support with minimal setup, ensuring smooth integration and reliable performance.\n\n**720p Video Generation**: **Wan 2.2 FLF2V** produces high-definition outputs with fluid scene transitions, perfect for professional and creative projects.\n\nWhether you're designing animated sequences, prototyping transitions, or visualizing ideas—**Wan 2.2 FLF2V** gives you cinematic results from minimal inputs.\n\n## How to Use Wan 2.2 FLF2V in ComfyUI\n\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1259/readme02.webp\" alt=\"Wan 2.2 FLF2V\" width=\"550\"/>\n\n### **Load Start and End Images**\n* Use the **first Load Image node** to upload your **starting frame** for **Wan 2.2 FLF2V** processing\n* Use the **second Load Image node** to upload your **ending frame** for **Wan 2.2 FLF2V** interpolation\n* These two images define the visual start and end of your **Wan 2.2 FLF2V** generated video\n\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1259/readme03.webp\" alt=\"Wan 2.2 FLF2V\" width=\"550\"/>\n\n### **Set Prompts (Optional)**\n* Write optional **positive** and **negative prompts** to guide the **Wan 2.2 FLF2V** visual style or motion behavior\n* **Positive Prompt Example**: `\"smooth camera motion, cinematic lighting, vibrant colors\"`\n* **Negative Prompt Example**: `\"blurry, distorted, low quality, jerky\"`\n* Prompts help influence how **Wan 2.2 FLF2V** handles scene transitions, but they're optional for basic **Wan 2.2 FLF2V** operation\n\n### **Adjust Video Resolution**\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1259/readme04.webp\" alt=\"Wan 2.2 FLF2V\" width=\"550\"/>\n\n* In the `WanFirstLastFrameToVideo` node, adjust the **width and height** for **Wan 2.2 FLF2V** output\n* By default, the **Wan 2.2 FLF2V** resolution is set to a **lower size** for performance optimization\n* For high-quality **Wan 2.2 FLF2V** output (if you have sufficient VRAM), set resolution to around **720×1280**\n* For faster **Wan 2.2 FLF2V** testing, use a smaller size like **480×854**\n\n> Note: Higher resolution gives cleaner **Wan 2.2 FLF2V** transitions but may consume more GPU memory.\n\n### **Generate the Video**\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1259/readme05.webp\" alt=\"Wan 2.2 FLF2V\" width=\"550\"/>\n\n* Click the **Run** button in ComfyUI to execute the **Wan 2.2 FLF2V** workflow\n* Or press **Ctrl (Cmd) + Enter** to start **Wan 2.2 FLF2V** processing\n* The **Wan 2.2 FLF2V** model will interpolate and generate the full video sequence between the two frames\n* Final **Wan 2.2 FLF2V** output will be saved to your ComfyUI `/output` folder using the video save node\n\n## Models\n\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1259/readme06.webp\" alt=\"Wan 2.2 FLF2V\" width=\"550\"/>\n\nModels are already pre-loaded in the **Wan 2.2 FLF2V** workflow, no need to manually download.\nPlease wait 2-3 mins for **Wan 2.2 FLF2V** models to auto-download for the first time.\n\n**Diffusion Model**\n* [wan2.2_i2v_high_noise_14B_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_i2v_high_noise_14B_fp16.safetensors)\n* [wan2.2_i2v_low_noise_14B_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_i2v_low_noise_14B_fp16.safetensors)\n\n**VAE**\n* [wan_2.1_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)\n\n**Text Encoder**\n* [umt5_xxl_fp8_e4m3fn_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)\n\nFile save location\n\n```\nComfyUI/\n├───📂 models/\n│   ├───📂 diffusion_models/\n│   │   ├─── wan2.2_i2v_low_noise_14B_fp16.safetensors\n│   │   └─── wan2.2_i2v_high_noise_14B_fp16.safetensors\n│   ├───📂 text_encoders/\n│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors \n│   └───📂 vae/\n│       └── wan_2.1_vae.safetensors\n```\n\n## Acknowledgement\n\nThe **Wan 2.2 FLF2V** model and ComfyUI workflow are developed by the **ComfyUI team**, extending on the work from **Wanxiang Alibaba**. This **Wan 2.2 FLF2V** version brings native integration, improved speed, and higher quality interpolation directly inside ComfyUI.\n\nThe modular design and native support make **Wan 2.2 FLF2V** an ideal choice for animators, artists, and creators looking for streamlined two-frame-to-video generation workflows with **Wan 2.2 FLF2V** technology.\n\n## More Resources About Wan 2.2 FLF2V\n\nExplore technical resources and documentation related to **Wan 2.2 FLF2V**:\n* **Blog** – Official **Wan 2.2 FLF2V** implementation and ComfyUI integration guide. [Wan 2.2 FLF2V ComfyUI Native Support](https://blog.comfy.org/p/wan22-flf2v-comfyui-native-support)\n"
    },
    {
        "id": "1260",
        "readme": "## What is the Qwen Image ComfyUI Workflow?\n\nThe Qwen Image ComfyUI workflow brings cutting-edge text-to-image generation to your creative toolkit, powered by a revolutionary 20B parameter MMDiT foundation model. Think of it as your personal design studio that doesn't just add text to images - it architecturally weaves typography into the visual DNA of every creation. Whether you're crafting magazine covers, brand posters, or marketing materials, Qwen Image transforms your text prompts into professionally designed visuals where every character feels like it belongs.\n\nThis Qwen Image workflow excels at what traditional AI image generators struggle with most: high-fidelity text rendering that maintains perfect clarity, layout coherence, and contextual harmony. From complex Chinese characters to elaborate English typography, Qwen Image preserves every typographic detail while seamlessly integrating text elements into the broader visual composition.\n\n## Key Features and Benefits of Qwen Image\n\n**Professional Text Integration**: Qwen Image delivers state-of-the-art text rendering where typography becomes an integral part of the design, maintaining perfect clarity and contextual harmony across diverse visual styles.\n\n**Multilingual Excellence**: Whether working with English headlines or Chinese characters, Qwen Image preserves typographic precision and cultural authenticity, ideal for global brand campaigns and multilingual content.\n\n**Versatile Design Applications**: From photorealistic product shots to stylized illustrations, Qwen Image adapts fluidly across artistic styles while maintaining consistent text quality for any creative project.\n\n**Advanced Creative Control**: The Qwen Image workflow includes sophisticated sampling controls and optimization features that let you fine-tune generation quality, balancing speed with visual fidelity.\n\n## How to Use Qwen Image in ComfyUI\n\n### **Setting Up Your Qwen Image Generation**\n* **Configure image dimensions**: Use the **EmptySD3LatentImage** node to set your output size:\n  * Standard: 1328x1328 pixels for square formats\n  * Adjust width and height based on your Qwen Image project needs\n  * Keep batch_size at 1 for focused generation \n* **Craft your text prompt**: In the **CLIP Text Encode (Positive Prompt)** node:\n  * Write detailed descriptions that include specific text elements you want rendered\n  * Qwen Image excels with prompts that specify typography styles, layouts, and text placement\n  * Include contextual details about the visual environment surrounding your text\n\n### **Essential Qwen Image Workflow Steps**\n* **Optimize model performance**: The **ModelSamplingAuraFlow** node fine-tunes the Qwen Image generation:\n  * **Shift value**: Set to 3.1 (increase for sharper details, decrease for softer results)\n  * This parameter significantly affects the quality balance in your Qwen Image output\n* **Configure sampling settings**: In the **KSampler** node for optimal Qwen Image results:\n  * **Steps**: 20 (official recommendation is 50, but 20 provides excellent quality-speed balance)\n  * **CFG**: 2.5 (set to 1.0 for speed boost with slightly reduced consistency)\n  * **Sampler**: euler (recommended for Qwen Image stability)\n  * **Scheduler**: simple (optimal for this Qwen Image workflow)\n\n### **Advanced Qwen Image Optimization**\n* **Quality vs Speed Balance**: For faster Qwen Image generation, reduce steps to 10-15 and set CFG to 1.0, especially when using alternative samplers like res_multistep\n* **Enhanced Detail Control**: Increase the shift value in ModelSamplingAuraFlow if your Qwen Image outputs appear blurry or lack definition\n* **Memory Management**: This Qwen Image workflow requires approximately 86% VRAM on RTX4090 24GB, with generation times of ~94 seconds for first run, ~71 seconds for subsequent generations\n\n## Acknowledgement\n\nThis Qwen Image ComfyUI workflow integrates the groundbreaking 20B parameter MMDiT model developed by the Qwen team at Alibaba Cloud. Special recognition goes to the Qwen research team for pioneering advanced text rendering capabilities in AI image generation, and to the ComfyUI community for enabling seamless Qwen Image integration. The Qwen Image model represents a significant leap forward in text-aware image synthesis, bringing professional-grade typography generation to accessible workflows.\n\n## More Resources About Qwen Image\nExplore comprehensive resources for mastering Qwen Image:\n* **Official Model Repository** – Access Qwen Image model weights and technical documentation. [Qwen/Qwen-Image](https://huggingface.co/Qwen/Qwen-Image)\n* **ComfyUI Integration Guide** – Complete tutorial for Qwen Image workflow setup and optimization. [Qwen Image Tutorial](https://docs.comfy.org/tutorials/image/qwen/qwen-image)\n\n"
    },
    {
        "id": "1261",
        "readme": "## What is the Wan 2.2 Low VRAM Kijai Wrapper ComfyUI Workflow?\n\nThe Wan 2.2 Low VRAM Kijai Wrapper ComfyUI workflow is a resource-friendly video generation workflow built on top of Kijai's optimized Wan 2.2 Low VRAM implementation. This Wan 2.2 Low VRAM version is tailored for users with limited VRAM and supports fast inference using a 2-stage sampler—just 8 total steps (4 for high-noise, 4 for low-noise).\n\nThe Wan 2.2 Low VRAM workflow supports both image-to-video and text-to-video generation, and includes an optional OpenAI-powered prompt assistant for creators who want fast idea-to-output generation with Wan 2.2 Low VRAM.\n\n## Key Features and Benefits of Wan 2.2 Low VRAM\n\n**Low VRAM Usage**: Wan 2.2 Low VRAM is designed for low VRAM usage, even on GPUs with tight memory limits.\n\n**Dual Generation Modes**: Two separate modes for Wan 2.2 Low VRAM - Image-to-Video and Text-to-Video generation capabilities.\n\n**Optimized Performance**: Wan 2.2 Low VRAM uses optimized 2-stage sampling for fast and coherent video generation.\n\n**AI Prompt Assistant**: Built-in OpenAI Chat node helps you write prompts from basic ideas for Wan 2.2 Low VRAM workflows.\n\n**Auto Setup**: Automatic model downloads with no manual setup required for Wan 2.2 Low VRAM.\n\n**Creative Applications**: Ideal for quick animations, transitions, and experimental storytelling using Wan 2.2 Low VRAM.\n\n## How to Use Wan 2.2 Low VRAM in ComfyUI\n\n### **Wan 2.2 Low VRAM Image to Video**\n**Upload your start image**\n* Upload a start image using the Load Image node for Wan 2.2 Low VRAM generation\n* Enter your prompt describing the scene or motion for Wan 2.2 Low VRAM\n* Edit sampler settings if needed (default: 4 steps + 4 steps for Wan 2.2 Low VRAM)\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1261/readme02.webp\" alt=\"Wan 2.2 Low VRAM\" width=\"550\"/>\n* Video output will be saved to your ComfyUI/outputs folder\n\n### **Wan 2.2 Low VRAM Text to Video**\n* **Enable text-to-video mode**\n  * Unmute this group to use Wan 2.2 Low VRAM T2V\n* **Configure generation settings**\n  * Enter a descriptive prompt to generate a video from text using Wan 2.2 Low VRAM\n  * Tune sampler settings and resolution as needed for Wan 2.2 Low VRAM\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1261/readme03.webp\" alt=\"Wan 2.2 Low VRAM\" width=\"550\"/>\n* Generated video is saved in the outputs folder\n\n### **OpenAI Prompt Generator for Wan 2.2 Low VRAM**\n* **Use AI prompt enhancement**\n  * This group contains an OpenAI Chat node for auto prompt generation with Wan 2.2 Low VRAM\n  * Type a basic idea or short concept\n  * The OpenAI node will expand it into a detailed video prompt for Wan 2.2 Low VRAM\n  * Works for both image and text modes\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1261/readme04.webp\" alt=\"Wan 2.2 Low VRAM\" width=\"550\"/>\n\n> ⚠️ *Use of the OpenAI node requires active credits in ComfyUI Settings → User. Payments go directly to ComfyUI.org.*\n\n**Essential Settings for Wan 2.2 Low VRAM**\n* **Model Setup**: All Wan 2.2 Low VRAM models are auto-downloaded when you first run the workflow\n* **Wait Time**: Wait around 2–3 minutes for initial setup\n* **Installation**: No manual installation is required for Wan 2.2 Low VRAM\n\n## Acknowledgement\n\nThis Wan 2.2 Low VRAM ComfyUI workflow brings high-quality video synthesis to low-resource systems. Whether you're working on concept animations, testing scene flows, or building short creative sequences, this Wan 2.2 Low VRAM workflow keeps it fast, light, and reliable.\n\nThanks to Kijai for designing the Wan 2.2 Low VRAM adaptation.\n\n## More Resources About Wan 2.2 Low VRAM\n\n* **GitHub Repository** – Kijai's ComfyUI wrapper for Wan 2.2 Low VRAM implementation: [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)\n"
    },
    {
        "id": "1262",
        "readme": "## What is the Wan 2.2 Lightning ComfyUI Workflow?\n\nThe Wan 2.2 Lightning ComfyUI workflow brings you the fastest video generation experience with the revolutionary 4-step Lightning LoRA models that deliver 20x faster generation while maintaining exceptional quality. This Wan 2.2 Lightning workflow features the latest Lightx2v V2 integration with enhanced OpenAI prompt assistance capabilities.\n\nThis Wan 2.2 Lightning workflow supports both image-to-video and text-to-video generation with automatic 2-stage sampling (4+4 steps total). The Wan 2.2 Lightning models are designed for efficiency, making professional video creation accessible even on moderate hardware setups.\n\n## Key Features and Benefits of Wan 2.2 Lightning\n\n**4-Step Lightning Speed**: Wan 2.2 Lightning delivers video generation in just 4 steps, achieving 20x faster processing compared to traditional methods.\n\n**Advanced Lightx2v V2 Models**: Features cutting-edge Lightx2v V2 architecture with HIGH and LOW noise specialized models for superior quality.\n\n**Comprehensive Generation Suite**: Complete text-to-video and image-to-video capabilities integrated into one powerful Wan 2.2 Lightning workflow.\n\n**Smart Prompt Enhancement**: Advanced OpenAI integration automatically crafts cinematic prompts to maximize Wan 2.2 Lightning output quality.\n\n**Zero-Configuration Setup**: Plug-and-play design with automatic model downloading eliminates manual setup for Wan 2.2 Lightning workflows.\n\n## How to Use Wan 2.2 Lightning in ComfyUI\n\n### **Wan 2.2 Lightning Text-to-Video Workflow**\n* **Configure text prompts**\n  * Enter your creative vision in the **WanVideoTextEncode** node for Wan 2.2 Lightning\n  * Include scene details, motion descriptions, and camera movements for best Wan 2.2 Lightning results\n* **Set up generation parameters**\n  * Use the **WanVideoEmptyEmbeds** node for text-only generation with Wan 2.2 Lightning\n  * Configure steps and sampling settings as needed\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1262/readme01.webp\" alt=\"Wan 2.2 Lightning T2V\" width=\"750\"/>\n* **Run generation**\n  * Execute the Wan 2.2 Lightning workflow to create smooth, high-quality videos from text descriptions\n\n### **Wan 2.2 Lightning Image-to-Video Workflow**\n* **Switch to I2V mode** \n  * Unmute the I2V group to activate Wan 2.2 Lightning image-to-video generation\n  * Mute the T2V group to avoid conflicts\n* **Upload your input image**\n  * Use the **Load Image** node to import your starting frame for Wan 2.2 Lightning video generation\n  * Image will be automatically resized to 832x480 for optimal Wan 2.2 Lightning performance\n* **Configure your prompt**\n  * Enter a detailed description of the motion or scene you want in the **WanVideoTextEncode** node\n  * Wan 2.2 Lightning works best with specific, motion-rich descriptions\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1262/readme02.webp\" alt=\"Wan 2.2 Lightning I2V\" width=\"750\"/>\n* **Adjust generation settings**\n  * **Steps**: 8 total (4 HIGH + 4 LOW for Wan 2.2 Lightning)\n  * **Split Step**: 4 (divides HIGH/LOW noise stages for Wan 2.2 Lightning)\n  * **CFG**: Auto-scheduled from 2 to 2 for Wan 2.2 Lightning optimization\n\n### **OpenAI Prompt Generator for Wan 2.2 Lightning**\n* **AI-powered prompt creation**\n  * Upload an image to the **OpenAI Chat** node for automatic prompt generation with Wan 2.2 Lightning\n  * AI analyzes your image and creates cinematic, motion-rich prompts optimized for Wan 2.2 Lightning\n  * Perfect for users who want professional results without manual prompt crafting\n  <img src=\"https://cdn.runcomfy.net/workflow_assets/1262/readme03.webp\" alt=\"Wan 2.2 Lightning OpenAI\" width=\"750\"/>\n* **Text prompt expansion**\n  * Input basic ideas in the text expansion group for detailed Wan 2.2 Lightning prompts\n  * AI transforms simple concepts into comprehensive video descriptions for Wan 2.2 Lightning\n> ⚠️ *OpenAI features require active credits in ComfyUI Settings → User. Payments processed through ComfyUI.org.*\n\n## Advanced Settings for Wan 2.2 Lightning\n\n**Model Configuration**: Wan 2.2 Lightning automatically loads HIGH and LOW noise Lightx2v V2 models with appropriate block swap settings for optimal memory usage.\n\n**Video Parameters**: Default 832x480 resolution with 81 frames provides the best balance of quality and speed for Wan 2.2 Lightning generation.\n\n**Sampling Strategy**: Wan 2.2 Lightning uses a 2-stage approach - HIGH noise model for initial generation, LOW noise model for refinement and detail enhancement.\n\n## Acknowledgement\n\nThis Wan 2.2 Lightning ComfyUI workflow integrates the latest Lightx2v V2 models with enhanced OpenAI prompt generation capabilities. For comparison with previous versions, see our [Wan 2.1 Lightning workflow](https://www.runcomfy.com/comfyui-workflows/wan-2-2-lightx2v-v2-comfyui-workflow-fast-image-text-to-video).\n\n## More Resources About Wan 2.2 Lightning\n\nExplore technical resources and documentation related to Wan 2.2 Lightning:\n* **HuggingFace Models** – Official Wan 2.2 Lightning LoRA weights and documentation. [lightx2v/Wan2.2-Lightning](https://huggingface.co/lightx2v/Wan2.2-Lightning)\n"
    },
    {
        "id": "1263",
        "readme": "## What is the Qwen-Image-Lightning ComfyUI Workflow?\n\nThe Qwen-Image-Lightning ComfyUI workflow revolutionizes image generation by delivering the exceptional quality of Qwen-Image in just 8 steps instead of the traditional 50. Building on our popular [Qwen-Image ComfyUI HD AI Text Generator](https://www.runcomfy.com/comfyui-workflows/qwen-image-comfyui-hd-ai-text-generator-create-posters), this Qwen-Image-Lightning acceleration cuts generation time in half while preserving the model's legendary text rendering capabilities and image quality. This Qwen-Image-Lightning workflow uses advanced distillation techniques to compress the original model's power into a lightning-fast package.\n\nQwen-Image-Lightning represents a new era in efficient AI image generation, where speed meets precision. Whether you're creating complex Chinese calligraphy, detailed signage, or intricate text-heavy designs, this Qwen-Image-Lightning workflow maintains the original model's excellence while dramatically reducing wait times for creators and professionals. The Qwen-Image-Lightning acceleration makes it perfect for rapid prototyping and high-volume content creation.\n\n## Key Features and Benefits of Qwen-Image-Lightning\n\n**Lightning-Fast Generation**: Qwen-Image-Lightning cuts generation time by 50% with 8-step acceleration technology.\n\n**Preserved Text Excellence**: Qwen-Image-Lightning maintains superior text rendering capabilities from the original model.\n\n**Quality Without Compromise**: Qwen-Image-Lightning delivers comparable visual quality despite the dramatic speed boost.\n\n**Professional Efficiency**: Qwen-Image-Lightning enables rapid iteration and high-volume content creation workflows.\n\n## How to Use Qwen-Image-Lightning in ComfyUI\n### **Set Image Dimensions**\n* In the **EmptySD3LatentImage** node for Qwen-Image-Lightning:\n  * Default: 1488x1488 pixels optimized for Qwen-Image-Lightning\n  * Adjust width and height as needed for your Qwen-Image-Lightning project\n  * Keep batch_size at 1 for optimal Qwen-Image-Lightning performance\n\n### **Craft Your Text Prompts**\n* **CLIP Text Encode (Positive Prompt)** for Qwen-Image-Lightning:\n  * Write detailed prompts for best Qwen-Image-Lightning results\n  * Include specific text elements and visual details that leverage Qwen-Image-Lightning capabilities\n* **CLIP Text Encode (Negative Prompt)** in Qwen-Image-Lightning:\n  * Add negative prompts to avoid unwanted artifacts in your Qwen-Image-Lightning generation\n\n### **Generate Your Image**\n* Hit `Run` to create Qwen-Image-Lightning images in record time\n* Enjoy 50% faster Qwen-Image-Lightning generation with maintained quality\n* Experience the full power of Qwen-Image-Lightning acceleration\n\n## Acknowledgement\nThis Qwen-Image-Lightning ComfyUI workflow integrates acceleration technology from the lightx2v team and Alibaba's Qwen-Image foundation model. Special recognition to the developers who created the Qwen-Image-Lightning LoRA models for professional-quality fast image generation. The Qwen-Image-Lightning implementation represents a significant advancement in efficient AI image synthesis.\n\n## More Resources About Qwen-Image-Lightning\nExplore technical resources and documentation for Qwen-Image-Lightning:\n* **GitHub Repository** – Official Qwen-Image-Lightning implementation and acceleration models: [Qwen-Image-Lightning](https://github.com/ModelTC/Qwen-Image-Lightning/)\n* **HuggingFace Hub** – Pre-trained Qwen-Image-Lightning LoRA models and technical documentation: [lightx2v/Qwen-Image-Lightning](https://huggingface.co/lightx2v/Qwen-Image-Lightning)\n"
    },
    {
        "id": "1265",
        "readme": "## What is the Instagirl Workflow?\n\nThe Instagirl ComfyUI workflow showcases the **Instagirl Wan 2.2** LORA by Instara, seamlessly integrated with the **Wan 2.2** image generation model. It's like having a professional portrait photographer's expertise built directly into your workflow. You get that cinematic, Instagram-worthy aesthetic without the complex setup.\n\nThis **Instagirl Wan 2.2** workflow leverages Wan 2.2's advanced diffusion technology combined with the specialized Instagirl LORA to deliver consistent, high-fidelity portraits. What makes **Instagirl Wan 2.2** special? It's pre-tuned for that polished, professional look that typically requires hours of fine-tuning, but now works right out of the box.\n\n## Key Features and Benefits of Instagirl Wan 2.2\n\n**Pre-integrated LORA**: The **Instagirl Wan 2.2** LORA comes already applied and optimized, eliminating the guesswork of manual integration and strength adjustments.\n\n**Wan 2.2 Foundation**: Built on Wan 2.2's advanced diffusion architecture for exceptional image fidelity and style preservation in every **Instagirl Wan 2.2** generation.\n\n**Instant Setup**: Plug-and-play **Instagirl Wan 2.2** workflow with auto-downloaded models. Just load and start creating professional portraits immediately.\n\n**Creative Control**: Full prompt flexibility while maintaining the signature **Instagirl Wan 2.2** aesthetic, perfect for both photorealistic and stylized results.\n\n## How to Use Instagirl Wan 2.2 in ComfyUI\n### **Model Loading**\n* The **Instagirl Wan 2.2** workflow automatically downloads all required models on first run\n* Allow 2-3 minutes for initial setup - no manual configuration needed\n* Once downloaded, **Instagirl Wan 2.2** generation starts immediately on subsequent uses\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1265/readme02.webp\" alt=\"Instagirl Wan 2.2\" width=\"750\"/>\n\n### **Craft Your Prompts**\n* Enter your **positive prompts** in the CLIP Text Encode node for **Instagirl Wan 2.2**:\n  * Focus on portrait descriptions and lighting details\n  * The **Instagirl Wan 2.2** LORA automatically enhances facial features and styling\n  * Example: `beautiful young woman, cinematic lighting, ultra detailed, 8k`\n* Add **negative prompts** to avoid unwanted elements in your **Instagirl Wan 2.2** generation:\n  * Common negatives: `blurry, low quality, distorted face, extra limbs`\n\n### **Generate and Save**\n* Configure your sampler settings for quality vs. speed balance in **Instagirl Wan 2.2**\n* Hit `Run` to generate your **Instagirl Wan 2.2** portrait\n* Images automatically save to your `comfyui/outputs` directory\n<img src=\"https://cdn.runcomfy.net/workflow_assets/1265/readme04.webp\" alt=\"Instagirl Wan 2.2\" width=\"750\"/>\n\n**Essential Settings for Instagirl Wan 2.2**\n* **Sampler Configuration**: Pre-optimized settings work best for **Instagirl Wan 2.2** - adjust steps for quality vs. speed preference\n* **Output Format**: Auto-detects best format for your **Instagirl Wan 2.2** system\n* **Batch Generation**: Keep batch_size at 1 for optimal **Instagirl Wan 2.2** quality\n\n## Acknowledgement\n\nThis **Instagirl Wan 2.2** ComfyUI workflow integrates the Instagirl LORA by Instara with the Wan 2.2 foundation model. Special recognition to Instara for developing this specialized **Instagirl Wan 2.2** portrait LORA and to the Wan 2.2 development team for their advanced diffusion architecture. The **Instagirl Wan 2.2** workflow implementation follows best practices for LORA integration, ensuring authentic performance for portrait generation.\n\n## More Resources About Instagirl Wan 2.2\nExplore additional resources and documentation related to [Instagirl Wan 2.2](https://civitai.com/models/1822984/instagirl-wan-22)\n"
    }
]